\subsubsection{Replacement Strategy: \emph{Which block should be replaced?}}\label{subsubsection: Replacement strategy}

\textbf{When a cache miss occurs}, the cache must bring a new block from the lower memory level. If the cache (or the relevant set) is \textbf{full}, one of the existing blocks must be \textbf{evicted} to make space. But, \emph{which block should be replaced?}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Depends on Mapping Type}}
\end{flushleft}
This decision depends on the type of mapping we have in the cache. With a direct-mapped cache, the answer is clear: always replace. However, with a set-associative or fully associative cache, it depends on the replacement policy.
\begin{enumerate}
    \item \important{Direct-Mapped Cache}. No choice: the mapping rule already determines \textbf{exactly one line} for the block. The existing block in that line is \textbf{\underline{always replaced}}. So more conflict misses.
    \item \important{Set-Associative} or \important{Fully Associative Cache}. There are multiple possible slots ($n$ lines in the set, or all lines for fully associative). A \textbf{replacement policy} decides which one to evict. A good replacement policy (like LRU, see below) can significantly reduce misses, especially in workloads with strong temporal locality.
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{gavel} \textbf{Common Replacement Policies}}
\end{flushleft}
A \important{Set-Associative} or \important{Fully Associative Cache} can adopt one of the following common replacement policies:
\begin{enumerate}
    \item \definitionWithSpecificIndex{Random Replacement}{Cache Replacement Policy: Random Replacement}{}. Pick any block in the set at random.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textbf{Simple hardware} and \textbf{fast}.
        \item[\textcolor{Green3}{\faIcon{check}}] \textbf{Avoids always evicting the same block} in some pathological patterns.
        \item[\textcolor{Red2}{\faIcon{times}}] It \textbf{doesn't exploit locality} because it is random and doesn't follow a heuristic. It \textbf{may evict a frequently used block}.
    \end{itemize}

    \item \definitionWithSpecificIndex{FIFO (First-In First-Out)}{Cache Replacement Policy: FIFO (First-In First-Out)}{}. Evict the block that has been in the cache the longest (oldest arrival).
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textbf{Easy to implement} with a queue per set.
        \item[\textcolor{Red2}{\faIcon{times}}] \textbf{Ignores recent usage}, so might \textbf{evict a frequently used block} if it's old.
    \end{itemize}

    \item \definitionWithSpecificIndex{LRU (Least Recently Used)}{Cache Replacement Policy: LRU (Least Recently Used)}{}. Evict the block that has \textbf{not been used for the longest time}.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] Matches the idea of \textbf{temporal locality}: recently used data is likely to be used again.
        \item[\textcolor{Red2}{\faIcon{times}}] More \textbf{complex hardware}: must track usage order for each block in the set.
    \end{itemize}
    \begin{examplebox}[: 2-Way Set-Associative Cache (LRU Policy)]
        Set 0 has:
        \begin{itemize}
            \item Way 0 $\rightarrow$ Block 4 (last used 5 cycles ago).
            \item Way 1 $\rightarrow$ Block 8 (last used 20 cycles ago).
        \end{itemize}
        CPU requests Block 12 (maps to Set 0), miss occurs. LRU chooses Way 1 (Block 8) to replace, since it was used least recently.
    \end{examplebox}
\end{enumerate}