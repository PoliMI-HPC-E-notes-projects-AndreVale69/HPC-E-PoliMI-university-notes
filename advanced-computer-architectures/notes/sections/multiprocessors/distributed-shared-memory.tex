\subsection{Distributed Shared Memory}

\definition{Distributed Shared Memory (DSM)} is a multiprocessor architecture where \textbf{physical memory} is \textbf{distributed} among nodes (each node has its own local memory), but the system provides a \textbf{global shared address space}, so any processor can address any memory location transparently. This is also called \definition{Cache-Coherent NUMA (CC-NUMA)}:
\begin{itemize}
    \item ``Distributed'': physical location of memory.
    \item ``Shared'': logical single address space for programming.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{UMA Problem}}
\end{flushleft}
In \textbf{UMA SMP}, all processors are connected to a \textbf{single centralized memory}. One memory controller, one set of DRAMs, and access latency is the same for everyone. The main problem is that it doesn't scale. Having too many processors causes too much contention on one memory system.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{DSM / NUMA idea}}
\end{flushleft}
To scale, designers spread memory \textbf{physically}. Each node has:
\begin{itemize}
    \item \textbf{Processor(s)} (with caches).
    \item \textbf{Local memory module} (part of the global memory).
\end{itemize}
All nodes are linked by an \textbf{interconnection network}. So instead of a single huge memory, the total memory is \textbf{partitioned} and attached to nodes.

\highspace
Now, if memory is physically distributed, how do we still let programmers think it is \emph{one shared memory}? That's where DSM gives the \textbf{illusion}. Using a \textbf{global shared address space}, any core can issue a load/store to any address. The system (hardware $+$ OS) automatically decides:
\begin{itemize}
    \item If the address belongs to its \textbf{local memory} (fast).
    \item If the address belongs to a \textbf{remote node}, so the request travels over the interconnect to that node's memory (slow).
\end{itemize}
This is why it's called \textbf{Distributed Shared Memory} (\emph{distributed} physically, \emph{shared} logically).

\highspace
\begin{examplebox}[: Distributed Shared Memory Analogy]
    Imagine a \textbf{university library system}:
    \begin{itemize}
        \item Every faculty (node) has its \textbf{own local library} (memory).
        \item All libraries together form the \textbf{global catalog} (shared address space).
        \newpage
        \item A student (processor) can request \emph{any} book (address).
        \begin{itemize}
            \item If the book is in their faculty's library, \textbf{local access} (fast).
            \item If the book is in another faculty, request goes over the campus shuttle (interconnect) to fetch it, \textbf{remote access} (slower).
        \end{itemize}
    \end{itemize}
    To ensure everyone has the latest version of a book (cache coherence), the \textbf{central catalog} (directory) tracks who borrowed which copy.
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Properties}}
\end{flushleft}
\begin{enumerate}
    \item \important{Non-uniform memory access (NUMA)}:
    \begin{itemize}
        \item Access to \textbf{local memory} (same node) is fast.
        \item Access to \textbf{remote memory} (different node) is slower.
        \item Latency depends on the distance over the interconnect.
    \end{itemize}
    \item \important{Single global address space}:
    \begin{itemize}
        \item Programmers can use \textbf{load/store instructions} without worrying about where memory physically resides.
        \item The hardware/OS ensures that references are routed correctly.
    \end{itemize}
    \item \important{Scalability}:
    \begin{itemize}
        \item Because memory is distributed, the system avoids the bottleneck of a single centralized memory controller (as in UMA SMPs).
        \item Larger multiprocessors (8-32 cores or 100s of cores in modern servers) rely on DSM.
    \end{itemize}
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{folder-open} \textbf{Why Directory-based protocols?}}
\end{flushleft}
In small SMPs ($\le 8$ cores), coherence is handled by \textbf{snooping}: every cache ``listens'' to a shared bus. But in DSM/NUMA there is \textbf{no single shared bus}, instead, there is a scalable interconnection network. Also, \textbf{broadcast snooping would not scale} (too much traffic, too many cores). The solution is \textbf{directory-based protocols}.
\begin{itemize}
    \item A \textbf{directory} keeps track of which caches have a copy of each memory block.
    \item Stored alongside each memory block (in the local memory module).
    \item When a block is read or written, the directory is updated, and only the relevant caches are notified.
\end{itemize}
This avoids global broadcasts.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Role of the interconnect}}
\end{flushleft}
The \textbf{interconnection network} ties everything together. Responsibilities:
\begin{itemize}
    \item \textbf{Data movement}: transfer cache lines between nodes.
    \item \textbf{Directory lookups}: route coherence requests to the memory module that ``owns'' a block.
    \item \textbf{Scalability}: provide enough bandwidth/low latency for dozens of nodes.
\end{itemize}
Topologies used: Crossbar (small systems), 2D mesh, torus, fat-tree, and often implemented as a Network-on-Chip (NoC) for chip multiprocessors.