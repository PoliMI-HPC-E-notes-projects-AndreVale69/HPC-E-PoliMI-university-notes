\subsection{Multithreading Basics}\label{subsection: Multithreading Basics}

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{The Problem to Solve}}
\end{flushleft}
Modern superscalar/VLIW processors are designed with \textbf{multiple functional units (FUs)}: integer ALUs, FP units, load/store units, branch units, etc.
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{bullseye}}] \textcolor{Green3}{\textbf{Goal:}} execute several instructions in parallel each cycle.
    \item[\textcolor{Red2}{\faIcon{exclamation-triangle}}] \textcolor{Red2}{\textbf{Reality:}} a single thread often \textbf{cannot provide enough ready instructions} because of:
    \begin{itemize}
        \item Cache misses $\rightarrow$ hundreds of cycles stall.
        \item Branch mispredictions $\rightarrow$ pipeline flush.
        \item Data dependencies $\rightarrow$ must wait for results.
    \end{itemize}
    \item[\textcolor{Red2}{\faIcon{thumbs-down}}] \textcolor{Red2}{\textbf{Result:}} many FUs stay \textbf{idle}, then wasted silicon, wasted power.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{The Goal of Hardware Multithreading}}
\end{flushleft}
Multithreading's central idea is simple: \textbf{when one thread is stalled, another can run, keeping the hardware busy.} Instead of \emph{widening issue width} (which hits ILP limits), we \textbf{interleave multiple threads} on the same datapath. Benefits:
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Hide stalls}} (memory latency, branch penalties).
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Improve utilization}} of functional units.
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Increase throughput}} (more instructions completed per cycle across threads).
\end{itemize}
The improvements are more visible when compared to an architecture without multithreading:
\begin{itemize}
    \item[\textcolor{Red2}{\faIcon{times}}] \textbf{Without Multithreading}
    \begin{itemize}
        \item One thread flows through the pipeline.
        \item If it stalls (say, waiting for L2 miss), the entire pipeline bubbles.
    \end{itemize}
    \item[\textcolor{Green3}{\faIcon{check}}] \textbf{With Multithreading}
    \begin{itemize}
        \item Several threads have state ``loaded'' in the CPU.
        \item Each cycle, the front-end can pick an instruction from whichever thread is ready.
        \item If Thread A stalls, Thread B (or C, D, etc.) keeps the pipeline fed.
    \end{itemize}
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{Throughput vs Latency}}
\end{flushleft}
With Multithreading, \important{Throughput} (instructions per cycle overall) increases because the pipeline rarely idles. Instead, \important{Latency} (time for a single thread's instruction to finish) may increase, since our thread sometimes has to wait its turn behind others. So:
\begin{itemize}
    \item Multithreading is \textbf{throughput-oriented} (maximize utilization).
    \item Not necessarily \textbf{latency-oriented} (our thread alone may not get faster).
\end{itemize}
This is why multithreading is great for \textbf{server workloads} (many independent threads), but less useful if we only run a \textbf{single-threaded application}. In other words, multithreading make the \textbf{core as busy as possible overall}, even if individual threads slow down.