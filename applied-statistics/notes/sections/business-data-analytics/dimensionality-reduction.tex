\subsection{Dimensionality Reduction}

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{The Challenge: Data in High Dimensions}}
\end{flushleft}
In many real-world problems, we \textbf{collect multiple variables for each observation}. For example, in a medical study, a patient might be described by age, weight, blood pressure, and dozens of test results. This leads to \textbf{high-dimensional data}, where \textbf{each observation is a point in a complex, multi-dimensional space} (formally, a Euclidean space of dimension $p$).

\highspace
\textcolor{Red2}{\textbf{\emph{The problem?}}} As the \textbf{number of variables} ($p$) \textbf{increases}, the \textbf{data becomes harder to visualize, interpret, and model}:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{exclamation-circle}}]
    \item Some \textbf{variables} might be \textbf{redundant} or \textbf{highly correlated}.
    \item \textbf{Computations} become more \textbf{expensive}.
    \item \textbf{Patterns become obscured} by the complexity.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{flag-checkered} \textbf{Goal of Dimensionality Reduction}}
\end{flushleft}
We want to \textbf{summarize the data using fewer variables}, say $k$ derived variables (with $k < p$), that still \textbf{retain most of the information}. This process is a balancing act:
\begin{itemize}
    \item \important{Clarity}: fewer variables make data \hl{easier to understand and visualize}.
    \item \important{Risk of oversimplification}: reducing dimensions too much can cause \hl{loss of important information}.
\end{itemize}
The key concept here is that in statistics, \textbf{information is variability}. The \hl{more variability we retain from the original data, the more information we preserve}.

\highspace
\begin{examplebox}[: Blood Cells]
    Imagine we measure thickness and diameter for a set of red blood cells:
    \begin{itemize}
        \item Each cell $=$ one observation with two variables.
        \item We can represent this as a table (numbers) or as a 2D scatterplot.
    \end{itemize}
    Now we ask ourselves: \emph{Can we describe these cells using only one feature instead of two?}
    
    If we choose only diameter or only thickness, we'll lose detail:
    \begin{itemize}
        \item Some cells will appear more similar than they really are.
        \item We miss variability that distinguishes them.
    \end{itemize}
    So, we seek a better single feature, one that captures the most variation possible from both thickness and diameter combined.
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-line} \textbf{The Statistical Insight: Maximize Variability}}
\end{flushleft}
Rather than randomly picking a feature, we \textbf{analyze the directions along which the data varies the most}.
\begin{enumerate}
    \item First, we find the \textbf{direction of maximum spread}.
    \item Then, the \textbf{second most spread direction}, orthogonal to the first.
\end{enumerate}
This is the essence of \definition{Principal Component Analysis (PCA)}, a \textbf{dimensionality reduction technique that finds the best directions} (linear combinations of variables) \textbf{to project the data}, while \textbf{maximizing retained variability}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Dimensionality Reduction in Practice}}
\end{flushleft}
Let's formalize the idea:
\begin{itemize}
    \item We start with a \textbf{data matrix $X$} of shape $n \times p$ ($n$ observations, $p$ variables).
    \item The \hl{goal} is to \textbf{obtain a new matrix} $M$ of shape $n \times k$, with $k < p$, that \textbf{captures most of the variability}.
    \item The \textbf{difference} between $X$ and $M$ is \textbf{residual variation}, the \textbf{information lost}.
\end{itemize}

\highspace
In summary, \definition{Dimensionality Reduction} is about simplifying complexity: \textbf{transforming a large set of variables into a smaller}, more interpretable set \textbf{without losing the essence of the data}. It's central to data exploration, preprocessing, and modeling, especially when working with high-dimensional datasets.