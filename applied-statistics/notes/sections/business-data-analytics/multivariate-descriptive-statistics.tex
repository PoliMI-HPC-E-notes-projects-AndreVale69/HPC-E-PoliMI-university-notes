\section{Business Data Analytics}

\subsection{Multivariate Descriptive Statistics}

When we move from \textbf{analyzing} a single variable (univariate analysis) to \textbf{multiple variables at once}, we enter the realm of \definition{Multivariate (MV) analysis}. A natural question arises: \emph{Is multivariate analysis just a replication of univariate analysis across several variables?}

\highspace
The answer is no, multivariate analysis introduces new and fundamental questions that cannot be answered by simply analyzing variables individually. The \textbf{core focus} shifts to \textbf{understanding how these variables interact with each other}. Specifically, we are concerned with the \textbf{dependence} and \textbf{correlation between variables}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Covariance: Measuring Joint Variability}}
\end{flushleft}
\textbf{To capture how two variables vary together}, we use \definition{Covariance}. The \definition{Sample Covariance} between variables $ x_{j} $ and $ x_{k} $ is calculated as:
\begin{equation}
    \text{cov}_{jk} = \text{Cov}\left(x_{j}, x_{k}\right) = s_{jk} = \dfrac{1}{n} \displaystyle\sum_{i=1}^{n} (x_{ij} - \bar{x}_{j})(x_{ik} - \bar{x}_{k})
\end{equation}
\begin{itemize}
    \item $s_{jk} = 0 \Rightarrow$ implies that there is \textbf{no linear relationship} between the two variables.
    \item $s_{jk} > 0 \Rightarrow$ suggests that \textbf{as one variable increases}, the \textbf{other tends to increase}.
    \item $s_{jk} < 0 \Rightarrow$ one \textbf{variable} tends to \textbf{decrease when the other increases}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Covariance Is Not Standardized}}
\end{flushleft}
The \textbf{value of covariance is not standardized}, it \textbf{depends on the units of measurement}, which makes comparisons difficult. For example:
\begin{itemize}
    \item Suppose we're measuring
    \begin{itemize}
        \item Height in centimeters
        \item Weight in kilograms
    \end{itemize}
    \item The covariance between height and weight will be expressed in \emph{centimeter-kilogram} units.
\end{itemize}
Now imagine we convert height to meters. The covariance value changes, because now you're multiplying meters $\times$ kilograms instead of centimeters $\times$ kilograms. Even though the \textbf{relationship between height and weight hasn't changed}, the \textbf{numerical value of covariance does change due to this unit change}. \textbf{Because of unit dependency, it's hard to compare covariances between different variable pairs}. Finally, it is \textbf{hard to interpret the magnitude of covariance in any absolute sense} (e.g., is 50 a large covariance or small? It depends on the units!).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Correlation: Standardized Covariance}}
\end{flushleft}
To standardize covariance and \textbf{measure the strength of a linear relationship} on a scale between $-1$ and $1$, we use the \definition{Correlation} coefficient, defined as:
\begin{equation}
    \text{cor}_{jk} = r_{jk} = \dfrac{s_{jk}}{\sqrt{s_{jj}}\sqrt{s_{kk}}} = \dfrac{
        \displaystyle\sum_{i=1}^{n}\left(x_{ij} - \bar{x}_{j}\right)\left(x_{ik} - \bar{x}_{k}\right)
    }{
        \sqrt{\displaystyle\sum_{i=1}^{n}\left(x_{ij} - \bar{x}_{j}\right)^{2}} \sqrt{\displaystyle\sum_{i=1}^{n}\left(x_{ik} - \bar{x}_{k}\right)^{2}}
    }
\end{equation}
This formula divides the covariance by the product of the standard deviations of the two variables, giving a \textbf{unitless value}:
\begin{itemize}
    \item $ r = 0 $: No linear correlation
    \item $ r > 0 $: Positive correlation (\textbf{both variables increase or decrease together})
    \item $ r < 0 $: Negative correlation (\textbf{one increases while the other decreases})
    \item $ \left|r\right| = 1 $: Perfect correlation (\textbf{exact linear dependence})
\end{itemize}
Thus, correlation not only \textbf{reveals the direction of the relationship} but also its \textbf{strength}.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/correlation.pdf}
    \caption{Direction of correlation: positive (left), none (center), negative (right).}
\end{figure}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Describing MV Data: Vector and Matrices}}
\end{flushleft}
When analyzing multivariate data:
\begin{itemize}
    \item We compute the \textbf{vector of \definition{Sample Means}}:
    \begin{equation}
        \bar{\mathbf{X}} = [\bar{X}_{1}, \bar{X}_{2}, \dots, \bar{X}_{p}]
    \end{equation}

    \item And the \textbf{symmetric and positive semidefinite} (eigenvalues are nonnegative) \hqlabel{def: covariance matrix}{\definition{Variance-Covariance matrix}} $ \mathbf{S} $, which \textbf{summarizes the covariances between all pairs of variables}:
    \begin{equation}
        S =
        \begin{bmatrix}
            s_{11} & \cdots & s_{1p} \\
            \vdots & \ddots & \vdots \\
            s_{p1} & \cdots & s_{pp}
        \end{bmatrix}
    \end{equation}

    \item Alternatively, we can use the \definition{Correlation matrix} $ \mathbf{R} $, where \textbf{all diagonal elements are 1} (because each variable is perfectly correlated with itself) and \textbf{off-diagonal elements are correlation coefficients}:
    \begin{equation}
        R =
        \begin{bmatrix}
            1      & \cdots & r_{1p} \\
            \vdots & \ddots & \vdots \\
            r_{p1} & \cdots & 1
        \end{bmatrix}
    \end{equation}
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Scatterplots - Visualizing Variable Pairs}}
\end{flushleft}
One of the most intuitive and widely used tools in multivariate analysis is the \definition{2D Scatterplot}. Each plot shows how two variables relate to each other:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item Clusters or linear trends can indicate correlation or dependence.
    \item Scatterplots are \textbf{ideal for spotting positive, negative, or no correlation} visually.
\end{itemize}
However, scatterplots have a \textcolor{Red2}{\textbf{limitation}}: they \textbf{only} allow us to \textbf{analyze two variables at a time}. When dealing with many variables, the \textbf{number of possible pairings becomes large}, making it \textbf{difficult to read or interpret} the scatterplots individually. This is where quantitative measures (like correlation matrices) and higher-dimensional graphics come into play.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.91\textwidth]{img/scatterplot.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{Scatterplot matrix of four variables. This scatterplot matrix displays all pairwise relationships among four variables:
    \begin{itemize}
        \item Diagonal plots (top-left to bottom-right): Histograms showing the distribution of each variable.
        \item Off-diagonal plots: 2D scatterplots illustrating the relationship between each pair of variables.
    \end{itemize}}
\end{figure}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Rotated Plots in 3D - Capturing Complexity}}
\end{flushleft}
When dealing with \textbf{three variables}, we can \textbf{extend scatterplots into 3D space} using \definition{Rotated plots}. These visualizations allow us to:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textbf{Explore interdependencies} among three variables at once.
    \item \textbf{Gain spatial insight} into how data points spread in three-dimensional space.
    \item Observe \textbf{complex patterns that are invisible in 2D}.
\end{itemize}
Yet again, as we move \textbf{beyond three variables}, visualizing becomes \textbf{impractical}, our brains cannot easily comprehend 4D or higher dimensions. Hence, dimensionality reduction techniques like PCA are often used alongside visualizations to make high-dimensional data ``digestible''.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/rotated-plots.pdf}
    \caption{A simple rotated plots in 3D.}
\end{figure}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Star Plots - Shape-Based Comparison}}
\end{flushleft}
\definition{Star plots} offer a creative way to \textbf{represent multivariate data}:
\begin{itemize}
    \item \textbf{Each variable is represented as a ray} (spoke) starting from a central point.
    \item The \textbf{length of each ray} corresponds to the \textbf{value} of that variable.
    \item When \textbf{rays are connected}, they form a ``star-like shape'' \textbf{unique} to each observation.
\end{itemize}
This method is \textbf{excellent for comparing patterns} between observations:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textbf{Similar shapes} suggest \textbf{similar data profiles}.
    \item \textbf{Differences} in shape can \textbf{quickly highlight outliers} or clusters.
\end{itemize}
However, star plots have limitations:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item They \textbf{do not quantify correlation}.
    \item The \textbf{direction} and \textbf{magnitude} of relationships between variables are \textbf{not explicit}.
    \item They are better for \textbf{visual pattern recognition} than for precise statistical analysis.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/star-plot.pdf}
    \caption{Star Plot (Radar Chart) of Multivariate Data.}
\end{figure}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Chernoff Faces - Human-Centric Visualization}}
\end{flushleft}
Chernoff faces \cite{chernoff1973use} are an \textbf{innovative visualization method} where \textbf{multivariate data is represented as a human face}:
\begin{itemize}
    \item Each \textbf{variable controls a facial feature} (e.g., mouth curvature, eye size, nose length).
    \item \textbf{People are naturally attuned to recognizing faces} and subtle differences in expressions.
    \item Hence, Chernoff faces \textbf{leverage human perception} for quickly comparing \textbf{multivariate observations}.
\end{itemize}
Despite being engaging, Chernoff faces also have \textbf{drawbacks}:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item They \textbf{do not provide numerical precision}.
    \item The \textbf{mapping of variables to facial features} can be \textbf{arbitrary}.
    \item They work best as a \textbf{qualitative summary tool} rather than for deep statistical inference.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/chernoff-1973.jpg}
    \caption{Some Chernoff faces.\cite{chernoff1973use}}
\end{figure}


\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l | l | l @{}}
            \toprule
            Graphic Type      & Strengths                                      & Limitations                                        \\
            \midrule
            Scatterplots      & Clear view of pairwise relationships           & Hard to scale beyond 2 variables                   \\ [.5em]
            3D Plots          & Visualizes 3-variable interaction              & Limited to 3 dimensions, requires rotation         \\ [.5em]
            Star Plots        & Quick shape-based comparison across variables  & No quantification, poor at showing correlations    \\ [.5em]
            Chernoff Faces    & Leverages facial perception for comparison     & Subjective, lacks precision                        \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{When and why to use graphics.}
\end{table}