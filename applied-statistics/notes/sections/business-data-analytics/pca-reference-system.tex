\subsection{PCA Reference System}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why Do We Talk About Projections in PCA?}}
\end{flushleft}
When we say that PCA projects data, we mean it \textbf{transforms the data points by placing them onto new axes}, the principal components, that better represent the structure of variability.

\highspace
\textcolor{Green3}{\faIcon{question-circle} \textbf{\emph{But why are projections so important?}}} Because PCA has \textbf{two goals}:
\begin{enumerate}
    \item \important{Maximize variance}: We want the \textbf{data to spread out as much as possible along the new axis}. This spread means we're capturing differences between observations.
    \item \important{Minimize residuals}: We want to \textbf{minimize the error} when we approximate each point using the new axes. This is done by projecting each point perpendicularly onto the new axes, just like the shortest path between a point and a line.
\end{enumerate}
This is why we ``keep talking about projections'': they allow us to \textbf{retain the most information} with \textbf{the least distortion}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{Generalizing to More Dimensions}}
\end{flushleft}
In real datasets, we often have \textbf{more than two variables}. PCA scales to $p$ dimensions, and the idea of projection still applies:
\begin{itemize}
    \item PC1: The \textbf{direction in $p$-dimensional space} where data varies most.
    \item PC2: The next direction, \textbf{perpendicular to the first}, that captures remaining variability.
    \item This continues until we have $p$ principal components, each orthogonal to the others.
\end{itemize}
So \textbf{PCA rotates the entire dataset into a new reference system} where the data structure is easier to understand.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{A New Reference System}}
\end{flushleft}
After PCA, our data now lives in a \textbf{new coordinate system}:
\begin{itemize}
    \item The \textbf{original variables} (e.g., height, weight) are no longer the axes.
    \item Instead, the axes are \textbf{principal components}, which are \textbf{combinations of the original variables}.
\end{itemize}
This new reference system has two main features:
\begin{enumerate}
    \item \textbf{PCA results are rotation invariant}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{book}}] \textcolor{Green3}{\textbf{\emph{What does this means?}}} It means that if we rotate our dataset (e.g., by changing the coordinate system), PCA still finds the same underlying structure, the same principal components (relative to the data).

        \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{\emph{Why?}}} PCA depends only on the relationships between the data points, specifically: the variances and covariances between variables. These are not affected by rotations of the data in space. Mathematically, PCA extracts eigenvectors of the covariance matrix that are invariant under rotation with respect to relative directions.
    \end{itemize}

    \item \textbf{Principal Components are independent (uncorrelated)}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{book}}] \textcolor{Green3}{\textbf{\emph{What does this means?}}} The principal components (PC1, PC2, ...) are uncorrelated with each other. Knowing the value of one PC tells us nothing about the value of the next.

        \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{\emph{Why?}}} Each new component is orthogonal to the previous one. And since the correlation is equal to the cosine of the angle between the variables ($\cos(90) = 0$), PAC ensures that the correlation between the PCs is zero.
        
        \item[\textcolor{Green3}{\faIcon{check}}] In high dimensions, \textbf{collinearity} often indicates redundant information between variables and can cause problems in the analysis. \textbf{PCA solves} this problem by giving us independent, uncorrelated variables.
    \end{itemize}
\end{enumerate}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/pca.pdf}
    \caption{We have a bunch of scattered red dots. We have drawn a black line through the middle of the cloud of dots in the direction where the dots are most scattered. This line is our first principal component (PC1). For each red point, we drop a perpendicular line onto the black line. Where it lands, we place a blue point. This is called projection. Furthermore, the longer the black line, the more red points are taken into account (thus maximizing variance); on the other hand, the blue points must be close to the red points (shorter the distance) to minimize residuals and lose less information.}
\end{figure}

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/pca-2.pdf}
    \caption{By drawing two lines, PC1 and PC2, we obtain a new reference system. It is a rotated system. The axes are principal components, which are combinations of the original variables.}
\end{figure}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Principal Components: Linear Combinations}}
\end{flushleft}
Let's now talk about \textbf{how principal components are constructed}. Each principal component is a \textbf{linear combination of the original variables}. This means:
\begin{equation*}
    \text{PC}_{1} = \phi_{11} \cdot x_{1} + \phi_{21} \cdot x_{2} + \dots + \phi_{p1} \cdot x_{p}
\end{equation*}
But in general:
\begin{equation}
    \text{PC}_{j} = \phi_{1j} \cdot x_{1} + \phi_{2j} \cdot x_{2} + \dots + \phi_{pj} \cdot x_{p}
\end{equation}
Where:
\begin{itemize}
    \item The $\phi$-values (phi) are called \definition{Loadings}.
    \item They are the \textbf{weights assigned to each original variable}.
    \item The \textbf{higher the loading}, the \textbf{more that variable contributes} to that principal component.
\end{itemize}
For \textbf{each observation} (e.g., a person, product), we now \textbf{compute their position in the new PCA space}:
\begin{equation}
    z_{ij} = \phi_{1j} \cdot x_{i1} + \phi_{2j} \cdot x_{i2} + \dots + \phi_{pj} \cdot x_{ip}
\end{equation}

\newpage

\noindent
These values $ z_{ij} $ are called \definition{Scores}. They tell us:
\begin{itemize}
    \item Where each observation lies \textbf{along a principal component axis}.
    \item How much that \textbf{component contributes} to describing this observation.
\end{itemize}
So now, instead of describing each person with $p$ original variables, we describe them with $k$ scores, where $k < p$, but we still capture the essence of their data.
