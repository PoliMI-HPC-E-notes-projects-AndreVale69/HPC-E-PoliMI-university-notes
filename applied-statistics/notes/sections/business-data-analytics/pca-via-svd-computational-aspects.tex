\subsection{PCA via SVD - Computational Aspects}

Instead of computing PCA by:
\begin{enumerate}
    \item Building the covariance matrix $ S = \dfrac{1}{n-1} X^{T} X $
    \item Diagonalizing $S$ to get eigenvectors and eigenvalues
\end{enumerate}
We can instead \textbf{perform PCA directly using} \definition{Singular Value Decomposition (SVD)} on the \textbf{centered data matrix} $X$ itself.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Singular Value Decomposition (SVD)}}
\end{flushleft}
\definition{Singular Value Decomposition (SVD)} method is a factorization of a matrix into three other matrices. If $X$ is the centered data matrix (size $n \times p$), then:
\begin{equation}
    X = U \Sigma V^{T}
\end{equation}
Where:
\begin{itemize}
    \item $\mathbf{U}$: matrix of \textbf{left singular vectors} (related to scores). It is orthogonal and $n \times n$.
    \item $\boldsymbol{\Sigma}$: diagonal matrix of \textbf{singular values} $ \Sigma_{1}, \Sigma_{2}, \dots$
    \begin{itemize}
        \item Non-negative real numbers on the diagonal (singular values)
        \item Singular values sorted in descending order (from largest to smallest)
        \item Number of eigenvalues guaranteed by the minimum between number of columns and number of rows: $\min\left(n, p\right)$.
    \end{itemize}
    \item $\mathbf{V}$: matrix of \textbf{right singular vectors}. It is orthogonal and $p \times p$. They are the main directions, so the loadings of PCA.
\end{itemize}
The eigenvalues of the covariance matrix can be calculated from the singular values $\Sigma_{i}$:
\begin{equation*}
    \lambda_{i} = \dfrac{\Sigma_{i}^{2}}{n-1}
\end{equation*}
This is because if we plug the SVD of $X$ into this formula, the eigen decomposition of $S$ naturally follows.

\begin{table}[!htp]
    \centering
    % \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l | l @{}}
            \toprule
            \textbf{Concept} & \textbf{Meaning/Role in PCA} \\
            \midrule
            $ \mathbf{X} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{T} $  & SVD decomposition of centered data matrix                 \\ [.5em]
            $ \mathbf{V} $                                                  & Principal directions (same as eigenvectors of covariance) \\ [.5em]
            $ \Sigma_{i} $                                                  & Singular values                                           \\ [.5em]
            $ \lambda_{i} = \dfrac{\Sigma_{i}^{2}}{n-1} $                   & Variance explained by each principal component            \\
            \bottomrule
        \end{tabular}
    % \end{adjustbox}
    \caption{Summary table of SVD for PCA.}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Numerically more stable}}: works better when the data matrix is large or ill-conditioned.
    \item \textcolor{Green3}{\textbf{No need to compute and store the covariance matrix}}: we can work directly on $X$.
    \item \textcolor{Green3}{\textbf{More efficient}} when $p$ is large (many variables), especially in high-dimensional problems.
\end{itemize}