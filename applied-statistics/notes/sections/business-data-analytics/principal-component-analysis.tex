\subsection{Principal Component Analysis}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Why PCA?}}
\end{flushleft}
Imagine we have a data set with \textbf{many variables}, such as measurements of people, products, or cities. Some \textbf{variables might be closely related} (like height and weight), \textbf{while others might carry similar information}. This creates two challenges:
\begin{enumerate}
    \item It becomes \textbf{hard to analyze and interpret} the data.
    \item \textbf{Redundancy} can lead to inefficiency and confusion.
\end{enumerate}
\definition{Principal Component Analysis (PCA)} helps solve this by providing a \textbf{simplified version of the dataset}, where we focus only on the \textbf{essential information}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{The Main Idea}}
\end{flushleft}
PCA works by \textbf{creating new variables}, called \textbf{principal components}, that are:
\begin{itemize}
    \item \textbf{Combinations} of the original variables.
    \item \textbf{Ordered} so that the \textbf{first component captures the most variability} in the data.
    \item Each \textbf{subsequent component captures the next most variability}, but only from what's left over, and each \textbf{new component is uncorrelated with the previous} ones.
\end{itemize}
Imagine this like finding better ``angles'' or ``perspectives'' from which to view our data, ones that maximize how much we can see (i.e., variability) with as few perspectives as possible.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Variability is Information}}
\end{flushleft}
In statistics, \definition{Variability}, \textbf{how much values change from one observation to the next}, is considered \textbf{information}. The more variability a component captures, the \textbf{more useful it is} in understanding the data. So, PCA's job is to \textbf{find the directions in which the data varies the most}, and to use those directions to summarize the dataset.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How PCA Changes the Dataset}}
\end{flushleft}
Let's say we start with $p$ variables (like height, weight, age, income...). \textbf{PCA gives us $p$ principal components}, but the \emph{magic} is that \textbf{we usually only need the first few} to understand most of what's going on.

\highspace
So instead of working with the full, original dataset, we now have a \textbf{simpler version}:
\begin{itemize}
	\item We still have the \textbf{same number of observations}.
	\item But each observation is now described by \textbf{fewer, more informative variables} (components).
\end{itemize}
This is called a \textbf{low-dimensional representation} of the data.

\highspace
\begin{flushleft}
	\textcolor{Green3}{\faIcon{poll} \textbf{Scores and Loadings - The Ingredients and the Result}}
\end{flushleft}
In this new system:
\begin{itemize}
    \item The \important{scores} tell us where \textbf{each observation lies along the new axes} (the principal components).
    \item The \important{loadings} tell us \textbf{how the original variables contribute to each component}.
\end{itemize}
Think of it like cooking:
\begin{itemize}
    \item \textbf{Original variables} $=$ ingredients
    \item \textbf{Loadings} $=$ recipe instructions
    \item \textbf{Principal components} $=$ final dishes
    \item \textbf{Scores} $=$ ratings of the dishes for each person (observation)
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{square-root-alt} \textbf{Matrix Representation}}
\end{flushleft}
Let's denote the data matrix as $X$, with $n$ rows (observations) and $p$ columns (variables). PCA is essentially about \textbf{transforming this matrix $X$} into a \textbf{new matrix}, where:
\begin{itemize}
    \item The data is now described by \textbf{principal components} instead of the original variables.
    \item The \textbf{goal is to rotate and simplify} the data in a way that \textbf{emphasizes the most important directions} (maximum variability).
\end{itemize}
Two key matrices in PCA:
\begin{itemize}
    \item \important{Loadings Matrix ($V$)}
    \begin{itemize}
        \item This matrix contains the \textbf{weights} used to build the principal components.
        \item Each \textbf{column} in V corresponds to a \textbf{principal component}.
        \item Each \textbf{value} in $V$ shows \textbf{how much each original variable contributes} to the component.
    \end{itemize}
    We can think of $V$ as the recipe book: it tells us how to combine original variables to create the new components.

    \item \important{Scores Matrix ($U$)}
    \begin{itemize}
        \item This matrix contains the \textbf{projections of the data} onto the principal components.
        \item Each \textbf{row} in $U$ represents an \textbf{observation in the new PCA space}.
        \item These are called scores, they tell us \textbf{where each observation lands along the new axes} (PC1, PC2...).
    \end{itemize}
    So $U$ is the result: it shows \textbf{how our data looks in the new, simplified system}.
\end{itemize}
PCA can be \textbf{computed using Singular Value Decomposition (SVD)}:
\begin{equation}
    X = U \cdot S \cdot V^{T}
\end{equation}
PCA \textbf{factorizes the data matrix $X$}. Here's what each matrix means:
\begin{itemize}
    \item $X$: Original data ($n \times p$)
    \item $U$: Scores matrix ($n \times p$), orthogonal (columns are independent)
    \item $S$: Diagonal matrix of \textbf{singular values} (related to the variance explained)
    \item $V^{T}$: Transposed loadings matrix ($p \times p$), also orthogonal.
\end{itemize}
But for understanding, focus on this \textbf{simpler form}:
\begin{equation*}
    \textbf{PCA result} = \text{Scores} = X \cdot V
\end{equation*}
We \textbf{multiply the data $X$ by the loadings matrix $V$} to obtain the \textbf{scores}.

\highspace
However, both $U$ and $V$ are \textbf{orthogonal matrices}, meaning:
\begin{itemize}
    \item Their columns are perpendicular (\textbf{no redundancy}).
    \item Principal \textbf{components are uncorrelated}, each new component captures new, non-overlapping information.
\end{itemize}
This is mathematically elegant and practically useful because it \textbf{removes multicollinearity} and makes downstream \textbf{analysis simpler and more robust}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How Much Information Do We Keep?}}
\end{flushleft}
Each principal component has a \textbf{percentage of variance explained}, this tells us how much of the original data's information it retains. Often, \textbf{the first 1 or 2 components explain so much that we can ignore the rest}.

\highspace
In conclusion, \textbf{PCA helps us focus on what matters} in our data. It's like cleaning our glasses: everything becomes sharper, simpler, and more meaningful. We go from being overwhelmed by numbers to seeing clear patterns. It's a tool used everywhere, from finance to biology, marketing to engineering, whenever people need to make sense of complex data.