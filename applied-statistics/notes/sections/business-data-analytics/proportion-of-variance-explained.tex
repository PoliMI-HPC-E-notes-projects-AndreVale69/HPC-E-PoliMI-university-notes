\subsection{Proportion of Variance Explained (PVE)}

In the previous sections, we explained how to compute PC1, PC2, etc. as new variables. Each PC is a linear combination of the original variables, and each maximizes the variance under orthogonality constraints. But now the question becomes: ``\emph{okay, we computed these new components... but how useful are they?}''.

\highspace
In PCA, variance equals information. So now our \textbf{goal} is: ``how much \textbf{information (variance) is explained by PC1}? And by \textbf{PC2}? And by \textbf{PC3}? And so on''. This is where \definition{Proportion of Variance Explained (PVE)} comes in.

\highspace
We know that each principal component explains some variance:
\begin{itemize}
    \item PC1 captures the \textbf{largest possible amount} of variance.
    \item PC2 captures the \textbf{next largest amount}, and so on.
    \item All PCs \textbf{together} explain \textbf{100\% of the variance} in the data (no information is lost if we keep all of them).
\end{itemize}
If the data matrix $X$ is centered (i.e., variables have mean 0), then the \important{total variance in the data} is:
\begin{equation}
    \text{Var}_{\text{TOT}} = \displaystyle\sum_{j=1}^{p} \text{Var}(X_{j}) = \displaystyle\sum_{j=1}^{p} \left( \dfrac{1}{n} \displaystyle\sum_{i=1}^{n} x_{ij}^{2} \right)
\end{equation}
This is just the \textbf{sum of the variances} of the original variables.

\highspace
To know the \textbf{variance} at the step $k$, so for the \textbf{$k$-th principal component}, we generalize:
\begin{equation}
    \text{Var}_{k} = \dfrac{1}{n} \displaystyle\sum_{i=1}^{n} z_{ik}^{2}
\end{equation}
Where $z_{ij}$ is the \textbf{score of observation $i$} on component $k$. In other words, it is the \textbf{spread of data along PC$k$}.

\highspace
Since the \definitionWithSpecificIndex{PVE}{Proportion of Variance Explained (PVE)}{} is a proportion, the equation is pretty obvious:
\begin{equation}
    \text{PVE}_{k} = \dfrac{\text{Var}_{k}}{\text{Var}_{\text{TOT}}}
\end{equation}
It is the \textbf{percentage of total variance} explained by PC$k$. This tells us how informative PC$k$ is, and whether it's worth keeping or can be discarded.

\highspace
In practice, we \textbf{often keep only the first few PCs}. For example, if we find that $PC1 + PC2$ explain 90\% of the variance, we can keep only those two (dimensionality reduction).