\subsection{Classification Trees}

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l l @{}}
            \toprule
            Feature & \textbf{Regression Tree} & \textbf{Classification Tree} \\
            \midrule
            Target variable $Y$ & \textbf{Numeric} (e.g., salary, age)  & \textbf{Categorical} (e.g., disease type)                 \\ [.5em]
            Leaf prediction     & Mean of $y_i$ in the region           & Most frequent \textbf{class} in the region (mode)         \\ [.5em]
            Split criterion     & Minimize \textbf{RSS}                 & Minimize \textbf{impurity} (e.g., Gini or Entropy)        \\ [.5em]
            Output              & A number                              & A class label                                             \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{How Classification Trees differ from Regression Trees.}
\end{table}

\noindent
So while regression trees try to \textbf{minimize squared error}, classification trees try to \textbf{maximize purity}, that is, how ``pure'' each region is in terms of class labels.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How Class Prediction Works}}
\end{flushleft}
Once the classification tree is built, each \textbf{terminal node (leaf)} contains:
\begin{itemize}
    \item The \textbf{most frequent class} among the training samples that fall into that region.
    \item Optionally: the \textbf{distribution} of classes (e.g., 80\% yes, 20\% no).
\end{itemize}
So for a new observation, the tree:
\begin{enumerate}
    \item Routes it to a terminal region via decision splits.
    \item Outputs the \textbf{majority class} in that region as the prediction
\end{enumerate}
This is why it's called a \textbf{majority vote} method within each region.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Purpose of Impurity Measures}}
\end{flushleft}
In classification trees, we need to \textbf{evaluate the ``quality'' of a split}, that is, \hl{how pure each resulting region is}. We use impurity functions to measure \textbf{how mixed the classes are} inside a node.
\begin{itemize}
    \item A \textbf{pure node} contains samples from only one class, low impurity.
    \item A \textbf{mixed node} contains samples from many classes, high impurity.
    \item \definition{Gini Index}. For a region (node) $R_j$, let:
    \begin{itemize}
        \item $\hat{p}_{jk}$ be the proportion of class $k$ samples in region $R_j$.
        \item $K$ be the number of classes.
    \end{itemize}
    The Gini index is:
    \begin{equation}
        G_j = \sum_{k=1}^K \hat{p}_{jk} \left(1 - \hat{p}_{jk}\right) = 1 - \sum_{k=1}^K \hat{p}_{jk}^2
    \end{equation}
    \begin{itemize}
        \item Minimum value (zero) when the node is \textbf{pure} (100\% of one class).
        \item Maximum value when classes are \textbf{evenly mixed}.
    \end{itemize}
    The Gini Index is \textbf{faster} to compute than the Cross-Entropy.
    \item \definition{Cross-Entropy} (or \definition{Log Loss} or \definition{Deviation})
    \begin{equation}
        D_j = -\sum_{k=1}^K \hat{p}_{jk} \log\left(\hat{p}_{jk}\right)
    \end{equation}
    \begin{itemize}
        \item Also \textbf{0} when the node is \textbf{pure} (e.g., one class with probability 1).
        \item Gets \textbf{larger} as class distribution becomes more \textbf{uncertain or mixed}
    \end{itemize}
\end{itemize}