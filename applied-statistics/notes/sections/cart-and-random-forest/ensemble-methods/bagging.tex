\subsubsection{Bagging}\label{subsubsection: Bagging}

\definition{Bagging} stands for \definition{Bootstrap Aggregating}. It is a technique to improve the \textbf{accuracy and stability of models} (especially decision trees) by \textbf{reducing their variance}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does Bagging work?}}
\end{flushleft}
\begin{enumerate}
    \item \important{Bootstrap Aggregation (Bagging)}. From our training data $n$ points, create $B$ new datasets, each of size $n$. Some points are repeated, some are left out in each sample. These are called \textbf{bootstrap samples}, where \emph{bootstrap} means sampling with replacement.

    \begin{examplebox}[: Bootstrap Sample]
        Original data:
        \begin{equation*}
            \left[1, 2, 3, 4, 5\right]
        \end{equation*}
        One bootstrap sample (size 5, sampled with replacement) could be:
        \begin{equation*}
            \left[1, 3, 3, 4, 5\right]
        \end{equation*}
        Some points can appear multiple times (3 appears twice), some points may not appear at all (2 is missing).
    \end{examplebox}


    \item \important{Train multiple models}. On each bootstrap sample, train a \textbf{separate tree}. Each tree sees \textbf{slightly different data} and thus makes slightly different mistakes.
    
    
    \item \important{Averaging Predictions}. After building $B$ trees:
    \begin{itemize}
        \item \important{Regression}: Average the predictions from all trees
        \begin{equation}
            \hat{f}_{\text{bag}}(x) = \dfrac{1}{B} \cdot \sum_{b=1}^B \hat{f}^{(b)}(x)
        \end{equation}
        \item \important{Classification}: Take the \textbf{majority vote} across all trees.
    \end{itemize}
\end{enumerate}
\textbf{Bagging is the foundation of Random Forest.}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Step & Action \\
        \midrule
        Bootstrap   & Draw $B$ samples with replacement                      \\ [.3em]
        Train       & Fit a tree on each sample                              \\ [.3em]
        Predict     & Average (regression) or majority vote (classification) \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why does bagging work?}}
\end{flushleft}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Problem with a Single Tree & How Bagging Helps \\
        \midrule
        High variance   & Different trees reduce variance when averaged \\ [.3em]
        Overfits easily & Averaging smooths out overfitting effects     \\ [.3em]
        Unstable        & Aggregation stabilizes predictions            \\
        \bottomrule
    \end{tabular}
\end{table}