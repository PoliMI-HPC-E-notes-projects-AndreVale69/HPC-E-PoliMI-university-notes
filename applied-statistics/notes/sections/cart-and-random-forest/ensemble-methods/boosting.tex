\subsubsection{Boosting}

\definition{Boosting} is an ensemble method that builds trees \textbf{sequentially}, \emph{not independently}. While Bagging (page \pageref{subsubsection: Bagging}) builds trees in parallel on random data samples, Boosting builds \textbf{each new tree to fix the mistakes of the previous ones}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How it works}}
\end{flushleft}
\begin{enumerate}
    \item Start with a simple model (maybe predicting the mean).
    \item Compute the \textbf{errors (residuals)} from this model.
    \item Build the \textbf{next tree to predict those residuals}.
    \item Repeat: Each tree tries to \textbf{correct the mistakes of the previous ones}
\end{enumerate}
The main idea is to create a small tree and make corrections in each loop to create a strong model. Boosting builds the final model \textbf{step-by-step (stagewise)}:
\begin{equation}
    \hat{f}(x) = \sum_{b=1}^B \lambda \cdot \hat{f}^{(b)}(x)
\end{equation}
Where:
\begin{itemize}
    \item $B$: Number of trees.
    \item $\lambda$: \textbf{Shrinkage} parameter (\hl{learning rate}).
    \item Each $\hat{f}^{(b)}$ is a \textbf{small tree} (often shallow, e.g., depth 1-3).
\end{itemize}
At each stage, the model gets \textbf{incrementally better}:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Bias reduction}}: By fitting residuals repeatedly, bias decreases.
    \item \textcolor{Green3}{\textbf{Variance control}}: Small trees and shrinkage keep variance low.
    \item \textcolor{Green3}{\textbf{Learning rate $\lambda$}}: Smaller $\lambda$, slower, more stable learning.
\end{itemize}
Boosting tends to produce \textbf{low-bias, low-variance models}, if tuned correctly.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Boosting Algorithm (Simplified for Regression)}}
\end{flushleft}
\begin{enumerate}
    \item Initialize model with a constant prediction (e.g., the mean of $y$).
    \item For $b = 1, 2, ..., B$ (from one to the number of trees):
    \begin{enumerate}
        \item Compute the \textbf{residuals}:
        \begin{equation*}
            r_i = y_i - \hat{f}^{(b-1)}(x_i)
        \end{equation*}
        \item Fit a \textbf{small regression tree} to predict $r_i$.
        \item Update the model:
        \begin{equation*}
            \hat{f}^{(b)}(x) = \hat{f}^{(b-1)}(x) + \lambda \cdot \hat{f}_{\text{tree}}(x)
        \end{equation*}
    \end{enumerate}
    \item Output the final model after $B$ steps.
\end{enumerate}
Boosting \textbf{works especially well} when:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item The signal is \textbf{subtle} and buried under noise.
    \item The model needs \textbf{many small refinements} to capture patterns.
\end{itemize}

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l l @{}}
            \toprule
            Aspect & Bagging / Random Forest & Boosting \\
            \midrule
            Trees           & Independent                   & Sequential, corrective                \\ [.3em]
            Error Handling  & Reduce variance via averaging & Reduce bias via residual fitting      \\ [.3em]
            Typical Trees   & Full-sized, deep trees        & Shallow trees (stumps or depth 3-4)   \\ [.3em]
            Output          & Average / vote                & Weighted sum of weak models           \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Bagging / Random Forest vs. Boosting.}
\end{table}
