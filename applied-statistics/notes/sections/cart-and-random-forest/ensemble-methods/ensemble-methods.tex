\subsection{Ensemble Methods}

Imagine that we build \textbf{one decision tree}. Sometimes it works well; other times, it makes poor decisions because it's too simple or overfits the data. So, we can ask, ``\emph{what if, instead of asking one tree, we ask many trees and take the average of their answers?}''. This is where ensemble methods come in.

\highspace
Instead of relying on \textbf{one weak model} (single decision tree), \definition{Ensemble Methods} build \textbf{many models} and the \textbf{combine them}. It's like: ``\emph{don't trust one opinion, ask 100 people and take the average or the majority vote}''.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why are they needed?}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l @{}}
            \toprule
            Problem with Single Trees & How Ensembles Help \\
            \midrule
            \textbf{High variance}                                      & Averaging many trees reduces variance \\ [.3em]
            \textbf{Unstable} (small changes $\rightarrow$ big effect)  & Aggregating stabilizes the output     \\ [.3em]
            \textbf{Overfitting prone}                                  & Combined models generalize better     \\ [.3em]
            \textbf{Piecewise constant}                                 & Ensembles smooth out sharp splits     \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How do they work conceptually?}}
\end{flushleft}
\begin{enumerate}
    \item Build \textbf{many trees} on variations of the data (different subsets, random splits, etc.)
    \item \textbf{Combine} their predictions:
    \begin{itemize}
        \item For regression: \textbf{average}.
        \item For classification: \textbf{majority vote}.
    \end{itemize}
\end{enumerate}
The main Ensemble Methods are:
\begin{itemize}
    \item \important{Bagging}: Build trees on \textbf{bootstrapped datasets} (random samples of the data).
    \item \important{Random Forest}: Like Bagging, but adds \textbf{randomness in splits}.
    \item \important{Boosting}: Build trees \textbf{sequentially}, each focusing on the errors of the previous.
\end{itemize}