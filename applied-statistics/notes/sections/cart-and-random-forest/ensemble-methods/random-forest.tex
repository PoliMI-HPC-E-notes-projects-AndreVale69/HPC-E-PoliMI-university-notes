\subsubsection{Random Forest}

\definition{Random Forest} builds on \textbf{Bagging} (page \pageref{subsubsection: Bagging}), but \hl{adds an important twist to make the trees more diverse (less correlated)}.

\highspace
\textcolor{Green3}{\faIcon{question-circle} \textbf{Why?}} Because \textbf{Bagging reduces variance}, but \hl{if all trees are similar, averaging doesn't help much}. \textbf{Random Forest reduces correlation between trees}, which strengthens the ensemble even further.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{What does Random Forest do differently?}}
\end{flushleft}
Create decorrelated trees using $m < p$, where:
\begin{itemize}
    \item $p$: Total number of predictors (features).
    \item $m$: Number of predictors considered at each split.
\end{itemize}
When growing each tree:
\begin{enumerate}
    \item \textbf{Bagging}: Each tree sees a different bootstrap sample.
    \item \textbf{Random Forest}: In addition, when splitting at each node:
    \begin{itemize}
        \item Only \textbf{a random subset of $m$ predictors} (features) is \textbf{considered}.
        \item Guarantee $m < p$.
    \end{itemize}
    This forces trees to \textbf{make different splits}, even if trained on similar data.
\end{enumerate}
This reduces correlation between trees, making the \textbf{ensemble stronger}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Prediction Mechanism}}
\end{flushleft}
\begin{itemize}
    \item \important{Regression}. Each tree predicts a number, and the final prediction is the \textbf{average} of all trees.
    \begin{equation*}
        \hat{f}_{\text{RF}}(x) = \frac{1}{B} \cdot \sum_{b=1}^B \hat{f}^{(b)}(x)
    \end{equation*}
    \item \important{Classification}. Each tree votes for a class, and the final prediction is \textbf{majority vote}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{square-root-alt} \textbf{Parameters to Set in Random Forest}}
\end{flushleft}
\begin{itemize}
    \item $B$: \textbf{Number of trees} in the forest (typically 100-1000).
    \item $m$: \textbf{Number of predictors considered at each split}. Typical defaults are:
    \begin{itemize}
        \item \textbf{Regression}: $m = \dfrac{p}{3}$
        \item \textbf{Classification}: $m = \sqrt{p}$
    \end{itemize}
    \item $p$: Total number of predictors (given by our data).
\end{itemize}

\newpage

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l p{15em} @{}}
        \toprule
        Feature & Bagging & Random Forest \\
        \midrule
        Bootstrap samples & \textcolor{Green3}{\faIcon{check}} Yes              & \textcolor{Green3}{\faIcon{check}} Yes                         \\ [.5em]
        Random features   & \textcolor{Red2}{\faIcon{times}} No (all $p$ used)  & \textcolor{Green3}{\faIcon{check}} Yes (random subset $m < p$) \\ [.5em]
        Goal              & Reduce variance                                     & Reduce variance $+$\newline \textbf{decorrelate trees}                 \\
        \bottomrule
    \end{tabular}
    \caption{Bagging vs. Random Forest}
\end{table}
