\subsubsection{Recursive Binary Splitting}\label{subsubsection: Recursive Binary Splitting}

\definition{Recursive Binary Splitting} is the algorithm that \textbf{builds a decision tree}, one split at a time.

\highspace
Imagine we have a dataset with two features:
\begin{itemize}
    \item Years (experience)
    \item Hits (performance)
\end{itemize}
And we want to predict Salary. The \textbf{goal} is to split the dataset into \textbf{subset (regions)} where the \textbf{salaries are similar}. Because if salaries are similar, the \textbf{mean is a good predictor}.

\highspace
In general, the goal is to divide the input space into regions $R_1, \dots, R_J$ such that:
\begin{itemize}
    \item The observations within each region are \textbf{as similar as possible}.
    \item This is measured using a \textbf{loss function}, typically \textbf{Residual Sum of Squares (RSS)} in regression.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{``Binary Splitting'' means}}
\end{flushleft}
At every step, the algorithm:
\begin{itemize}
    \item Picks \textbf{one variable} (like Years).
    \item Chooses a \textbf{threshold} (like $\text{Years} < 4.5$).
    \item Splits the data into \textbf{two groups}:
    \begin{itemize}
        \item Left: where the condition is true.
        \item Right: where it is false.
    \end{itemize}
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{``Recursive'' means}}
\end{flushleft}
After the first split:
\begin{itemize}
    \item We \textbf{repeat the same process within each of the two resulting groups}.
    \item Each group is treated like a smaller dataset.
    \item We keep splitting until some stopping condition is met: max depth, too few data points or small error.
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Recap: How it works}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Start with all the data} (entire feature space).
    \item At each step:
    \begin{itemize}
        \item \textbf{Try all variables} $X_j$.
        \item For each variable, \textbf{try all possible split points} $s$.
        \item Evaluate how much RSS is reduced by splitting at $X_j < s$.
    \end{itemize}
    \item Choose the \textbf{best split} (i.e., the one that minimizes total RSS).
    \item \textbf{Split the data} into two regions.
    \item \textbf{Repeat recursively} on each region.
\end{enumerate}
This continues until a maximum tree depth is reached, or a minimum number of observations per leaf is hit, or no split significantly improves performance.

\highspace
Recursive binary splitting is a \textbf{greedy algorithm} because it makes the \textbf{best immediate split} at each step, but it doesn't look ahead. So the tree might be \textbf{pruned later} to improve generalization.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Step & Action \\
        \midrule
        \textbf{Initialization}       & Start with all training data.              \\ [.5em]
        \textbf{Splitting}            & Pick best variable and threshold to split. \\ [.5em]
        \textbf{Evaluation criterion} & Minimize RSS (for regression).             \\ [.5em]
        \textbf{Recursion}            & Reapply splitting to subregions.           \\ [.5em]
        \textbf{Stopping}             & Stop when splits are no longer helpful.    \\
        \bottomrule
    \end{tabular}
\end{table}