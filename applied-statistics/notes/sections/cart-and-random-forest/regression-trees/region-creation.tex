\subsubsection{Region Creation}

In a regression tree, the idea is to divide the \textbf{input feature space} $\mathbb{R}^p$ (all possible combinations of our input features) into \textbf{disjoint rectangular regions} $R_1, R_2, \dots, R_J$.
\begin{itemize}
    \item Each region $R_j$ contains a \textbf{subset of the data}.
    \item For all data points $\mathbf{x}_i \in R_j$, the predicted value is \textbf{the average of their $y_i$ values}.
\end{itemize}
Our \hl{goal} is to \textbf{create regions that are as homogeneous as possible} in their response values.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How are Regions created?}}
\end{flushleft}
Regions are formed through \textbf{binary splits}. At each step, we choose:
\begin{itemize}
    \item A \textbf{feature} $X_j$
    \item A \textbf{threshold} $s$,
    \item And split the data into:
    \begin{itemize}
        \item $R_{\text{left}} = \left\{ X_j < s \right\}$
        \item $R_{\text{right}} = \left\{ X_j \geq s \right\}$
    \end{itemize}
\end{itemize}
This results in \textbf{axis-aligned rectangular regions}, even in higher dimensions.

\begin{examplebox}[: Baseball Case (page \pageref{example: Baseball Salary})]
    Start with all observations in one region.
    \begin{enumerate}
        \item Split on $\text{Years} < 4.5$. Creates two regions called: young vs veteran players.
        \item Then within the left region (young), split on $\text{Hits} < 117.5$: now we have three total regions.
    \end{enumerate}
    These are our final $R_1, R_2, R_3$, each with its own \textbf{mean salary} as the prediction.
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why Regions?}}
\end{flushleft}
Instead of \textbf{fitting one equation}, like linear regression:
\begin{equation*}
    \hat{y} = \beta_0 + \beta_1 X_1 + \beta_2 X_2
\end{equation*}
Regression trees says: Let's \textbf{split the space into zones} (called \textbf{regions}) where the behavior of the data is similar. Inside each zone, we just take the \textbf{average} of the outputs in that zone.

\highspace
A regression tree \textbf{does not try to learn a smooth function} like linear regression. Instead, it approximates $f(X)$ with a \textbf{step function} that is \textbf{constant within each region}. Mathematically:
\begin{equation}
    \hat{f}(x) = \sum_{j=1}^J c_j \cdot \mathbf{1}_{\left\{x \in R_j\right\}}
\end{equation}
Where:
\begin{itemize}
    \item $R_j$: rectangular region.
    \item $c_j$: mean of $y_i$'s in region $R_j$.
    \item $\mathbf{1}_{\{x \in R_j\}}$: indicator function.
\end{itemize}

\highspace
In summary, \textbf{region creation} splits the input space to isolate groups with similar $Y$ values. Each region becomes a \textbf{leaf} in the regression tree. The prediction is the \textbf{average of training responses in the region}.