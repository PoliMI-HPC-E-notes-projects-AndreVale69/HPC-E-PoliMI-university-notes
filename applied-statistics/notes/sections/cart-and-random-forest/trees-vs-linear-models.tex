\subsection{Trees vs. Linear Models}

Comparison in terms of:
\begin{itemize}
    \item \important{Flexibility}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l p{0.33\textwidth} p{0.33\textwidth} @{}}
            \toprule
            Aspect & Linear Models & Decision Trees \\
            \midrule
            Functional Form      & Requires predefined structure (e.g., linear).        & No assumption about form (nonparametric).  \\ [.8em]
            Interaction handling & Must be manually included (e.g., $X_1 \cdot X_2$).   & Automatically captured via splits.         \\ [.8em]
            Nonlinearity         & Hard to model unless transformed manually.           & Easily modeled via recursive partitioning. \\
            \bottomrule
        \end{tabular}
    \end{table}

    \textcolor{Green3}{\faIcon{check-circle}} \textbf{Trees are more flexible}, especially in capturing interactions and nonlinearities.


    \item \important{Accuracy}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} p{8em} p{0.33\textwidth} p{0.33\textwidth} @{}}
            \toprule
            Aspect & Linear Models & Decision Trees \\
            \midrule
            On simple/linear problems   & High accuracy.                                      & May overfit if not pruned.                      \\ [1.5em]
            On complex\newline patterns & Can underfit (high bias).                           & Can adapt better (lower bias, higher variance). \\ [.6em]
            Overall                     & May outperform trees on clean, well-specified data. & Often less accurate unless used in ensembles.   \\ 
            \bottomrule
        \end{tabular}
    \end{table}

    \textcolor{Red2}{\faIcon{exclamation-triangle}} \textbf{Trees can be more accurate} in complex, nonlinear situations, but \textbf{prone to overfitting}. Linear models often perform better when the true relationship is nearly linear.


    \item \important{Interpretability}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} p{6em} p{0.33\textwidth} p{0.33\textwidth} @{}}
            \toprule
            Aspect & Linear Models & Decision Trees \\
            \midrule
            Coefficients                   & Each predictor has an interpretable effect.              & No explicit coefficients, uses decision rules.            \\ [.8em]
            Visual model                   & Hard to visualize with many variables.                   & Tree diagrams\newline are intuitive.                      \\ [.8em]
            Explaining\newline predictions & Linear effect explanations (e.g., $+3$ points for 1 hr). & Human-readable paths (e.g., ``if $\text{age} < 30$...''). \\
            \bottomrule
        \end{tabular}
    \end{table}

    \textcolor{Green3}{\faIcon{check-circle}} \textbf{Trees are easier to explain} to non-experts, because they mimic how humans reason with rules.
\end{itemize}

\newpage

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l l @{}}
            \toprule
            Feature & Linear Models & Decision Trees \\
            \midrule
            Assumptions      & \textcolor{Red2}{\faIcon{shield-alt}} Strong (linearity, additivity) & \textcolor{Green3}{\faIcon{check}} Minimal                         \\ [.3em]
            Flexibility      & \textcolor{Red2}{\faIcon{thumbs-down}} Low                           & \textcolor{Green3}{\faIcon{thumbs-up}} High                            \\ [.3em]
            Interpretability & \textcolor{Orange3}{\faIcon{balance-scale}} Moderate                 & \textcolor{Green3}{\faIcon{check}} High (via tree structure)       \\ [.3em]
            Accuracy         & \textcolor{Green3}{\faIcon{check}} High on simple data               & \textcolor{Green3}{\faIcon{check}} High on complex, nonlinear data \\ [.3em]
            Risk             & \textcolor{Red2}{\faIcon{exclamation-triangle}} Underfitting         & \textcolor{Red2}{\faIcon{exclamation-triangle}} Overfitting (\textcolor{Green3}{\faIcon{check}} if not pruned)     \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Trees vs. Linear Models.}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Pros}} \textbf{and} \textcolor{Red2}{\faIcon{times-circle} \textbf{Cons}} \textbf{of Tree Models}
\end{flushleft}
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
    \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
        \item Intuitive and \textbf{easy to explain}.
        \item Naturally handle \textbf{nonlinearities and interactions}.
        \item Can work with both \textbf{numerical and categorical} variables.
        \item Require \textbf{little preprocessing}.
        \item Can handle \textbf{missing data} and \textbf{collinearity}.
        \item Output is \textbf{readable as a set of rules}.
    \end{itemize}
    \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
    \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
        \item Can easily \textbf{overfit} without pruning or tuning.
        \item Generally \textbf{less accurate} than ensemble methods (Random Forest, Boosting).
        \item Highly \textbf{unstable}: small changes in data $\rightarrow$ very different tree.
        \item Not smooth or differentiable (bad for gradient-based optimization).
        \item \textbf{Hard to generalize} to very high-dimensional spaces.
    \end{itemize}
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/trees/trees-vs-linear-models.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{A situation where decision trees outperform linear models:
    \begin{itemize}
        \item The green curve (\textbf{regression tree}) adapts to the nonlinear pattern in the data.
        \item The red line (\textbf{linear model}) fails to capture the underlying sine wave; it's limited to fitting a straight line.
    \end{itemize}
    This example demonstrates that Trees are better than linear models when the relationship between features and the target is nonlinear and complex.}
\end{figure}