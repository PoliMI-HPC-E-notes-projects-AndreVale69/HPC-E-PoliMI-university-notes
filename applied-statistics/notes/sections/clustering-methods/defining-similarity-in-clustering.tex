\subsection{Defining Similarity in Clustering}

At the heart of clustering lies a simple yet crucial idea: ``objects in the same cluster should be \textbf{more similar} to each other than to objects in other clusters''. But how do we define ``\emph{similar}''? Clustering methods don't work in a vacuum, they \textbf{need a distance} (or similarity) \textbf{function to determine how close two data points are}. This choice directly affects the clustering result.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{Types of Distance Metrics}}
\end{flushleft}
Given two data points $x = \left(x_{1}, x_{2}, \dots, x_{p}\right)$ and $y = \left(y_{1}, y_{2}, \dots, y_{p}\right)$, the distance between them can be computed in different ways, depending on the nature of the data and the goals of the analysis.
\begin{itemize}
    \item \definition{Euclidean Distance}
    \begin{equation}
        d_{E}\left(x,y\right) = \sqrt{\displaystyle\sum_{i=1}^{p} \left(x_{i} - y_{i}\right)^{2}}
    \end{equation}
    \begin{itemize}
        \item Measures straight-line distance.
        \item Works well when \textbf{features are on the same scale}.
        \item Variants: squared euclidean and standardized euclidean (normalize variables before computing).
    \end{itemize}


    \item \definition{Manhattan Distance}
    \begin{equation}
        d_{M}\left(x, y\right) = \displaystyle\sum_{i=1}^{p} \left|x_{i} - y_{i}\right|
    \end{equation}
    Also called \definition{L1 norm}, measures \textbf{grid-like distance} (think of navigating city blocks).


    \item \definition{Chebyshev Distance}
    \begin{equation}
        d_{C}\left(x, y\right) = \max_{i} \left|x_{i} - y_{i}\right|
    \end{equation}
    Takes the \textbf{maximum absolute difference} across dimensions. Sensitive to the \textbf{worst-case} coordinate difference.


    \item \definition{Cosine Similarity (Angle-based)}
    \begin{equation}
        \text{similarity} \left(x,y\right) = \cos\left(\theta\right)
    \end{equation}
    Based on the \textbf{angle between vectors}. The smaller the angle, the more similar the vectors. Common in \textbf{text mining} and \textbf{high-dimensional sparse data}.


    \item \definition{Correlation-Based Distance}
    \begin{equation}
        d_{R}\left(x,y\right) = 1 - \text{corr}\left(x,y\right)
    \end{equation}
    Measures \textbf{shape similarity} and ignores magnitude; focuses on patter of variation.


    \item \definition{Mahalanobis Distance}
    \begin{equation}
        d_{M}\left(x,y\right) = \sqrt{\left(x-y\right)^{T} \Sigma^{-1} \left(x - y\right)}
    \end{equation}
    Accounts for \textbf{correlation between variables}. Useful when features have \textbf{different variances} or \textbf{covariances}.


    \item \definition{Minkowski Distance}
    \begin{equation}
        d_{p}\left(x, y\right) = \left(\displaystyle\sum_{i=1}^{p} \left|x_{i} - y_{i}\right|^{p}\right)^{\frac{1}{p}}
    \end{equation}
    General formula that generalizes Euclidean and Manhattan. If $p=1$ is Manhattan, if $p=2$ is Euclidean.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why Distance Matters}}
\end{flushleft}
\textbf{Clustering doesn't ``know'' what matter, we tell it via distance}.
\begin{itemize}
    \item The \textbf{choice of distance metric} can lead to \textbf{entirely different clusters}.
    \item Different applications require different definitions of similarity. For example, in customer segmentation, cosine similarity may be more appropriate than Euclidean distance, because we care about \emph{purchasing trends}, not \emph{absolute values}.
\end{itemize}
So instead of asking ``\emph{what's the best distance?}'', ask: ``\textbf{\emph{what kind of similarity is meaningful in our application?}}''