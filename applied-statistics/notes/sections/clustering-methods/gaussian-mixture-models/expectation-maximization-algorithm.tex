\subsubsection{Expectation-Maximization (EM) Algorithm}

The \definition{Expectation-Maximization (EM) Algorithm} is how we \textbf{train} a Gaussian Mixture Model (GMM). Meaning, how we \textbf{estimate} the unknown parameters:
\begin{itemize}
    \item Mixing coefficients $\pi_{k}$
    \item Means $\boldsymbol{\mu}_{k}$
    \item Covariances $\Sigma_{k}$
\end{itemize}
We use the EM algorithm \textbf{due to the unknown cluster labels}, $z_{i}$. The data is incomplete, so we cannot group the points by component. The EM algorithm usually works when some variables are latent (hidden).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How EM Works (Big Picture)}}
\end{flushleft}
EM is an \textbf{iterative algorithm} with two main steps:
\begin{enumerate}
    \item \important{E-step (Expectation)}. In this step, we \textbf{compute responsibilities}:
    \begin{equation*}
        \gamma_{ik} = P\left(z_i = k \mid \mathbf{x}_i\right) = \frac{\pi_k \cdot \mathcal{N}\left(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \Sigma_k\right)}{\displaystyle\sum_{j=1}^K \pi_j \cdot \mathcal{N}\left(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \Sigma_j\right)}
    \end{equation*}
    This is the \textbf{expected value of the latent variable} $z_{i}$, given the current parameter estimates. In simpler terms: we update our guess of how much each cluster $k$ is responsible for each point $\mathbf{x}_{i}$.
    
    \item \important{M-step (Maximization)}. In this step, we \textbf{update the model parameters} using the responsibilities:
    \begin{enumerate}
        \item \textbf{Update weights} $\pi_k$:
        \begin{equation*}
            \pi_k = \dfrac{1}{n} \cdot \displaystyle\sum_{i=1}^{n} \gamma_{ik}
        \end{equation*}

        \item \textbf{Update means} $\boldsymbol{\mu}_k$:
        \begin{equation*}
            \boldsymbol{\mu}_k = \dfrac{
                \displaystyle\sum_{i=1}^{n} \gamma_{ik} \cdot \mathbf{x}_i
            }{
                \displaystyle\sum_{i=1}^{n} \gamma_{ik}
            }
        \end{equation*}

        \item \textbf{Update covariances} $\Sigma_k$:
        \begin{equation*}
            \Sigma_k = \dfrac{
                \displaystyle\sum_{i=1}^{n} \gamma_{ik} \cdot \left(\mathbf{x}_i - \boldsymbol{\mu}_k\right) \cdot \left(\mathbf{x}_i - \boldsymbol{\mu}_k\right)^{T}
            }{
                \displaystyle\sum_{i=1}^{n} \gamma_{ik}
            }
        \end{equation*}
    \end{enumerate}
    We're \textbf{maximizing the likelihood} of the data under the current soft assignments.
\end{enumerate}
We alternate E and M steps \textbf{until the log-likelihood stops improving significantly}, or we \textbf{reach a maximum number of iterations}. This gives us the best estimate of the parameters, despite not knowing the cluster labels.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What EM is really doing}}
\end{flushleft}
Each iteration:
\begin{itemize}
    \item Uses the \textbf{E-step} to ``\emph{guess}'' hidden information (soft labels).
    \item Uses the \textbf{M-step} to ``\emph{retrain}'' the model based on those guesses.
\end{itemize}
That's why it's called \textbf{Expectation-Maximization}: 
\begin{itemize}
    \item We \textbf{expect} the \hl{missing values}.
    \item Then \textbf{maximize} the \hl{likelihood given that expectation}.
\end{itemize}