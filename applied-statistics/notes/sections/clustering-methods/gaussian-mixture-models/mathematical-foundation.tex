\subsubsection{Mathematical Foundation}\label{subsubsection: Mathematical Foundation}

Even though we have already introduced the GMM formula, we provide a thorough explanation here.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is a Mixture Model?}}
\end{flushleft}
A \textbf{mixture model} assumes that our data was generated from \textbf{multiple probability distributions}, not just one. For GMMs, we assume: each data point $\mathbf{x}_i \in \mathbb{R}^d$ was \textbf{generated by one of several multivariate Gaussian distributions}. But we don't know:
\begin{itemize}
    \item \emph{Which point came from which component.}
    \item \emph{What the component parameters are.}
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{GMM Definition}}
\end{flushleft}
The probability density function of a Gaussian Mixture Model with $K$ components is:
\begin{equation}
    p\left(\mathbf{x}\right) = \sum_{k=1}^{K} \pi_{k} \cdot \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}, \Sigma_{k}\right)
\end{equation}
Where:
\begin{itemize}
    \item $\mathbf{x} \in \mathbb{R}^{d}$ is the observed data point.
    \item $\pi_{k}$ is the \textbf{mixing coefficient} for component $k$, with $\displaystyle\sum_{k=1}^{K} \pi_{k} = 1$, and $\pi_{k} \geq 0$.
    \item $\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_{k}, \Sigma_{k})$ is the \textbf{multivariate Gaussian density} of the $k$-th component, with:
    \begin{itemize}
        \item Mean $\boldsymbol{\mu}_{k} \in \mathbb{R}^{d}$
        \item Covariance matrix $\Sigma_{k} \in \mathbb{R}^{d \times d}$
    \end{itemize}
    This is the \textbf{probability density} of the Multivariate Gaussian Distribution, also called the \textbf{likelihood of observing the point} $\mathbf{x}$, \textbf{given the parameters of component} $k$ (mean $\mu$ and covariance $\Sigma$).

    For example, imagine we're standing at a point $\mathbf{x}$ in space, and asking: \emph{how likely is it that a point like this came from Gaussian cluster $k$?} The answer is:
    \begin{equation*}
        \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \Sigma_k)
    \end{equation*}
    The value is \textbf{higher} if $\mathbf{x}$ is \textbf{closer to the center} of cluster $k$, and \textbf{lower} if it's farther away.
\end{itemize}

\newpage

\noindent
The meaning of each term is as follows:
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Symbol                                                      & Meaning                                                               \\
        \midrule
        $\pi_{k}$                                                   & Prior probability of choosing cluster $k$.                            \\ [.5em]
        $\boldsymbol{\mu}_{k}$                                      & Mean (center) of the $k$-th Gaussian component.                       \\ [.5em]
        $\Sigma_{k}$                                                & Covariance (shape and orientation) of the $k$-th component.           \\ [.5em]
        $\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \Sigma_k)$ & Likelihood of $\mathbf{x}$ under the $k$-th Gaussian.                 \\ [.5em]
        $p(\mathbf{x})$                                             & Overall probability of observing $\mathbf{x}$ under the full model.   \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
To generate a data point $\mathbf{x}$, first choose a component $k$ with probability $\pi_k$, then draw $\mathbf{x}$ from the Gaussian $\mathcal{N}(\boldsymbol{\mu}_k, \Sigma_k)$. This is why GMM is a generative model, \textbf{it models how data could have been generated}.