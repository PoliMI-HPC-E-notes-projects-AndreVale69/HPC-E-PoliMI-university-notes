\subsubsection{Responsibilities}

Unlike \textbf{K-Means}, where each point is assigned to exactly \textbf{one cluster} (hard clustering), \textbf{GMM} performs \textbf{soft clustering}: each point has a \hl{probability of belonging to every cluster}. These probabilities reflect \emph{how likely} it is that a point came from each component of the mixture.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why do we need to introduce Responsibilities in GMM?}}
\end{flushleft}
\textcolor{Red2}{\faIcon{exclamation-triangle}} We have a \textcolor{Red2}{\textbf{problem}} at the moment. When using a Gaussian Mixture Model, we assume that each data point $\mathbf{x}_{i}$ was generated by \textbf{one of the $K$ Gaussian components}. But, we \textbf{don't know which one}. That information is \textbf{hidden} or \textbf{latent}. We only observe:
\begin{itemize}
    \item The data point $\mathbf{x}_{i}$
    \item The parameters of the Gaussian $\left(\pi_{k}, \boldsymbol{\mu}_{k}, \Sigma_{k}\right)$
\end{itemize}
We don't observe which component $k$ generated $\mathbf{x}_{i}$.

\highspace
\textcolor{Red2}{\textbf{\emph{I still don't understand what the problem is.}}} In a Gaussian Mixture Model, we assume that each data point $\mathbf{x}_{i}$ is generated by \textbf{one of the $K$ Gaussian components}. So the true process that generates each point is:
\begin{enumerate}
    \item Randomly choose a component $k$ with probability $\pi_{k}$.
    \item Generate a point $\mathbf{x}_{i}$ from the Gaussian $\mathcal{N}\left(\mu_{k}, \Sigma_{k}\right)$
\end{enumerate}
But we don't know which Gaussian generated each point and this is the core issue:
\begin{itemize}
    \item We \textbf{observe only the data points} $\mathbf{x}_{1}, \dots, \mathbf{x}_{n}$.
    \item We \textbf{do not observe the ``label''} of which component (cluster) generated each point.
\end{itemize}
That \textbf{label is missing}! This is problematic because, if we want to \textbf{train a model} (i.e., estimate the $\pi_{k}$, $\mu_{k}$, and $\Sigma_{k}$), we would \textbf{ideally want to group the data by component}. However, we can't do that because we don't know which point belongs to which component.

\highspace
\textcolor{Red2}{\textbf{\emph{Okay, but if we're using the GMM formula, then we know everything about each cluster that produces each point.}}} That's a good point, but the practice is slightly different. In the real world, especially in machine learning, we \hl{don't know the origin of each point because we only have the observed data}. So, we are trying to \textbf{infer} the underlying \textbf{clusters}, \textbf{without knowing} which one generated each point.

\begin{examplebox}[: analogy of the GMM problem]\label{example: analogy of the GMM problem}
    Imagine a factory that has 3 machines (A, B, C) making cookies. Each machine:
    \begin{itemize}
        \item Adds a different amount of sugar and chocolate
        \item Sometimes makes similar-looking cookies, but not exactly the same
        \item Has a different production rate (e.g., Machine A makes 50\% of all cookies, B makes 30\%, C makes 20\%)
    \end{itemize}
    Now, someone gives us a box of mixed cookies and says: ``\emph{These were made using machines A, B, and C, but I'm not telling you which cookie came from which machine.}'' We taste each cookie and write down how sweet and chocolatey it is. That's our data.

    \highspace
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{The Problem.}} We don't know:
    \begin{itemize}
        \item Which machine made each cookie.
        \item How sweet/chocolatey each machine's recipe really is.
        \item How often each machine is used.
    \end{itemize}
    In GMM terms:
    \begin{itemize}
        \item We have the data $\mathbf{x}_{1}, \dots, \mathbf{x}_{n}$ (cookie features).
        \item But the label ``\emph{this came from machine A}'', is hidden.
    \end{itemize}

    \textcolor{Green3}{\faIcon{check-circle} \textbf{The Solution.}} Since we can't know for sure where each cookie came from, we say: ``\emph{Let me estimate how likely it is that this cookie came from each machine.}''. So for Cookie \#1, we might say:
    \begin{itemize}
        \item 70\% Machine A
        \item 20\% Machine B
        \item 10\% Machine C
    \end{itemize}
    This set of numbers is the responsibility vector for Cookie \#1. We do this for every cookie. Now, even though we don't know the true labels, we have soft guesses, and that's enough to:
    \begin{itemize}
        \item Estimate what each machine's recipe looks like (mean and variance).
        \item Estimate how many cookies each machine probably made (mixing proportions).
    \end{itemize}

    The core idea is that we need ``responsibilities'' because the cluster label is missing. Rather than guessing the labels directly, we estimate the probability that each point belongs to each cluster. These probabilities are called ``responsibilities''.
\end{examplebox}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Solution: Responsibilities $=$ Posterior Probabilities}}
\end{flushleft}
We come back to GMM. We assume that:
\begin{enumerate}
    \item Each point $\mathbf{x}_{i}$ was generated by \textbf{one of $K$ Gaussians}.
    \item But we \textbf{don't observe} which one.
\end{enumerate}
So we introduce a \textbf{latent variable} $z_{i} \in \{1, \dots, K\}$ that \textbf{represents the unknown cluster} of $\mathbf{x}_{i}$ (hidden cluster assignments).

\highspace
We introduce the idea of a \textbf{latent variable} $z_{i}$, which says: ``let's pretend each point $\mathbf{x}_{i}$ has a hidden label $z_{i} \in \left\{1, \dots, K\right\}$''. But since we don't observe $z_{i}$, we'll estimate the \textbf{probability} that it takes each value.

\highspace
A \definition{Latent Variable} is a variable that:
\begin{enumerate}
    \item \textbf{Exists conceptually}, and
    \item \textbf{Affects the data}, but
    \item Is \textbf{not directly observed} in our dataset
\end{enumerate}
In other words, a latent variable is something we don't see, but it explains patterns in the things we do see.

\begin{examplebox}[: continuation of the example on page \pageref{example: analogy of the GMM problem}]
    Let's go back to our cookie story.
    \begin{itemize}
        \item We \textbf{see the cookie} $\Rightarrow$ that's observed data.
        \item We \textbf{don't see the machine} that made it $\Rightarrow$ that's a \textbf{latent variable}.
    \end{itemize}
    The \textbf{machine label} (e.g. A, B, or C) is a latent variable $z$ because:
    \begin{enumerate}
        \item It determines the cookie's sweetness/chocolate level.
        \item But we aren't told what it is, we must infer it.
    \end{enumerate}
\end{examplebox}

\noindent
Since we cannot observe $z_{i}$ directly, we can \textbf{estimate the probability} that $z_{i} = k$ for each component $k$. This estimate is called \definition{Responsibility} and is mathematically expressed as follows:
\begin{equation}
    \gamma_{ik} = P\left(z_{i} = k \mid \mathbf{x}_i\right)
\end{equation}
We are trying to \emph{approximate} the unknown values of $z_{i}$. This enables us to \textbf{learn from incomplete data} without needing the true labels.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | l @{}}
        \toprule
        \textbf{Latent Variable}    & \textbf{Responsibility} \\
        \midrule
        Missing truth               & Out best guess of that truth. \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\noindent
The responsibility is computed using \textbf{Bayes' Rule}:
\begin{equation}
    \gamma_{ik} = \dfrac{
        \pi_k \cdot \mathcal{N}\left(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \Sigma_k\right)
    }{
        \displaystyle\sum_{j=1}^{K} \pi_{j} \cdot \mathcal{N}\left(\mathbf{x}_{i} \mid \boldsymbol{\mu}_{j}, \Sigma_{j}\right)
    }
\end{equation}
In simple terms, we have:
\begin{itemize}
    \item The numerator is: how well component $k$ explains point $\mathbf{x}_{i}$, weighted by how likely that component is in general.
    \item The denominator ensures that the total probability sums to 1 (normalization across all components).
\end{itemize}

\highspace
The \textbf{main idea} behind GMM is very simple. Rather than stating that ``point $i$ is in cluster 2'', GMM says that point $i$ is 80\% likely to be in cluster 2, 15\% likely to be in cluster 1, and 5\% likely to be in cluster 3. These values are the \textbf{responsibilities} $\gamma_{ik}$. The word \emph{responsibility} reflects that \textbf{each component is partially responsible} for explaining each point.