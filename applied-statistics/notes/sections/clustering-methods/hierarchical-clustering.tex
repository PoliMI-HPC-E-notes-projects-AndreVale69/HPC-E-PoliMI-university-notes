\subsection{Hierarchical Clustering}

\definition{Hierarchical Clustering} is a \textbf{method of clustering that builds a hierarchy of clusters}, either by \textbf{merging} smaller clusters into larger ones (bottom-up) or \textbf{splitting} a large cluster into smaller ones (top-down).
\begin{itemize}
    \item \definition{Agglomerative Clustering (Bottom-Up)}
    \begin{enumerate}
        \item Starts with \textbf{each data point as its own cluster}
        \item At each step:
        \begin{enumerate}
            \item Find the \textbf{two closest clusters}
            \item \textbf{Merge} them into one
            \item \textbf{Repeat} until there's one single cluster or a predefined number $k$
        \end{enumerate}
    \end{enumerate}
    This is the most common form of hierarchical clustering.
    
    \item \definition{Divisive Clustering (Top-Down)}
    \begin{enumerate}
        \item Starts with \textbf{all points in one large cluster}
        \item At each step:
        \begin{enumerate}
            \item \textbf{Split} one cluster into two
            \item \textbf{Continue recursively} until each point is in its own cluster (or until $k$ is reached)
        \end{enumerate}
    \end{enumerate}
    This method is less commonly used due to higher complexity.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Implementation Agglomerative Hierarchical Clustering}}
\end{flushleft}
Hierarchical clustering works with a \textbf{distance matrix}, which \textbf{contains the distance between every pair of observations}. We don't need to specify $k$ beforehand, but we need a \textbf{stopping criterion} (e.g., stop when there are $k$ clusters, or use a threshold on distance).
\begin{enumerate}
    \item Compute the \textbf{proximity matrix} (distances between all points)
    \item Let each point be its \textbf{own cluster}
    \item \textbf{Repeat}:
    \begin{enumerate}
        \item Find and \textbf{merge the two closest clusters}
        \item \textbf{Update} the distance matrix
    \end{enumerate}
    \item \textbf{Stop} when only one cluster remains (or another stopping condition is met)
\end{enumerate}
The behavior of step 3, how we defined ``closest clusters'', depends on the \textbf{linkage method}.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{link} \textbf{Linkage Methods: How we Merge Clusters}}
\end{flushleft}
The \textbf{linkage method} defines the distance between two clusters, based on the distances between points in those clusters.
\begin{enumerate}
    \item \definition{Single Linkage Method}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Distance}}. It is the \textbf{minimum distance} between any two points (nearest neighbors)
        \item[\textcolor{Green3}{\faIcon{check-circle}}] Can handle \textbf{non-globular shapes}.
        \item[\textcolor{Red2}{\faIcon{exclamation-triangle}}] Sensitive to \textbf{noise and chaining effects}.
    \end{itemize}

    \item \definition{Complete Linkage Method}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Distance}}. It is the \textbf{maximum distance} between any two points (furthest points).
        \item[\textcolor{Green3}{\faIcon{check-circle}}] Less sensitive to noise, but biased toward \textbf{spherical or compact clusters} (we mean that the method tends to favor or naturally produces clusters that have a specific shape or property).
    \end{itemize}

    \item \definition{Average Linkage Method}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Distance}}. It is the \textbf{average distance} between all points pairs.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] Uses \textbf{centroids} as reference and offers a \textbf{balance} between single and complete.
    \end{itemize}
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Pros}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item Does not require predefining the number of clusters $k$.
    \item \textbf{Produces a dendrogram}\footnote{%
        The dendrogram is a tree where the leaves are individual data points and branches are merges between clusters. To choose $k$, ``cut'' the dendrogram at the level where the structure is stable (e.g., large vertical gaps suggest good splits).
    }, a tree-like diagram showing how clusters are formed.
    \item Can reveal structure at \textbf{multiple levels of granularity}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{times-circle} \textbf{Limitations and Challenges}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textbf{Computational complexity}: distance matrix is $O(n^{2})$, problematic for large datasets.
    \item Once a merge/split is made, it \textbf{cannot be undone}.
    \item There's \textbf{no direct optimization} of a global objective (unlike k-means minimizing SSE)  .
    \item \textbf{Results depend heavily on the linkage method used}, can lead to very different clusterings.
\end{itemize}