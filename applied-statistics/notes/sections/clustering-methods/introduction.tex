\section{Clustering Methods}

\subsection{Introduction}

In the previous section, we talked about PCA for compression or visualization. However, this topic falls under a type of technique called unsupervised learning.
\begin{itemize}
    \item \definition{Supervised Learning} is a type of machine learning where the algorithm is trained on a labeled dataset, meaning \textbf{each} training \textbf{example includes both the input data and the correct output}.
    
    The \textbf{goal} is to learn a function that maps inputs to outputs, in order to make predictions on new, unseen data. Typical tasks include:
    \begin{itemize}
        \item Classification (e.g., spam detection, image recognition)
        \item Regression (e.g., predicting house prices or credit scores)
    \end{itemize}
    

    \item \definition{Unsupervised Learning} is a type of machine learning where the algorithm is given \textbf{only input data without any labeled output}.
    
    The \textbf{objective} is to identify patterns, structures or groupings within the data. Common applications include:
    \begin{itemize}
        \item Clustering (this section)
        \item Dimensionality Reduction (e.g., PCA for compression or visualization, section \ref{subsection: Dimensionality Reduction}, page \pageref{subsection: Dimensionality Reduction})
    \end{itemize}
\end{itemize}
In essence, while supervised learning is about \emph{prediction}, unsupervised learning is about \emph{discovery}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is Clustering?}}
\end{flushleft}
\definition{Clustering} is the \hl{process of grouping a set of objects} in such a way that \textbf{objects within the same group (cluster)} are \textbf{more similar} to each other than to those in other groups.
\begin{itemize}
    \item No ``true'' labels are provided (unsupervised learning), the objective is to \textbf{uncover structure}.
    \item Often used in exploratory data analysis.
    \item The notion of ``similarity'' is fundamental and context-dependent (commonly based on \textbf{distance measures}).
\end{itemize}
The basic idea of clustering is:
\begin{enumerate}
    \item \important{Minimize intra-cluster distances}: member of the \textbf{same cluster} should be \textbf{close to each other}.
    \item \important{Maximize inter-cluster distances}: \textbf{different clusters} should be \textbf{well separated}.
\end{enumerate}

\begin{examplebox}[: Energy Consumption in Milan]
    A practical example:
    \begin{itemize}
        \item Consider $n$ users, each described by their energy consumption across $p$ time slots.
        \item The task is to identify \textbf{groups of users with similar consumption patterns}, potentially revealing roles like \emph{residences vs offices} or \emph{daytime vs nighttime} usage.
        \item Since there is no predefined label, this is a classic unsupervised learning task.
    \end{itemize}
    This example encapsulates the \textbf{essence of clustering}, discovering structure in data that wasn't explicitly labeled.
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How Many Clusters?}}
\end{flushleft}
A fundamental \textbf{challenge} in clustering is \textbf{deciding the number of clusters} ($k$). Visualizations show that data can often be reasonable clustered in 2, 4, or 6 groups, depending on the chosen definition or similarity.

\highspace
The \textbf{clustering result depends on the similarity measure} (e.g., Euclidean distance), and the notion of a ``true'' cluster is often ambiguous. This ambiguity is a central theme in unsupervised learning: since there's no ground truth, \textbf{evaluating the ``correctness'' of a clustering is inherently difficult}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{Types of Clustering}}
\end{flushleft}
There are two main paradigms:
\begin{enumerate}
    \item \important{Hierarchical Clustering}: builds a tree of clusters. Two main strategies:
    \begin{itemize}
        \item \textbf{Agglomerative}: start from individual points and merge them.
        \item \textbf{Divisive}: start from the whole dataset and split it recursively/
    \end{itemize}

    \item \important{Partitional Clustering} (also called flat clustering): divides data into $k$ non-overlapping groups. Each data point belongs to exactly one cluster.
\end{enumerate}
Hierarchical methods are more interpretable, while partitional methods (e.g., k-means) are often faster and scalable.