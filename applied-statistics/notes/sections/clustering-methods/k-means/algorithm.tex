\subsubsection{Algorithm}

The K-Means algorithm is an iterative method that alternates between assigning data points to clusters and updating the cluster centroids, with the goal of minimizing within-cluster variability.
\begin{enumerate}
    \setcounter{enumi}{-1}
    \item \important{Choose the number of clusters $K$}. Before running the algorithm, we must decide the number of clusters $K$. Each cluster will have:
    \begin{itemize}
        \item A \textbf{set of points} $C_{1}, \dots, C_{K}$
        \item A \textbf{centroid} $\mathbf{v}_{1}, \dots, \mathbf{v}_{K}$
        \item A \textbf{partition matrix} $\Gamma = \left[\gamma_{ij}\right]$ where:
        \begin{equation*}
            \gamma_{ij} = \begin{cases}
                1 & \text{if } \mathbf{x}_{j} \in C_{i} \\
                0 & \textbf{otherwise}
            \end{cases}
        \end{equation*}
    \end{itemize}


    \item \important{Initialization}. Randomly assign each observation to one of the $K$ clusters. Then compute the \textbf{initial centroids} for each cluster:
    \begin{equation*}
        \mathbf{v}_{k} = \dfrac{1}{\left|C_{k}\right|} \displaystyle\sum_{\mathbf{x}_{i} \in C_{k}} \mathbf{x}_{i}
    \end{equation*}
    These are the \textbf{mean vectors} of the assigned points.

    
    \item \important{Assignment}. For each point $\mathbf{x}_{i}$, compute the distance to all centroids, and assign it to the cluster with the \textbf{closest centroid}. This updates the \textbf{partition matrix} $\Gamma$.
    
    \item \important{Update Centroids}. After the new assignments, \textbf{recompute the centroid} of each cluster using the updated points:
    \begin{equation*}
        \mathbf{v}_{k} = \dfrac{1}{\left|C_{k}\right|} \displaystyle\sum_{\mathbf{x}_{i} \in C_{k}} \mathbf{x}_{i}
    \end{equation*}
\end{enumerate}
\textbf{Repeat Step 2 and Step 3} until:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item No points change cluster (\hl{convergence}), or
    \item A \hl{stopping criterion} is met (e.g., max iterations)
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Properties of the algorithm}}
\end{flushleft}
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Good}}
    \begin{itemize}
        \item The algorithm \textbf{always converges} (it stabilizes after a finite number of iterations)
        \item Simple and fast
    \end{itemize}

    \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Bad}}
    \begin{itemize}
        \item It does \textbf{not always find the global optimum}
        \item The \textbf{results depends} heavily on the \textbf{initialization}
    \end{itemize}
\end{itemize}
In practice, K-means clustering is \textbf{typically run multiple times with different random initializations}, and the \textbf{result with the lowest cost is kept}.