\subsubsection{Definition}

\definition{K-Means} is a \textbf{unsupervised learning algorithm} used to partition a dataset into $K$ \textbf{distinct non-overlapping groups} called \textbf{clusters}, where each observation belongs to the cluster with the nearest mean (called the \definitionWithSpecificIndex{centroid}{K-Means centroid}{}).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{bullseye} \textbf{Main Goal}}
\end{flushleft}
The goal of K-Means is to find a partition of the data that \textbf{minimizes the total within-cluster variation}, usually measured as the \textbf{sum of squared Euclidean distances} between each point and the centroid of its assigned cluster.

\highspace
We can express the previous goals in raw mathematical terms as follows:
\begin{equation}
    \min_{C_{1}, ..., C_{K}} \:\: \displaystyle\sum_{k=1}^{K} \:\: \sum_{\mathbf{x}_i \in C_k} \|\mathbf{x}_i - \boldsymbol{\mu}_k\|^{2}
\end{equation}
Where:
\begin{itemize}
    \item $C_{k}$ is the \textbf{set of points} in the $k$-th cluster.
    \item $\boldsymbol{\mu}_{k}$ is the \textbf{centroid} (mean vector) of cluster $C_{k}$.
    \item $\|\mathbf{x}_i - \boldsymbol{\mu}_k\| = \| \cdot \|$ denotes \textbf{Euclidean distance}. It is the quantity that K-Means tries to minimize over all clusters.
\end{itemize}

\begin{examplebox}[: K-Means]
    Below is a simple run of the K-means algorithm on a random dataset.
    \begin{itemize}
        \item Iteration 0 - \textbf{Initialization}
        \begin{center}
            \includegraphics[width=.8\textwidth]{img/k-means/iter_0.pdf}
        \end{center}
        This is the starting point of the K-Means algorithm. \textbf{Three centroids are randomly placed in the feature space}. At this point, no data points are assigned to clusters yet, or all are assumed to be uncolored/unclustered. The positions of the centroids will strongly influence how the algorithm proceeds.

        The goal here is to start with some guesses. The next step will use these centroids to form the initial clusters.


        \item Iteration 1 - \textbf{First Assignment and Update}
        \begin{center}
            \includegraphics[width=.7\textwidth]{img/k-means/iter_1.pdf}
        \end{center}
        Each data point is assigned to the closest centroid, forming the first version of the clusters. New centroids are computed by taking the average of the points in each cluster. We can already see structure forming in the data, as points begin grouping around centroids.

        This step is the first real clustering, and centroids begin to move toward dense regions of data.


        \item Iteration 2 - \textbf{Re-Assignment and Refinement}
        \begin{center}
            \includegraphics[width=.7\textwidth]{img/k-means/iter_2.pdf}
        \end{center}
        Clusters are recomputed based on updated centroids. Many points remain in the same clusters, but some may shift to a new cluster if a centroid has moved. Centroids continue moving closer to the center of their respective groups.

        The algorithm is now refining the clusters and reducing the total distance from points to centroids.


        \item Iteration 3 - \textbf{Further Convergence}
        \begin{center}
            \includegraphics[width=.7\textwidth]{img/k-means/iter_3.pdf}
        \end{center}
        At iteration 3, the K-Means algorithm reached convergence. The centroids no longer moved, and no points changed cluster.This means:
        \begin{itemize}
            \item The algorithm has found a locally optimal solution.
            \item Further iterations would not improve or change the clustering.
            \item The final configuration is considered the result of the algorithm.
        \end{itemize}
        In practice, this is how K-Means stops: it checks whether the centroids remain unchanged, and if so, it terminates automatically.
    \end{itemize}
\end{examplebox}