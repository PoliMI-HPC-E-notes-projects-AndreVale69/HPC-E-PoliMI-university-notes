\subsubsection{Initialization Issues in K-Means}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why initialization matters}}
\end{flushleft}
The K-Means algorithm starts by choosing $K$ \textbf{initial centroids}. These \textbf{starting positions are crucial} because K-Means builds the entire clustering process based on them.

\highspace
Since K-Means is an \textbf{iterative algorithm}, it gradually improves the clusters from where it begins. However, if it starts from a bad configuration (i.e. bad initial centroids), it can easily get stuck in a \textbf{local minimum}, a clustering solution that isn't optimal but looks good from that starting point.

\highspace
This means that:
\begin{itemize}
    \item \textbf{Different initial centroids} can lead to \textbf{different final results}.
    \item Some clusters ma be poorly formed or even empty.
    \item The quality of the final clustering can vary, even on the same data.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{What can go wrong}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Centroids too close}: if initial centroids are placed near each other, they might all end up \hl{capturing the same group of points}, leaving \hl{other areas underrepresented}.
    \item \textbf{Unbalanced clusters}: a poor start can lead to \hl{clusters with very different sizes or shapes}, even if the data has nicely separated groups.
    \item \textbf{Empty clusters}: it's possible that \hl{some centroids end up with no points assigned}, which breaks the algorithm's assumptions.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{How to improve initialization (conceptually)}}
\end{flushleft}
To avoid these problems, two common strategies are used:
\begin{enumerate}
    \item \textcolor{Green3}{\textbf{Multiple random initializations}}. Instead of relying on just one random start, K-Means is \textbf{run several times with different initial centroids}. The \textbf{result with the lowest total distance is selected}. This increases the change of finding a good clustering.
    
    \item \textcolor{Green3}{\textbf{Smart initialization}}. Instead of choosing all centroids randomly, a better approach is to choose the \hl{first centroid randomly}, then pick the \hl{next one in a way that they are \textbf{as far apart as possible}}. This increases the chances of covering different regions of data from the beginning.
    
    This smarter approach significantly reduces the risk of bad clustering and makes the algorithm more stable and reliable.
\end{enumerate}