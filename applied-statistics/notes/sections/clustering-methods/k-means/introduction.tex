\subsection{K-Means}

\subsubsection{Introduction}

\definition{K-Means} is one of the most popular \textbf{clustering method}, specifically a type of \textbf{partitional clustering} (page \pageref{def: Partitional Clustering}). It's used to \textbf{group data into distinct clusters based on similarity}.

\highspace
K-Means divides a dataset into $K$ groups, where:
\begin{itemize}
    \item Each group is called a \textbf{cluster}.
    \item \textbf{Each cluster} has a \textbf{center} called \definitionWithSpecificIndex{centroid}{K-Means centroid}{}.
    \item Every data point belongs to the cluster with the \textbf{nearest centroid}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How does it work?}}
\end{flushleft}
The K-means method can be divided into \textbf{five steps}:
\begin{enumerate}
    \item \textbf{Choose} $K$ (the number of clusters).
    \item \textbf{Randomly assign} each data point to a cluster.
    \item \textbf{Compute the centroid} of each cluster (the mean of all its points).
    \item \textbf{Reassign} each \textbf{point to the cluster} with the closest centroid.
    \item \textbf{Repeat} steps 3 and 4 \textbf{until} the points \textbf{no longer change} clusters (convergence criterion).
\end{enumerate}
The algorithm tries to \textbf{minimize the distance} between each point and the center of its cluster. This means we want the \textbf{clusters to be as compact} and \textbf{well-separated as possible}.

\highspace
Note that this is only a simple, quick introduction to K-Means clustering. In the following chapters, we will delve deeply into each topic.