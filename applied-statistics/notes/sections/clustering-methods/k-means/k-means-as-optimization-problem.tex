\subsubsection{K-Means as an Optimization Problem}

Given:
\begin{itemize}
    \item A set $X = \left\{\mathbf{x}_{1}, \dots, \mathbf{x}_{n}\right\}$ of $n$ data points in $\mathbb{R}^{d}$
    \item A chosen number of clusters $K$
\end{itemize}
The goal of K-Means is to \textbf{partition the data into $K$ clusters}:
\begin{equation*}
    C = \left\{C_{1}, C_{2}, \dots, C_{K}\right\}   
\end{equation*}
So that a measure of \textbf{within-cluster variability} is \textbf{minimized}.

\highspace
\begin{flushleft}\label{def: within-cluster variability}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is Within-Cluster Variability?}}
\end{flushleft}
In simple terms:
\begin{itemize}
    \item \textbf{Cluster}: a group of points that are similar (close together).
    \item \textbf{Variability}: how spread out the points are.
    \item \definition{Within-Cluster Variability} how spread out the points are \textbf{inside each cluster}.
\end{itemize}
So within-cluster variability measures \textbf{how close the points in the same cluster are to each other}. We have \hl{low variability} when the \hl{points are tightly} packed and \hl{high variability} when the \hl{points are spread out}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Okay, but why do we need to minimize within-cluster variability?}}
\end{flushleft}
Because a \textbf{good cluster} should contain \textbf{points that are close together}, not randomly scattered. So K-Means algorithm tries to form clusters where the \textbf{internal ``spread'' is as small as possible}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Objective Function: what are we minimizing?}}
\end{flushleft}
For each cluster $C_{k}$, define its \textbf{centroid} $\mathbf{c}_{k}$ as the \textbf{mean of the points in the cluster}. The within-cluster variability is measured by the following:
\begin{equation}
    W\left(C_{k}\right) = \dfrac{1}{\left|C_{k}\right|} \displaystyle\sum_{i, i' \in C_k} \:\: \sum_{j=1}^{d} \left(x_{ij} - x_{i'j}\right)^{2}
\end{equation}
\begin{itemize}
    \item Each observation is a point:
    \begin{equation*}
        \mathbf{x}_{i} = \left(x_{i1}, x_{i2}, \dots, x_{id}\right) \in \mathbb{R}^{d}
    \end{equation*}
    That means $\mathbf{x}_{i}$ is a vector with $d$ features.

    \item $\displaystyle\sum_{i, i' \in C_k}$ means we are looking at \textbf{all pairs of points} $i$ and $i'$ inside the same cluster $C_{k}$.
    
    \item $\displaystyle \sum_{j=1}^{d} \left(x_{ij} - x_{i'j}\right)^{2}$, for each feature $j$, we compute the \textbf{squared distance} between point $i$ and point $i'$.

    \item $\displaystyle\sum_{i, i' \in C_k} \displaystyle \sum_{j=1}^{d} \left(x_{ij} - x_{i'j}\right)^{2}$ measures the \textbf{total squared distance} between \textbf{all point pairs inside the cluster}.

    \item $\dfrac{1}{\left|C_{k}\right|}$, we divide by the number of points to get an \textbf{average distance}.
\end{itemize}
So $W\left(C_{k}\right)$ is the \textbf{average squared distance between all pairs of points inside cluster $C_{k}$}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{\speedIcon} \textbf{Optimization Criterion}}
\end{flushleft}
The full K-Means objective is to \textbf{find the clustering} that \textbf{minimizes the total within-cluster variability}:
\begin{equation}
    \min_{C_{1}, \dots, C_{K}} \displaystyle\sum_{k=1}^{K} W\left(C_{k}\right)
\end{equation}
This means we are searching for the best possible partition $\left\{C_{1}, \dots, C_{K}\right\}$ that results in clusters where \textbf{points are as close as possible to each other}, i.e., the clusters are compact.

\highspace
In other words, we want \textbf{every cluster to be compact}. So we add up the within-cluster variability for all clusters. We \hl{minimize the problem while keeping it as compact as possible}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Interpretation}}
\end{flushleft}
A \textbf{good clustering} has \textbf{low within-cluster variability}: points in the same cluster are close together. Also, K-Means solves an optimization problem where it tries to find such a clustering by minimizing a specific cost function.