\section{Cross-Validation}\label{section: Cross-Validation}

\subsection{Introduction}

We are studying \textbf{how to estimate a function} $f(x)$ from data. For example, we have a dataset of houses with:
\begin{itemize}
    \item $x$: number of rooms.
    \item $y$: house price.
\end{itemize}
And we want to learn a function $\hat{f}(x)$ that predicts the price from the number of rooms. But there's a \hl{problem}: \textbf{we don't know the true function} $f$. We \textbf{only have data} that was generated by:
\begin{equation*}
    Y = f(X) + \varepsilon    
\end{equation*}
Where:
\begin{itemize}
    \item $f(X)$ is the \textbf{true relationship}.
    \item $\varepsilon$ is some \textbf{random noise} (unpredictable).
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What's the challenge?}}
\end{flushleft}
We want our model $\hat{f}(x)$ to make \textbf{accurate predictions on new data}, not just on training data. But there's a dilemma:

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l @{}}
            \toprule
            Model Type & Characteristics \\
            \midrule
            \textbf{Too simple} (e.g. straight line)        & Can't capture real patterns, \textbf{underfitting} \\ [.3em]
            \textbf{Too complex} (e.g. crazy wiggly curve)  & Memorizes noise in training, \textbf{overfitting} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\noindent
That's the \textbf{bias-variance tradeoff}. To measure this, we use the Mean Squared Error (MSE, see page \pageref{eq: Mean Squared Error}) metric:
\begin{equation*}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{f}(x_i)\right)^2
\end{equation*}
This measures the average squared distance between the true values and the model predictions. In other words, quantify how close our model's predictions are to the true values. Both \textbf{training error} and \textbf{test error} are measured using MSE:
\begin{itemize}
    \item \definition{Training Error} is computed on training data:
    \begin{equation}
        \text{MSE}_{\text{Training Error}} = \dfrac{1}{n} \cdot \sum \left(y_i - \hat{f}(x_i)\right)^2
    \end{equation}
    It is often misleading, because it always gets smaller with more complex models.
    \item \definition{Test Error} is computed on unseen data:
    \begin{equation}
        \text{MSE}_{\text{Test Error}} = \mathbb{E}\left[\left(Y_0 - \hat{f}(X_0)\right)^2\right]
    \end{equation}
    It may increase with complexity, this is the \textbf{core issue} in model selection.
\end{itemize}


\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is Bias and what is Variance?}}
\end{flushleft}
\begin{itemize}
    \item \important{Bias} is \textbf{how far our model's average prediction is from the true function} $f(x)$. It is high when the model is too simple.

    For example, we fit a straight line to a curved pattern, then our predictions are always wrong in the same way.


    \item \important{Variance} is \textbf{how much our model's prediction \emph{changes} if we\break change the training data}. It is high when the model is too flexible.

    For example, we fit a high-degree polynomial that changes completely if we remove 1 point from the dataset.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why is this important?}}
\end{flushleft}
Because our \textbf{goal is to minimize the total prediction error} on unseen data. And the test error (see above) can be decomposed as:
\begin{equation*}
    \text{Test Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Noise}
\end{equation*}
We \textbf{cannot eliminate noise}, but we can \textbf{choose a model that balances bias and variance}.

\highspace
\begin{examplebox}[: Analogy]
    Imagine we're throwing darts at a target.
    \begin{center}
        \begin{tabular}{@{} l l l @{}}
            \toprule
            Situation & Bias & Variance \\
            \midrule
            Darts are tightly clustered but far from the center & \textcolor{Red2}{\faIcon{times}} High & \textcolor{Green3}{\faIcon{check}} Low \\ [.3em]
            Darts are all over the place & \textcolor{Green3}{\faIcon{check}} Low & \textcolor{Red2}{\faIcon{times}} High \\ [.3em]
            Darts are close together and near the bullseye & \textcolor{Green3}{\faIcon{check}} Low & \textcolor{Green3}{\faIcon{check}} Low \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{examplebox}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How Bias-Variance tradeoff is linked to Cross-Validation}}
\end{flushleft}
We just saw:
\begin{equation*}
    \text{Test Error} = \text{Bias}^2 + \text{Variance} + \text{Noise}
\end{equation*}
But we \textbf{don't have} access to the test data! So \textbf{we can't compute the real test error directly}.

\highspace
The \textbf{solution} is to use Cross-Validation. \important{Cross-Validation} is a \textbf{resampling method} that simulates what happens on test data by:
\begin{enumerate}
    \item Hiding some of the training data.
    \item Fitting the model on the remaining data.
    \item Testing it on the hidden part.
    \item Repeating this several times.
\end{enumerate}
This lets us \textbf{estimate} the test error without needing new data.

\highspace
\textcolor{Green3}{\faIcon{check-circle} \textbf{Why is that useful for Bias-Variance?}} Because cross-validation helps us understand how models behave with different \textbf{complexities}:
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} p{10em} l l l @{}}
            \toprule
            \textbf{Model} & \textbf{Bias} & \textbf{Variance} & \textbf{CV Error} \\
            \midrule
            Simple (e.g. linear)                    & \textcolor{Red2}{\faIcon{times}} High     & \textcolor{Green3}{\faIcon{check}} Low    & \textcolor{Red2}{\faIcon{times}} CV error is high due to underfitting \\ [.5em]
            Complex (e.g. 10th degree polynomial)   & \textcolor{Green3}{\faIcon{check}} Low    & \textcolor{Red2}{\faIcon{times}} High     & \textcolor{Red2}{\faIcon{times}} CV error is high due to overfitting  \\ [1.6em]
            Moderate complexity                     & Balanced                                  & Balanced                                  & \textcolor{Green3}{\faIcon{check}} CV Error is lowest                 \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\noindent
So \textbf{Cross-Validation helps us find the sweet pot} where bias and variance are balanced, with lowest test error. In other words, bias-variance  explains \emph{why} generalization is hard, and cross-validation is the tool we use to \emph{measure} how well a model generalizes.