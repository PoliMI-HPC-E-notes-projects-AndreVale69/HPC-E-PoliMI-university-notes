\subsection{Resampling Methods}

We've seen that:
\begin{itemize}
    \item \textbf{Training error} underestimates true prediction error.
    \item \textbf{Test error} is what we care about, but test labels are usually \textbf{not available} during training.
\end{itemize}
\important{Resampling Methods} solve this by reusing the training data to simulate the process of testing on unseen data. They prove a \textbf{data-efficient way to estimate test error}, especially when we don't want to waste data by holding our a large validation set.

\highspace
\definition{Resampling Methods} are techniques in statistics and machine learning where we \textbf{repeatedly draw subsets of data from our dataset}, fit models on those subsets, and then evaluate performance. \hl{They simulate the process of testing a model on new data, using only the available training data.}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why do we need them?}}
\end{flushleft}
We care about \textbf{how well a model performs on unseen data}, the test error. But during training, we usually \textbf{don't have test labels}, only training data. Resampling lets us:
\begin{enumerate}
    \item \textbf{Hold out part of the training data.}
    \item \textbf{Train on the rest.}
    \item \textbf{Test on the hold-out.}
    \item \textbf{Repeat} the process several times.
    \item \textbf{Average} the error estimates.
\end{enumerate}
Common resampling techniques are Validation Set approach (see page \pageref{paragraph: Validation Set}), Cross-Validation, Leave-One-Out CV (LOOCV), Bootstrap. We \textbf{focus on K-Fold CV and LOOCV} because they give robust, low-variance estimates of test error, especially for model evaluation and selection.