\subsubsection{Bayes Error Rate}

Even the \textbf{best possible classifier} (the Bayes classifier) can make mistakes. \emph{Why?} Because \textbf{sometimes different classes overlap}: for some values of $\mathbf{x}$, it's just not clear which class it came from, even with perfect knowledge of the distributions. This unavoidable error is called the \definition{Bayes Error Rate}.

\highspace
The \definition{Bayes Error Rate} is the \textbf{lowest possible classification error achievable by any classifier}. It's the error made by the Bayes classifier, which is optimal. Mathematically, it's the probability that the Bayes classifier \textbf{predicts the wrong class}:
\begin{equation}
    \text{Bayes Error Rate} = \mathbb{E}_{\mathbf{X}} \left[ 1 - \max_k P\left(Y = \mathcal{C}_k \mid \mathbf{X} = \mathbf{x}\right) \right]
\end{equation}
This means:
\begin{itemize}
    \item For each observation $\mathbf{x}$, compute the \textbf{posterior probabilities} for each class.
    \item Keep the \textbf{highest one}, that's our confidence in the prediction.
    \item Subtract it from 1, that's the chance of being wrong at that point.
    \item Average this over all possible $\mathbf{x}$, gives us the total Bayes error.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Binary Case (2 classes)}}
\end{flushleft}
Since the previous formula works for any number of classes, \emph{why focus on the binary case?} Because in the 2-class case, everything simplifies nicely:
\begin{itemize}
    \item We only need to compare two posterior probabilities.
    \item The error at each $\mathbf{x}$ is just the smaller of the two posteriors.
    \item The total Bayes error becomes the integral of $\min\left(\pi_{1}f_{1}\left(x\right), \pi_{2}f_{2}\left(x\right)\right)$, which is easy to compute or visualize.
\end{itemize}
So the binary case gives a clean formula and is ideal for examples.

\highspace
For two classes $\mathcal{C}_1$ and $\mathcal{C}_2$, the error at any point $\mathbf{x}$ is:
\begin{equation}
    \text{Error at } \mathbf{x} = \min \left( P\left(Y = \mathcal{C}_1 \mid \mathbf{x}\right), \; P\left(Y = \mathcal{C}_2 \mid \mathbf{x}\right) \right)
\end{equation}
So:
\begin{equation}
    \text{Bayes Error Rate} = \int \min\left(\pi_{1} f_{1}\left(\mathbf{x}\right), \pi_{2} f_{2}\left(\mathbf{x}\right)\right) \, \mathrm{d}\mathbf{x}
\end{equation}
That is: \textbf{integrate over the overlap} of the two class distributions, where the ``wrong'' class is more likely.
\begin{itemize}
    \item If the class distributions are \textbf{well separated}, the Bayes error is \textbf{very low} or even 0.
    \item If they \textbf{overlap}, the Bayes classifier will still make \textbf{errors}, no matter how our model is.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/bayes-classifier/bayes-error-rate.pdf}
    \caption{This plot shows the Bayes error rate visually as the gray shaded area. The value (from the integral of the shaded region) is $\approx 0.059$, meaning even the best classifier will make an error $5.9\%$ of the time in this setting.}
\end{figure}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/bayes-classifier/bayes-error-rate-almost-zero.pdf}
    \caption{This plot is similar to the previous one, but here we increase the separation between the class means and change the priors. The two curves overlap much less, then the shaded area is smaller. The Bayes error rate drops significantly (now around $0.006$, $0.6\%$) compared to $\approx 5.9\%$ before.}
\end{figure}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{\speedIcon} \textbf{Performance Measures}}
\end{flushleft}
First, it's important to know that the \definition{Oracle Classifier} \textbf{is another name for the Bayes classifier}. It emphasizes that the classifier knows the true class-conditional distributions and gives the lowest possible error.

\highspace
In simulation studies or theoretical analysis, we can:
\begin{itemize}
    \item Generate data from known distributions;
    \item Then simulate the Bayes/Oracle classifier (since we know $f_k$);
    \item And evaluate its \textbf{true error rate}.
\end{itemize}
This gives us a \textbf{reference point} to compare real classifiers like LDA, QDA, or logistic regression.

\highspace
Once we have a classifier (e.g., LDA, QDA, Bayes), we \textbf{need to evaluate how well it performs}. We use two key error measures:
\begin{itemize}
    \item \definition{Apparent Error Rate (APER)}. It is the \textbf{training error}. It's calculated by applying the classifier to the \textbf{same data it was trained on}. Often \textbf{optimistic}, it \hl{underestimates the real error}, especially if the model is overfitting.
    \begin{equation}
        \text{APER} = \dfrac{1}{n} \cdot \displaystyle\sum_{i=1}^{n} \mathbb{I}\left( \hat{y}_{i} \neq y_{i} \right) \quad \text{(on training data)}
    \end{equation}

    \item \definition{Actual Error Rate (AER)} (or \definition{True Error Rate}). The \textbf{true classification error}, measured on \textbf{independent test data}. Gives a \hl{more honest estimate of model performance}. In simulations, we can also compare to the \textbf{true labels generated from known distributions}.
    \begin{equation}
        \text{AER} = \dfrac{1}{m} \cdot \displaystyle\sum_{j=1}^{m} \mathbb{I}\left( \hat{y}_{j} \neq y_{j} \right) \quad \text{(on test data or true labels)}
    \end{equation}
\end{itemize}