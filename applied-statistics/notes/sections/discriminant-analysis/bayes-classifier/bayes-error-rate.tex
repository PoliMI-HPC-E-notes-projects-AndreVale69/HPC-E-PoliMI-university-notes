\subsubsection{Bayes Error Rate}

Even the \textbf{best possible classifier} (the Bayes classifier) can make mistakes. \emph{Why?} Because \textbf{sometimes different classes overlap}: for some values of $\mathbf{x}$, it's just not clear which class it came from, even with perfect knowledge of the distributions. This unavoidable error is called the \definition{Bayes Error Rate}.

\highspace
The \definition{Bayes Error Rate} is the \textbf{lowest possible classification error achievable by any classifier}. It's the error made by the Bayes classifier, which is optimal. Mathematically, it's the probability that the Bayes classifier \textbf{predicts the wrong class}:
\begin{equation}
    \text{Bayes Error Rate} = \mathbb{E}_{\mathbf{X}} \left[ 1 - \max_k P\left(Y = \mathcal{C}_k \mid \mathbf{X} = \mathbf{x}\right) \right]
\end{equation}
This means:
\begin{itemize}
    \item For each observation $\mathbf{x}$, compute the \textbf{posterior probabilities} for each class.
    \item Keep the \textbf{highest one}, that's our confidence in the prediction.
    \item Subtract it from 1, that's the chance of being wrong at that point.
    \item Average this over all possible $\mathbf{x}$, gives us the total Bayes error.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Binary Case (2 classes)}}
\end{flushleft}
Since the previous formula works for any number of classes, \emph{why focus on the binary case?} Because in the 2-class case, everything simplifies nicely:
\begin{itemize}
    \item We only need to compare two posterior probabilities.
    \item The error at each $\mathbf{x}$ is just the smaller of the two posteriors.
    \item The total Bayes error becomes the integral of $\min\left(\pi_{1}f_{1}\left(x\right), \pi_{2}f_{2}\left(x\right)\right)$, which is easy to compute or visualize.
\end{itemize}
So the binary case gives a clean formula and is ideal for examples.

\highspace
For two classes $\mathcal{C}_1$ and $\mathcal{C}_2$, the error at any point $\mathbf{x}$ is:
\begin{equation}
    \text{Error at } \mathbf{x} = \min \left( P\left(Y = \mathcal{C}_1 \mid \mathbf{x}\right), \; P\left(Y = \mathcal{C}_2 \mid \mathbf{x}\right) \right)
\end{equation}
So:
\begin{equation}
    \text{Bayes Error Rate} = \int \min\left(\pi_{1} f_{1}\left(\mathbf{x}\right), \pi_{2} f_{2}\left(\mathbf{x}\right)\right) \, \mathrm{d}\mathbf{x}
\end{equation}
That is: \textbf{integrate over the overlap} of the two class distributions, where the ``wrong'' class is more likely.
\begin{itemize}
    \item If the class distributions are \textbf{well separated}, the Bayes error is \textbf{very low} or even 0.
    \item If they \textbf{overlap}, the Bayes classifier will still make \textbf{errors}, no matter how our model is.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/bayes-classifier/bayes-error-rate.pdf}
    \caption{This plot shows the Bayes error rate visually as the gray shaded area. The value (from the integral of the shaded region) is $\approx 0.059$, meaning even the best classifier will make an error $5.9\%$ of the time in this setting.}
\end{figure}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/bayes-classifier/bayes-error-rate-almost-zero.pdf}
    \caption{This plot is similar to the previous one, but here we increase the separation between the class means and change the priors. The two curves overlap much less, then the shaded area is smaller. The Bayes error rate drops significantly (now around $0.006$, $0.6\%$) compared to $\approx 5.9\%$ before.}
\end{figure}