\subsubsection{Bayes vs LDA/QDA}

As we've seen in previous sections, the \textbf{Bayes classifier} is the gold standard because it yields the \textbf{lowest possible error} \hl{when the true distributions of the data are known}.

\highspace
But in practice, we rarely know those distributions. That's where LDA (Linear Discriminant Analysis) and QDA (Quadratic Discriminant Analysis) come in. They are \textbf{approximations} of the Bayes classifier under some assumptions.

\highspace
As we've seen, the Bayes classifier is:
\begin{equation*}
    \hat{y} = \arg\max_k \; \pi_k \cdot f_k\left(\mathbf{x}\right)
\end{equation*}
Where $f_{k}\left(\mathbf{x}\right)$ is the \textbf{true class-conditional density that is not known in practice} because we only have data. \hl{\textbf{That's why we can't use the Bayes classifier in practice, it's only a theoretical benchmark.}} Instead, LDA and QDA assume that the shape of $f_{k}$ is known. However, they do not know the shape of $f_{k}$; they estimate it from the data.
\begin{itemize}
    \item \hl{LDA}: \textbf{assumes all classes share the same covariance matrix}.

    \textcolor{Green3}{\faIcon{book} \textbf{Assumptions}}
    \begin{itemize}
        \item Class-conditional distributions: Multivariate Gaussian.
        \item \textbf{Same covariance matrix} $\Sigma$ for all classes.
        \item Different means $\mu_{k}$.
    \end{itemize}
    This leads to a \textbf{linear decision boundary}. It is linear because when we plug the Gaussian formula into the Bayes rule and simplify under equal $\Sigma$, we get a linear function of $\mathbf{x}$.
    
    \item \hl{QDA}: \textbf{allows each class to have its own covariance matrix}.

    \textcolor{Green3}{\faIcon{book} \textbf{Assumptions}}
    \begin{itemize}
        \item Class-conditional distributions: Multivariate Gaussian.
        \item \textbf{Different covariance matrix} $\Sigma_k$ for each class.
    \end{itemize}
    This leads to a \textbf{quadratic decision boundary}.
\end{itemize}

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l p{10.384em} p{10.384em} @{}}
            \toprule
            Property & \textbf{Bayes Classifier} & \textbf{LDA} & \textbf{QDA} \\
            \midrule
            \textbf{Requires true} $f_k$ & \textcolor{Green3}{\faIcon{check}} Yes    & \textcolor{Red2}{\faIcon{times}} No, assumes Gaussian     & \textcolor{Red2}{\faIcon{times}} No, assumes Gaussian     \\ [.5em]
            \textbf{Covariance}          & Anything                                  & Shared across classes                                     & One per class                                             \\ [.5em]
            \textbf{Decision boundary}   & Arbitrary (optimal)                       & Linear                                                    & Quadratic                                                 \\ [.5em]
            \textbf{Flexibility}         & Highest                                   & Low                                                       & Medium                                                    \\ [.5em]
            \textbf{Risk of overfitting} & None (theoretical)                        & Low                                                       & Higher                                                    \\ [.5em]
            \textbf{When to use}         & Only in theory                            & When classes are well-separated, simple                   & When boundaries are curved, and more data is available    \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What Is the Purpose of the Bayes Classifier?}}
\end{flushleft}
Although the Bayes classifier cannot be used in practice, it plays a fundamental role in machine learning and statistics:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{It Defines the Best Possible Classifier}}. It is the \textbf{gold standard}: the classifier with the \textbf{lowest possible error rate}. This error is called the Bayes error rate. All other classifiers are trying to get as close as possible to this theoretical optimum.

    \item \textcolor{Green3}{\textbf{It Helps Us Understand the Role of Assumptions}}. By comparing real classifiers to the Bayes rule, we can say:
    \begin{itemize}
        \item \emph{How much error is due to \textbf{model approximation}?}
        \item \emph{How much is due to \textbf{data limitations}?}
    \end{itemize}
    For example:
    \begin{itemize}
        \item LDA $\approx$ Bayes \textbf{only} when its assumptions are correct (Gaussian, equal covariance).
        \item When assumptions are wrong (e.g., non-Gaussian data), LDA will do worse than Bayes.
    \end{itemize}
    
    \item \textcolor{Green3}{\textbf{It Provides a Foundation for Deriving Other Classifiers}}. Many practical classifiers (LDA, QDA, Naive Bayes, even some neural networks) are \textbf{inspired by Bayes' rule}. They make simplifying assumptions or estimate $f_{k}$, but the \textbf{form of the classifier} is based on Bayes.

    \item \textcolor{Green3}{\textbf{It Lets Us Theoretically Analyze the Limits of Learning}}. It helps define concepts like bias-variance tradeoff, irreducible error, and Bayes consistency. We can measure how ``good'' a classifier is by how close it comes from to \textbf{Bayes-optimal}.
\end{itemize}

\highspace
\begin{examplebox}[: Bayes Analogy]
    Think of the Bayes classifier like a \textbf{perfect GPS} that always knows the best route, because it sees the \textbf{entire traffic map of the world}. In practice, we only get local signals (data), so our actual GPS (LDA, QDA, etc.) is trying to guess the best route, and we measure its performance \textbf{by how close it is to the perfect one}.
\end{examplebox}