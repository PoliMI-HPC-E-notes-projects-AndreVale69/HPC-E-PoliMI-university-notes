\subsection{Bayes Classifier}

\subsubsection{What is the Bayes Classifier?}

The \definition{Bayes Classifier} is a \textbf{decision rule} that assigns each observation to the class with the \textbf{highest posterior probability}, based on the conditional probability distribution of the features given the class.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{square-root-alt} \textbf{Formal Definition}}
\end{flushleft}
Let:
\begin{itemize}
    \item $\mathbf{x} \in \mathbb{R}^p$ be a new observation (feature vector).
    \item $\mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_K$ be the possible \textbf{classes}.
    \item $\pi_k = P\left(Y = \mathcal{C}_k\right)$ be the \textbf{prior probability} of class $k$.
    \item $f_k(\mathbf{x}) = P\left(\mathbf{X} = \mathbf{x} \mid Y = \mathcal{C}_k\right)$ be the \textbf{class-conditional density}.
\end{itemize}
Then the \textbf{posterior probability} of class $\mathcal{C}_k$ given $\mathbf{x}$ is:
\begin{equation}
    P\left(Y = \mathcal{C}_k \mid \mathbf{X} = \mathbf{x}\right) = \dfrac{\pi_{k} f_{k}\left(\mathbf{x}\right)}{\displaystyle\sum_{j=1}^{K} \pi_{j} f_{j}\left(\mathbf{x}\right)}
\end{equation}
The \textbf{Bayes classifier} assigns $\mathbf{x}$ to the class with the \textbf{highest posterior}:
\begin{equation}
    \hat{y} = \arg\max_k \, P\left(Y = \mathcal{C}_k \mid \mathbf{X} = \mathbf{x}\right)
\end{equation}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Math Behind the Bayes Classifier}}
\end{flushleft}
We will try to explain it in the simplest terms possible. Imagine we're a doctor trying to decide if a patient has \textbf{flu} or \textbf{allergy}, based on their \textbf{temperature}. We know from past experience:
\begin{itemize}
    \item People with the flu usually have higher temperatures.
    \item People with allergies usually have normal temperatures.
\end{itemize}
Now, a new patient walks in with a temperature of $37.8$\textdegree C. We ask: ``\emph{given this temperature, what is the most likely diagnosis?}''. The Bayes Classifier answer this by saying ``\emph{pick the class (flu or allergy) that is most likely, based on what we know about temperatures in each class.}''. So:
\begin{itemize}
    \item If flu patients often have temperatures around $37.8$\textdegree C, and allergy patients don't;
    \item Then the Bayes Classifier says: ``\emph{assign this patient to the flu class.}''.
\end{itemize}
Now that the intuition is clear, let's explain the math behind the Bayes Classifier.

\highspace
We are trying to \textbf{classify a new observation} $\mathbf{x}$, such as someone's temperature, into one of $K$ classes (e.g., flu vs allergy). We want to choose the class $\mathcal{C}_{k}$ that \textbf{maximizes the probability that it's the correct class}, given the data. This is the \textbf{posterior probability} (page \pageref{eq: posterior probability}):
\begin{equation*}
    P\left(Y = \mathcal{C}_k \mid \mathbf{X} = \mathbf{x}\right)
\end{equation*}
To compute this, we use \textbf{Bayes' theorem}:
\begin{equation}
    P\left(Y = \mathcal{C}_k \mid \mathbf{X} = \mathbf{x}\right) = \frac{\pi_k f_k\left(\mathbf{x}\right)}{\displaystyle\sum_{j=1}^{K} \pi_j f_j\left(\mathbf{x}\right)}
\end{equation}
Let's explain each term:
\begin{itemize}
    \item $\pi_k = P(Y = \mathcal{C}_k)$ is the \textbf{prior}, \emph{how common is class $k$ in general?} For example, if 40\% of patients have the flu, then $\pi_{\text{flu}} = 0.4$.

    \item $f_k(\mathbf{x}) = P(\mathbf{X} = \mathbf{x} \mid Y = \mathcal{C}_k)$ is the \textbf{Class-conditional density}. For example, how likely is temperature $37.8$\textdegree C \emph{if} someone has the flu?
    
    \item The \textbf{numerator} $\pi_k f_k(\mathbf{x})$ tells us: how likely is it that someone is in class $k$ and has this data $\mathbf{x}$?

    \item The \textbf{denominator} just makes sure that all the probabilities sum to 1.
\end{itemize}
Although the formula may be complex, we \textbf{don't need to compute the entire formula}. Since the denominator is the same for all classes, we only care about comparing:
\begin{equation*}
    \pi_{k} f_{k}\left(\mathbf{x}\right)
\end{equation*}
We choose the class $k$ that \textbf{maximizes} this value:
\begin{equation*}
    \hat{y} = \arg\max_k \; \pi_{k} f_{k}\left(\mathbf{x}\right)
\end{equation*}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/bayes-classifier/bayes-classifier-with-two-classes.pdf}
    \caption{This plot shows how the Bayes classifier works in a simple case with two classes (flu and allergy), and one feature (body temperature). The blue curve $\pi_{\text{flu}} \cdot f_{\text{flu}}\left(x\right)$ is the likelihood of flu at each temperature, and the orange curve $\pi_{\text{allergy}} \cdot f_{\text{allergy}}\left(x\right)$ is the likelihood of allergy. At any temperature $x$, we compare the two curves; whichever is higher, we choose that class. The black dashed line shows the Bayes decision boundary, around 37.37\textdegree C: for $x < 37.37$ choose allergy, otherwise for $x > 37.37$ choose flu.}
\end{figure}