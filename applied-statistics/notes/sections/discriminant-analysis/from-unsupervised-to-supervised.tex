\section{Discriminant Analysis}

\subsection{From Unsupervised to Supervised}

So far, we've been working with \textbf{GMMs} and \textbf{clustering}, which are \textbf{unsupervised} methods. This meaning:
\begin{itemize}
    \item We had no labels.
    \item We tried to discover patterns (e.g., clusters) \textbf{just from the data}.
\end{itemize}
Now we're moving to \definition{Supervised Learning} where:
\begin{itemize}
    \item Each observation $\mathbf{x}_i$ \textbf{comes with a label} $y_i \in \{1, \dots, K\}$
    \item We \textbf{want to learn a function} that can classify new inputs based on past labeled data.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why This Transition Is Natural (GMM $\rightarrow$ LDA)}}
\end{flushleft}
Let's revisit how we modeled things in GMM. In GMM, we assumed:
\begin{equation*}
    \mathbf{x} \sim \sum_{k=1}^K \pi_k \cdot \mathcal{N}(\mu_k, \Sigma_k)
\end{equation*}
But we \textbf{didn't know} which $k$ each $\mathbf{x}$ came from, hence, EM algorithm. In Supervised Learning (like Linear Discriminant Analysis, LDA), we \textbf{do know} which class $k$ each $\mathbf{x}$ comes from. This lets us directly estimate the parameters $\pi_k, \mu_k, \Sigma_k$ from the \textbf{labeled data}. Thus:

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | l @{}}
        \toprule
        \textbf{GMM (Unsupervised)}         & \textbf{LDA/QDA (Supervised)}  \\
        \midrule
        No labels                           & Labels $y_i \in \{1, \dots, K\}$    \\ [.3em]
        Estimate hidden assignments (EM)    & Directly estimate class parameters  \\ [.3em]
        Responsibilities: soft membership   & Class probabilities from Bayes rule \\ [.3em]
        Soft clustering                     & Hard classification                 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | p{20em} @{}}
        \toprule
        Term & Meaning \\
        \midrule
        \textbf{Latent Variable}       & In GMM, the unknown class membership $Z$; in supervised learning, it's known.                                              \\ [.5em]
        \textbf{Generative Model}      & A model for how data is generated: first sample label $y \sim \pi_k$, then $\mathbf{x} \sim \mathcal{N}(\mu_k, \Sigma_k)$. \\ [.5em]
        \textbf{Supervised Learning}   & Learn a classifier from labeled data â€” i.e., a function $f(\mathbf{x}) = \hat{y}$.                                         \\ [.5em]
        \textbf{Discriminant Analysis} & Use Bayes' rule with class-conditional distributions to assign classes.                                                    \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Key Formulation Shift}}
\end{flushleft}
In GMM (unsupervised), we care about:
\begin{equation*}
    \mathbb{P}(\text{class } = k \mid \mathbf{x}) = \dfrac{
            \pi_k \cdot \phi\left(\mathbf{x} \mid \mu_k, \Sigma_k\right)
        }{
            \displaystyle\sum_{j=1}^K \pi_j \cdot \phi\left(\mathbf{x} \mid \mu_j, \Sigma_j\right)
        }
\end{equation*}
Now in \textbf{supervised learning}, we do the same, but:
\begin{itemize}
    \item We estimate $\mu_k, \Sigma_k$ \textbf{per class} from labeled data.
    \item Then use Bayes' rule to classify a new point $\mathbf{x}$.
\end{itemize}
This is the basis for LDA and QDA (Quadratic Discriminant Analysis).

\highspace
We're not throwing away the GMM mindset, we're now using it \textbf{with supervision}:
\begin{itemize}
    \item Same assumptions: Gaussian class-conditional densities.
    \item But now, we \textbf{know the true class}, so we don't need EM.
\end{itemize}
This is why \textbf{LDA} is sometimes called the \textbf{supervised version of GMM with equal covariances}.