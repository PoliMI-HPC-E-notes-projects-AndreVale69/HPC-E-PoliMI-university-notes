\subsection{Introduction to Supervised Learning}

\definition{Supervised Learning} is about learning a mapping from inputs to outputs using \textbf{labeled data}. We're given a dataset:
\begin{equation}
    \left\{\left(\mathbf{x}_1, y_1\right), \left(\mathbf{x}_2, y_2\right), \dots, \left(\mathbf{x}_n, y_n\right)\right\}
\end{equation}
Where:
\begin{itemize}
    \item $\mathbf{x}_i \in \mathbb{R}^p$: vector of features (e.g., age, salary, pixel values, etc.).
    \item $y_i \in \{1, 2, \dots, K\}$: label.
\end{itemize}
Our goal is to \textbf{learn a function}:
\begin{equation}
    f\left(\mathbf{x}\right) = \hat{y}
\end{equation}
That can predict the correct $y$ for a new input $\mathbf{x}$.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{Types of Supervised Learning}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l | l | l | l @{}}
            \toprule
            \textbf{Problem Type}   & \textbf{Output} $y$   & \textbf{Examples} & \textbf{Algorithms}       \\
            \midrule
            Classification          & Categorical/Discrete  & Tumor type        & LDA, QDA, logistic reg    \\ [.3em]
            Regression              & Real-valued           & Temperature       & Linear regression         \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{Supervised vs. Unsupervised}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | l | l @{}}
        \toprule
                    & \textbf{Unsupervised}         & \textbf{Supervised}   \\
        \midrule
        Input       & $\mathbf{x}_{i}$              & $\left(\mathbf{x}_{i}, y_{i}\right)$  \\ [.3em]
        Goal        & Discover structure (clusters) & Predict outcome $y$ from $\mathbf{x}$ \\ [.3em]
        Methods     & GMM, K-Means                  & LDA, QDA                              \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
So in GMM, we tried to \textbf{infer the labels}. In Supervised Learning, the labels are \textbf{given}, so the task becomes more about \textbf{learning rules}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is Discriminant Analysis, and how is it linked to Supervised Learning?}}
\end{flushleft}
Because \definition{Discriminant Analysis} is one of the \textbf{oldest and most classic forms of supervised learning}, specifically for \textbf{classification} tasks.

\highspace
In Supervised Learning, we want to \textbf{learn a rule that predicts the class} $y$ \textbf{given an input} $\mathbf{x}$. Discriminant Analysis \textbf{does exactly that}, by:
\begin{enumerate}
    \item Using the labeled data $(\mathbf{x}_i, y_i)$
    \item Estimating how each class looks in feature space, with a \textbf{probability model} for $\mathbb{P}(\mathbf{x} \mid y = k)$
    \item Applying \textbf{Bayes' rule} to invert that: estimate $\mathbb{P}(y = k \mid \mathbf{x})$
    \item Choosing the class with the highest probability
\end{enumerate}
So we use the label $y_{i}$ to:
\begin{itemize}
    \item Group the data by class;
    \item Estimate a distribution for each class;
    \item Then \textbf{discriminate} new points by which class distribution they're most likely to belong to.
\end{itemize}

\highspace
\textcolor{Red2}{\faIcon{book} \textbf{More in-depth.}} \definition{Discriminant Analysis} is a method to \textbf{classify observations into categories} (like ``spam'' vs ``not spam'', or species A vs species B) by:
\begin{enumerate}
    \item \textbf{Modeling how each class generates data}, using a probability distribution (usually multivariate normal).
    \item Then using \textbf{Bayes' Rule} to compute the probability that a new point belong to each class.
    \item Finally, assigning the new point to the \textbf{most probable class}.
\end{enumerate}
The word \emph{discriminant} refers to the idea of \textbf{finding rules or boundaries that \emph{discriminate}} between classes based on the features $\mathbf{x}$. So:
\begin{itemize}
    \item LDA: uses linear boundaries
    \item QDA: uses quadratic boundaries
\end{itemize}
These are both types of discriminant functions.

\highspace
Discriminant Analysis \textbf{is called generative because it is a generative model} that assumes we can model:
\begin{itemize}
    \item $\mathbb{P}(\mathbf{x} \mid y = k)$: the distribution of features \textbf{given} the class.
    \item $\mathbb{P}(y = k)$: the prior probability of each class.
\end{itemize}
Using Bayes' theorem:
\begin{equation*}
    \mathbb{P}(y = k \mid \mathbf{x}) = \frac{\mathbb{P}(\mathbf{x} \mid y = k) \cdot \mathbb{P}(y = k)}{\mathbb{P}(\mathbf{x})}
\end{equation*}
This is discriminant analysis, we \textbf{use these class-conditional densities to make predictions}.

\highspace
\begin{definitionbox}[: Generative Model]
    A \definition{Generative Model} models \textbf{how the data is generated}, i.e., it learns the \textbf{joint distribution}:
    \begin{equation*}
        \mathbb{P}(\mathbf{x}, y)
    \end{equation*}
    From which we can \textbf{generate new data}, or compute the conditional:
    \begin{equation*}
        \mathbb{P}(y \mid \mathbf{x})
    \end{equation*}
    More specifically, in classification tasks, a generative model models:
    \begin{equation*}
        \mathbb{P}(\mathbf{x} \mid y=k) \quad \text{(class-conditional density)}
    \end{equation*}
    And:
    \begin{equation*}
        \mathbb{P}(y = k) \quad \text{(class prior)}
    \end{equation*}
    Then it uses \textbf{Bayes' Rule} to compute:
    \begin{equation*}
        \mathbb{P}(y = k \mid \mathbf{x}) = \frac{\mathbb{P}(\mathbf{x} \mid y = k)\cdot \mathbb{P}(y = k)}{\mathbb{P}(\mathbf{x})}
    \end{equation*}

    \begin{examplebox}[: LDA as a Generative Model]
        LDA assumes:
        \begin{itemize}
            \item Each class $k$ has a Gaussian distribution:
            \begin{equation*}
                \mathbf{x} \mid y = k \sim \mathcal{N}(\mu_k, \Sigma)
            \end{equation*}
            \item We learn:
            \begin{itemize}
                \item $\mu_{k}$, the mean of class $k$
                \item $\Sigma$, shared covariance matrix
                \item $\pi_{k}$, prior probability of class $k$
            \end{itemize}
        \end{itemize}
        With those, we can:
        \begin{itemize}
            \item \textbf{Generate data}: sample a class $y \sim \pi_k$, then sample $\mathbf{x} \sim \mathcal{N}\left(\mu_k, \Sigma\right)$
            \item \textbf{Classify new points} using \textbf{Bayes' Rule}
        \end{itemize}
    \end{examplebox}
\end{definitionbox}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Practical Importance}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Data}: we often have real-world datasets where class labels are known (e.g., in medicine, finance, spam detection).
    \item \textbf{Goal}: we want to \emph{classify} new observations as accurately as possible.
    \item \textbf{Approach}: fit models like LDA, logistic regression, or modern ML classifiers (SVM, Trees, etc.).
\end{itemize}