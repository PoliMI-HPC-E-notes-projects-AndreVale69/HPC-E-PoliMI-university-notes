\subsection{Quadratic Discriminant Analysis (QDA)}

\definition{Quadratic Discriminant Analysis (QDA)} is a \textbf{supervised classification method} that models each class as a multivariate Gaussian distribution \textbf{with its own covariance matrix}.

\highspace
It uses \textbf{Bayes' Rule} to classify new observations by computing posterior probabilities, like LDA, but allows \textbf{more flexible class shapes}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Intuition}}
\end{flushleft}
QDA is just like LDA, \textbf{but more flexible}:
\begin{itemize}
    \item In \textbf{LDA}, all classes share the same shape (same covariance, then same spread, same orientation).
    \item In \textbf{QDA}, each class has its \textbf{own shape and orientation} in feature space.
\end{itemize}
This means we can model curved boundaries and non-symmetric class regions. But the \textbf{price is more parameters}, and \textbf{more chance of overfitting} on small datasets.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{What is Overfitting?}}
    \label{def: Overfitting}
\end{flushleft}
\definition{Overfitting} occurs when a \textbf{model learns} not just the \textbf{true patterns} in the data, \textbf{but also the random noise}. It's like memorizing answer for an exam instead of understanding the subject; we do well on the questions we saw before (training data), but fail on new ones (test data).

\highspace
Imagine fitting a curve to data points:
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | p{20em} @{}}
        \toprule
        Fit Type & Description \\
        \midrule
        \textbf{Underfitting}   & The model is too simple (e.g., a straight line when the pattern is curved), it misses important structure. \\ [.5em]
        \textbf{Good Fit}       & The model captures the true pattern without being too complex. \\ [.5em]
        \textbf{Overfitting}    & The model bends and twists to go exactly through every training point, even if those points contain random noise. \\
        \bottomrule
    \end{tabular}
    \caption{Fitting type.}
\end{table}

\noindent
Our model is overfitting when we observe the following:
\begin{itemize}
    \item \textbf{Training error}: very \emph{low}
    \item \textbf{Test error}: \emph{high}
\end{itemize}
The model performs \textbf{too well} on the training set because it's \textbf{memorized it}, not generalized it.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.76\textwidth]{img/qda/overfitting.pdf}
    \caption{An example of overfitting. This figure shows polynomial regression models with increasing complexity. Although polynomial regression itself hasn't been explained, the purpose of this figure is to illustrate the effect of overfitting as model complexity increases (degree 1 underfits, too simple, degree 4 fits well, good generalization, degree 15 overfits, captures noise).}
\end{figure}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{How does QDA differ from LDA?}}
\end{flushleft}
\begin{itemize}
    \item \important{Covariance Matrix}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | l @{}}
            \toprule
            LDA & QDA \\
            \midrule
            Shared across all classes, $\Sigma$ & Unique per class, $\Sigma_{k}$ for each class \\
            \bottomrule
        \end{tabular}
        \caption*{Covariance Matrix.}
    \end{table}

    \item \important{Decision Boundary}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | l @{}}
            \toprule
            LDA & QDA \\
            \midrule
            \textbf{Linear} (straight lines / hyperplanes) & \textbf{Quadratic} (curved lines / surfaces) \\
            \bottomrule
        \end{tabular}
        \caption*{Decision Boundary.}
    \end{table}

    \item \important{Model Complexity}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | p{16em} @{}}
            \toprule
            LDA & QDA \\
            \midrule
            Simpler, fewer parameters to estimate & More complex, more parameters (one full covariance per class) \\
            \bottomrule
        \end{tabular}
        \caption*{Model Complexity.}
    \end{table}

    \item \important{Bias vs. Variance}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} p{16.5em} | p{16.5em} @{}}
            \toprule
            LDA & QDA \\
            \midrule
            Lower variance, higher bias (more stable) & Lower bias, higher variance (risk of overfitting) \\
            \bottomrule
        \end{tabular}
        \caption*{Bias vs. Variance.}
    \end{table}

    \item \important{Shape of Class Regions}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} p{16.6em} | p{16.6em} @{}}
            \toprule
            LDA & QDA \\
            \midrule
            All classes modeled with the \textbf{\underline{same} shape \& orientation} & Each class can have a \textbf{\underline{different} shape \& orientation} \\
            \bottomrule
        \end{tabular}
        \caption*{Shape of Class Regions.}
    \end{table}

    \newpage

    \item \important{Interpretability}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} p{15em} | p{15em} @{}}
            \toprule
            LDA & QDA \\
            \midrule
            Easier to interpret decision boundaries & Harder to visualize, especially in higher dimensions \\
            \bottomrule
        \end{tabular}
        \caption*{Interpretability.}
    \end{table}

    \item \important{Performance with Few Data}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | p{18em} @{}}
            \toprule
            LDA & QDA \\
            \midrule
            Works well, robust & Needs more data to estimate separate covariances reliably \\
            \bottomrule
        \end{tabular}
        \caption*{Performance with Few Data.}
    \end{table}
\end{itemize}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What's the key difference?}}
\end{flushleft}
LDA and QDA both model each class as a multivariate Gaussian, but the key difference is:
\begin{itemize}
    \item \textbf{LDA assumes all classes share the same covariance matrix}. The data clouds (ellipses) all have the same shape and orientation.
    \item \textbf{QDA allows each class to have its own covariance matrix}. The data clouds (ellipses) can have different shapes, sizes, and orientations.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What this means in practice}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l l @{}}
            \toprule
            Concept & LDA & QDA \\
            \midrule
            \textbf{Boundary shape} & Straight lines (linear)               & Curved lines (quadratic)              \\ [.3em]
            \textbf{Flexibility}    & Less, all classes ``look similar''    & More, each class can ``look unique''  \\ [.3em]
            \textbf{Math form}      & Linear discriminant function          & Quadratic discriminant function       \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/qda/lda-vs-qda.pdf}
    \caption{We have two clouds of points. In LDA, they are both ellipses of the same shape, just located in different places. In QDA, one ellipse might be wide and flat, the other tall and narrow, and the model adapts to those shapes.}
\end{figure}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{The Decision Rule and Why It's Quadratic}}
\end{flushleft}
We want to classify a new point $\mathbf{x}$ by computing the \textbf{posterior probability} (same as page \pageref{eq: posterior probability}):
\begin{equation*}
    \mathbb{P}\left(y = k \mid \mathbf{x}\right)
\end{equation*}
And assigning $\mathbf{x}$ to the class $k$ with the \textbf{highest posterior}. This is the same idea as LDA, but now, each class has \textbf{its own covariance matrix}, which changes the math.
\begin{enumerate}
    \item \important{Bayes' Rule} (same as page \pageref{eq: bayes rule - lda}).
    \begin{equation*}
        \mathbb{P}\left(y = k \mid \mathbf{x}\right) = \dfrac{\mathbb{P}\left(\mathbf{x} \mid y = k\right) \cdot \mathbb{P}\left(y = k\right)}{\mathbb{P}\left(\mathbf{x}\right)}
    \end{equation*}
    Again, since the denominator $\mathbb{P}\left(\mathbf{x}\right)$ is the same for all classes, we only need to \textbf{maximize the numerator}:
    \begin{equation*}
        \mathbb{P}\left(y = k \mid \mathbf{x}\right) \propto \pi_{k} \cdot \phi\left(\mathbf{x} \mid \mu_{k}, \Sigma_{k}\right)
    \end{equation*}
    \begin{itemize}
        \item $\pi_{k}$ is the class prior.
        \item $\phi\left(\mathbf{x} \mid \mu_{k}, \Sigma_{k}\right)$ is the Gaussian density with \textbf{class-specific} $\Sigma_{k}$
    \end{itemize}


    \item \important{Log of the Gaussian Density}. Taking the \textbf{log} of this expression (just like in LDA, page \pageref{eq: discriminant function - lda}), we get a \definitionWithSpecificIndex{Discriminant Function}{Discriminant Function - QDA}{}:
    \begin{equation}
        \delta_k(\mathbf{x}) = -\dfrac{1}{2} \cdot \log \left|\Sigma_k\right| - \dfrac{1}{2} \cdot \left(\mathbf{x} - \mu_{k}\right)^{T} \: \Sigma_{k}^{-1} \: \left(\mathbf{x} - \mu_{k}\right) + \log\left(\pi_{k}\right)
    \end{equation}
\end{enumerate}
The \textbf{quadratic part} comes from:
\begin{equation*}
    \left(\mathbf{x} - \mu_{k}\right)^{T} \Sigma_k^{-1} \left(\mathbf{x} - \mu_{k}\right)
\end{equation*}
Which is a \textbf{quadratic form}. A \definition{Quadratic Form} is just a fancy way to describe any expression where variables are multiplied together, especially with squares and cross-terms.

\highspace
The \important{decision boundary} between class $k$ and $l$ is:
\begin{equation*}
    \delta_{k}\left(\mathbf{x}\right) = \delta_{l}\left(\mathbf{x}\right)
\end{equation*}
This becomes a \textbf{quadratic equation} in $\mathbf{x}$. Hence, the boundary is a \textbf{curve}, not a straight line (in 2D curved lines, in 3D curved surfaces, and in $p$-D quadratic hypersurfaces).

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{When to Prefer QDA Over LDA}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Class Covariances Are Clearly Different}}. If our data shows that:
    \begin{itemize}
        \item Class A is tightly clustered.
        \item Class B is widely spread out.
        \item Or they have very different \textbf{orientations}.
    \end{itemize}
    Then QDA is the better choice because it allows:
    \begin{equation*}
        \Sigma_1 \neq \Sigma_2 \neq \dots \neq \Sigma_K
    \end{equation*}
    This leads to \textbf{more accurate boundaries}, especially when classes have \textbf{non-linear separation}.

    \item \textcolor{Green3}{\textbf{We Have Enough Data}}. QDA estimates \textbf{a full covariance matrix for each class}, which means:
    \begin{itemize}
        \item More parameters to estimate.
        \item Higher risk of overfitting if sample size is small
    \end{itemize}
    So we \textbf{use QDA when our dataset is large enough} to support all those estimates reliably. 

    \item \textcolor{Green3}{\textbf{We Suspect Curved Boundaries}}. If we visually or intuitively suspect:
    \begin{enumerate}
        \item The data cannot be separated with a straight line.
        \item There's curvature in how the classes divide.
    \end{enumerate}
    Then QDA is more likely to model the real structure.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{When to Avoid QDA}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textcolor{Red2}{\textbf{Small sample size}}. Estimating separate covariance matrices can become unstable.
    \item \textcolor{Red2}{\textbf{High-dimensional data}}. In $p$-dimensional space, each $\Sigma_{k}$ has $\dfrac{p\left(p+1\right)}{2}$ parameters, QDA becomes data-hungry.
    \item \textcolor{Red2}{\textbf{Risk of overfitting}}. More flexibility, then higher variance, especially with noise or unbalanced classes.
\end{itemize}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | l @{}}
        \toprule
        Use \textbf{LDA} when... & Use \textbf{QDA} when... \\
        \midrule
        We want simplicity and robustness   & We want flexibility and accuracy \\ [.3em]
        Covariances are similar             & Covariances are different \\ [.3em]
        Data is limited                     & We have enough samples per class \\ [.3em]
        Boundaries are approximately linear & Boundaries appear curved \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\begin{examplebox}[: Overfitting]
    \begin{center}
        \includegraphics[width=.8\textwidth]{img/qda/overfitting-2.pdf}
        \highspace
        \includegraphics[width=.8\textwidth]{img/qda/overfitting-3.pdf}
    \end{center}

    In this example, both QDA and LDA were trained on a very small dataset with only 10 training samples per class. QDA achieved 80\% accuracy on the training set by fitting a flexible, curved decision boundary that tightly adapted to the few observed points. However, this resulted in poor generalization, with test accuracy dropping to just 70\%, a possible case of overfitting. In contrast, LDA used a simpler, linear decision boundary, achieving same training accuracy (80\%) but significantly better performance on the test set (80\%). This demonstrates that LDA, being more constrained, generalizes better in low-data regimes, whereas QDA requires more data to avoid overfitting its more complex model.
\end{examplebox}