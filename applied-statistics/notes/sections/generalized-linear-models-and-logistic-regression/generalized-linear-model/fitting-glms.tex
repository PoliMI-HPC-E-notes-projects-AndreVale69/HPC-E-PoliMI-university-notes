\subsubsection{Fitting GLMs}\label{subsubsection: Fitting GLMs}

In \textbf{Linear Regression}, we find $\beta$ by minimizing the \textbf{Sum of Squared Errors}, that's the famous \textbf{Ordinary Least Squares (OLS)}. \hl{But for \textbf{GLMs}, OLS \textbf{does not work}} because:
\begin{itemize}
    \item The error distribution is not normal anymore.
    \item The mean is transformed by a link function.
    \item We need to respect the correct likelihood for the chosen distribution.
\end{itemize}
So \textbf{GLMs are fit by} \definition{Maximum Likelihood Estimation (MLE)}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How does Maximum Likelihood work?}}
\end{flushleft}
Imagine we have data, like:
\begin{equation*}
    \left(Y_1, Y_2, \dots, Y_n\right)
\end{equation*}
We assume:
\begin{itemize}
    \item The data points are \textbf{random}, but they follow some probability model that depends on parameters $\beta$.
    \item Our goal is to find the \textbf{best} $\beta$.
\end{itemize}
But how do we decide what ``best'' means? The ``best'' parameters are the ones that make the data we actually observed as \textbf{likely} as possible. If our model is good, the \textbf{probability of our actual data should be high}. So we \hl{pick $\beta$ that \textbf{maximizes} that probability}.
\begin{enumerate}
    \item \important{Write the probability of one point}. For example, for Logistic Regression:
    \begin{equation*}
        Y_i \sim \text{Bernoulli}(p_i), \quad p_i = \text{sigmoid}(X_i^T \beta)
    \end{equation*}
    So:
    \begin{equation*}
        P(Y_i \, | \, X_i, \beta) = p_i^{Y_i} (1 - p_i)^{1 - Y_i}
    \end{equation*}

    \item \important{Multiply them all together}. For the whole dataset:
    \begin{equation*}
        L(\beta) = \prod_{i=1}^n P(Y_i \, | \, X_i, \beta)
    \end{equation*}
    This is the \textbf{likelihood function}.

    \item \important{Take the log}. Multiplying many tiny probabilities is numerically annoying. So we take the \textbf{log-likelihood}:
    \begin{equation*}
        \ell(\beta) = \sum_{i=1}^n \log P(Y_i \, | \, X_i, \beta)
    \end{equation*}
    The maximum is in the same place because the log is monotonic.

    \item \important{Maximize the log-likelihood}. Find the $\beta$ that gives the highest $\ell(\beta)$. In other words, find the $\beta$ that makes the data we observed \textbf{most probable} under our model.

    \textcolor{Green3}{\faIcon{question-circle} \textbf{How do we do the maximization?}} We take the \textbf{derivate} of the log-likelihood (\emph{score function}):
    \begin{equation}
        U(\beta) = \frac{\partial \ell(\beta)}{\partial \beta}
    \end{equation}
    We solve $U\left(\beta\right) = 0$. In practice, we can't solve this exactly, we do it \textbf{numerically}:
    \begin{itemize}
        \item Start with a guess for $\beta$
        \item Compute the gradient (the slope)
        \item Step in the direction that increases the likelihood
        \item Repeat until the slope is flat (maximum found)
    \end{itemize}
    This is like climbing a hill until we reach the top. It works by iteratively solving the score equations until the likelihood is maximized.
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{How is this different from OLS?}}
\end{flushleft}
In OLS, we don't think in terms of probabilities. We just minimize the sum of squared differences:
\begin{equation*}
    \text{SSE} = \sum_{i=1}^n (Y_i - X_i^T \beta)^2
\end{equation*}
This \emph{is} MLE \textbf{when} the errors are normal. If we assume $Y$ is Normal, the MLE for the mean is exactly the same as minimizing SSE. But for \textbf{binary data}, \textbf{counts}, the Normal is wrong, so SSE doesn't make sense. MLE naturally picks the correct probability model.

\highspace
In conclusion, \hl{OLS is a special case of MLE when the errors are Normal}. When the errors follow Bernoulli, Poisson, etc., we use the \textbf{correct likelihood}, not squared errors. MLE finds the parameter values that make our data ``least surprising'' under our chosen probability model.

\highspace
\begin{takeawaysbox}[: Fitting GLMs]
    \begin{itemize}
        \item For \textbf{linear regression}: find $\beta$ that \textbf{minimize sum of squared errors}.
        \item For \textbf{GLMs}: find $\beta$ that \textbf{maximize the likelihood}, because we now have a proper probability model for our special data.
    \end{itemize}
\end{takeawaysbox}