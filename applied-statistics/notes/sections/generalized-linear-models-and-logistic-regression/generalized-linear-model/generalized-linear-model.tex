\subsection{Generalized Linear Model (GLM)}

A \definition{Generalized Linear Model (GLM)} is a flexible extension of the standard linear regression that models the \textbf{mean} of a response variable $Y$ through a \textbf{link function}. A GLM has three key components:
\begin{itemize}
    \item \important{Random component}. The response variable $Y$ follows a distribution from the \textbf{exponential family} (e.g., normal, Bernoulli, Poisson).
    \item \important{Systematic component}. Predictors $X_i$ are combined linearly:
    \begin{equation*}
        \eta_i = X_i^T \beta
    \end{equation*}
    \item \important{Link function}. A function $g(\cdot)$ relates the expected value of $Y$ to the linear predictor:
    \begin{equation*}
        g(E[Y_i]) = \eta_i = X_i^T \beta
    \end{equation*}
\end{itemize}
In short, a GLM is any model where:
\begin{itemize}
    \item The \textbf{mean} is not modeled directly, but \emph{through a link}.
    \item The data can follow various distributions (not just normal).
    \item Fitting is done by Maximum Likelihood, not just OLS.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why do we need Generalized Linear Models (GLMs)?}}
\end{flushleft}
As we saw in the previous section, the key problem with linear regression is that we model:
\begin{equation*}
    E[Y_i] = X_i^T \beta
\end{equation*}
But \textbf{some types of data don't fit this}.

\highspace
\textcolor{Green3}{\faIcon{question-circle} \textbf{So what's the big idea of GLMs?}} \textbf{Don't model} $E[Y_i]$ \textbf{directly}. Instead, model a \textbf{transformation} of it, so it stays inside valid limits. This is called the \textbf{link function}, $g\left(\cdot\right)$:
\begin{equation*}
    g(E[Y_i]) = X_i^T \beta
\end{equation*}

\begin{examplebox}[: Binary Data]
    Suppose:
    \begin{equation*}
        Y_i \in \left\{0, 1\right\}
    \end{equation*}
    Maybe it's: \emph{did the custom buy?} yes (1) or no (0). The thing we want to predict is a \textbf{probability}, which must be between 0 and 1. If we use a simple line:
    \begin{equation*}
        P(Y_i = 1) = X_i^T \beta
    \end{equation*}
    This can easily give us values like $1.2$ or $-0.1$, \textbf{impossible} for a probability.

    \highspace
    We use the logistic link:
    \begin{equation*}
        g(p) = \log \left( \dfrac{p}{1-p} \right)
    \end{equation*}
    So instead of saying:
    \begin{equation*}
        P(Y_i = 1) = X_i^T \beta
    \end{equation*}
    We say:
    \begin{equation*}
        \log \left( \frac{P(Y_i = 1)}{1 - P(Y_i = 1)} \right) = X_i^T \beta
    \end{equation*}
    This forces the probability to stay between 0 and 1. This is \textbf{Logistic Regression}.
\end{examplebox}