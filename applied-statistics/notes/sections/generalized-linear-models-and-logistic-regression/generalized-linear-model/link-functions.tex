\subsubsection{Link Functions}

A \definition{Link Function} connects the mean of our response variable $Y$ to the linear predictor (the same linear piece as in ordinary linear regression).

\highspace
A \textbf{link function} $g(\cdot)$ is a mathematical function such that:
\begin{equation}
    g(E[Y_i]) = X_i^T \beta
\end{equation}
So instead of predicting the mean \textbf{directly}, we predict a \textbf{transformation} of it.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why is it essential?}}
\end{flushleft}
Because for some data types (like binary or counts), the mean must lie in a specific range. The link function \textbf{ensures} that the model output stays inside valid bounds.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{Types of link functions}}
\end{flushleft}
\begin{itemize}
    \item \definition{Identity link} (Linear Regression). It is used in standard linear regression and is considered \naive because it \textbf{does not make any transformations}:
    \begin{equation}
        g(\mu) = \mu
    \end{equation}


    \item \definition{Logit Link} (Logistic Regression). It is used for \textbf{binary outcomes}, because we need the mean (the probability) to stay in $(0, 1)$. So we model the \textbf{log-odds}:
    \begin{equation}\label{eq: Logit Link}
        g(\mu) = \log \left( \dfrac{\mu}{1-\mu} \right)
    \end{equation}
    This stretches the probability $\mu$ from $(0,1)$ to the whole real line $(-\infty, \infty)$. So any value of $X^T \beta$ makes sense.

    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why take the log?}} Because the \textbf{odds} are always positive (0 to infinity), and the \textbf{log-odds} stretch this to the whole real line $\left(-\infty, +\infty\right)$.
    \begin{itemize}
        \item If $\text{odds} = 1 \rightarrow \text{log-odds} = 0$.
        \item If $\text{odds} < 1 \rightarrow \text{log-odds}$ negative.
        \item If $\text{odds} > 1 \rightarrow \text{log-odds}$ positive.
    \end{itemize}
    So \textbf{log-odds} can be any real number and it is perfect for linear modeling!

    \begin{remarkbox}[: \emph{What are odds and log-odds?}]
        Suppose:
        \begin{equation*}
            p = P(Y = 1) = 0.8    
        \end{equation*}
        So we have an 80\% chance of success. The \textbf{odds} of success are:
        \begin{equation*}
            \text{odds} = \dfrac{p}{1 - p} = \dfrac{0.8}{0.2} = 4
        \end{equation*}
        So we are 4 times more likely to succeed than to fail.

        \highspace\index{Odds}
        In other words, the \textbf{odds are the ratio of success to failure}.

        \highspace\index{Log-Odds}
        The \textbf{log-odds} is just:
        \begin{equation}
            \text{log-odds} = \log(\text{odds}) = \log \left( \frac{p}{1 - p} \right)
        \end{equation}
    \end{remarkbox}


    \item \definition{Log Link} (Poisson Regression). It is used for counts, where the mean must be $\geq 0$:
    \begin{equation}
        g(\mu) = \log(\mu)
    \end{equation}
    So:
    \begin{equation}
        \log(\lambda) = X^T \beta \quad \Longrightarrow \quad \lambda = e^{X^T \beta} > 0
    \end{equation}
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What do we really model in a GLM?}}
\end{flushleft}
In a \textbf{GLM}, we do \textbf{not} directly model the mean $E[Y]$. Instead, we model the \textbf{result of applying the link function} to the mean. Formally:
\begin{equation*}
    g(E[Y]) = X^T \beta    
\end{equation*}
So \textbf{the actual linear model} is for:
\begin{equation*}
    \underbrace{g(E[Y])}_{\text{link of the mean}}
\end{equation*}
In other words, we choose a link function $g(\cdot)$ to \emph{transform} the mean of $Y$ so it can be modeled linearly. Then we fit the linear predictor to that \textbf{transformed mean}. The output of the link is \textbf{not} the raw mean, but a transformed version of it.

\highspace
\textbf{In a GLM, we always model the link of the mean, not the mean itself.}