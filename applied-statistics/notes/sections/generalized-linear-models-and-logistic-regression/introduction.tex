\section{GLMs and Logistic Regression}

\subsection{Introduction}

A \definition{Linear Model} (e.g., Linear Regression, page \pageref{subsection: Simple Linear Regression}) is a very simple rule to \emph{predict} a value $Y$ (like someone's weight, income, house price, etc.). It says: take some \textbf{input variables} (like age, income, years of education), multiply each by a number (the \textbf{beta}), add them up, we get our \textbf{prediction}.

\highspace
So for the $i$-th person:
\begin{equation*}
    Y_i = X_i^T \beta + \varepsilon_i
\end{equation*}
\begin{itemize}
    \item $X_i$ means all the \textbf{inputs} for person \emph{i} (e.g., height, age, income), all together in a list (a vector).
    \item $\beta$ is a set of \textbf{weights} telling us how important each input is.
    \item $\varepsilon_i$ is the \textbf{random error}, the difference between what we predict and what we really see.
\end{itemize}

\highspace
When we learned Ordinary Least Squares (OLS, page \pageref{def: Ordinary Least Squares Estimation}) regression, we saw:
\begin{equation*}
    Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \dots + \varepsilon_i
\end{equation*}
That is a linear model. It predicts a continuous outcome (like price, weight, score). It fits the coefficients $\beta$ by minimizing the squared differences between the predicted and actual values.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Okay, but what's the point?}}
\end{flushleft}
In other words, the model says \emph{on average}, the value we expect for $Y_i$ is:
\begin{equation*}
    E[Y_i] = X_i^T \beta
\end{equation*}
So, for a normal linear regression, we \hl{believe the average outcome \textbf{IS} just a line (or plane) in our input space}.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{So what is the problem?}}
\end{flushleft}
The problem is simple. Imagine we use this same idea to predict whether someone buys a product: yes (1) or no (0). If we use a line, it can predict values like $1.2$ or $-0.3$, \textbf{nonsense}, because we can't have a probability bigger than 1 or smaller than 0.

\highspace
So, a \hl{standard linear model} is \textbf{good for numbers that can be any real number} (like income), but \hl{bad for things like}:
\begin{itemize}
    \item Probabilities, must be in $\left[0, 1\right]$
    \item Counts, must be $\ge 0$
\end{itemize}
And \textbf{that's where Generalized Linear Models (GLMs) comes in}. They generalize Linear Regression to handle: Binary data (Logistic Regression), Count data (Poisson Regression) and other special cases.
