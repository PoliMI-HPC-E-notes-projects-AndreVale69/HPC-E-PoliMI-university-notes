\subsubsection{Evaluating Logistic Regression}

Once we've:
\begin{itemize}
    \item Chosen our variables (AIC and/or BIC)
    \item Fitted the model (MLE)
    \item Interpreted the coefficients (odds, log-odds)
\end{itemize}
But we still need to ask: ``\emph{\textbf{how well does our model actually predict 0 vs. 1?}}''. GLMs are \textbf{probabilistic models}, but real-world classification problems often care about:
\begin{itemize}
    \item \emph{Did we predict the right class?}
    \item \emph{How often?}
    \item \emph{Where did we make mistakes?}
\end{itemize}
So we need \textbf{evaluation metrics} for \emph{classification performance}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Confusion Matrix: TP, FP, TN, FN}}
\end{flushleft}
When we \textbf{classify} an observation (0 or 1), 4 things can happen:
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} c c c @{}}
        \toprule
        \textbf{Actual} & \textbf{Predicted 1}          & \textbf{Predicted 0} \\
        \midrule
        \textbf{1}      & \textbf{TP} (True Positive)   & \textbf{TN} (True Negative)   \\ [.3em]
        \textbf{0}      & \textbf{FP} (False Positive)  & \textbf{FN} (False Negative)  \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{itemize}
    \item \definition{TP (True Positive)}: we predicted 1 $\rightarrow$ actually 1 (correct)
    \item \definition{TN (True Negative)}: we predicted 0 $\rightarrow$ actually 0 (correct)
    \item \definition{FP (False Positive)}: we predicted 1 $\rightarrow$ actually 0 (wrong, false alarm)
    \item \definition{FN (False Negative)}: we predicted 0 $\rightarrow$ actually 1 (miss)
\end{itemize}
The \definition{Confusion Matrix} shows us \textbf{how many} examples fell into each box. A \definition{Confusion Matrix} is a simple \textbf{table} that shows us \textbf{how well our classifier is doing} by comparing what our model \textbf{predicted} vs. what actually \textbf{happened}.

\highspace
Note that the Confusion Matrix is a useful tool because it breaks down our predictions. It doesn't just tell us \emph{how many were correct}, but also \textbf{where and how we were wrong}. Also, it is important because \textbf{Accuracy alone can hide problems}. For example, if 95\% of our data are zeros, a dumb model can always predict 0 and get 95\% accuracy, but the Confusion Matrix shows it never fins the 1s!

\highspace
In summary, the \textbf{Confusion Matrix is the core tool to see exactly how our Logistic Regression is working as a classifier}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Accuracy, Precision, Recall}}
\end{flushleft}
The confusion matrix gives us the raw counts: how many TP, TN, FP, and FN we have. But we still need \textbf{summary numbers} to \emph{quantify} performance in a clear way. So we calculate: Accuracy, Precision and Recall; all by plugging in TP, TN, FP, FN.
\begin{itemize}
    \item \definition{Accuracy}
    \begin{equation}
        \text{Accuracy} = \dfrac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
    \end{equation}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{What does it tell us?}} \emph{What fraction of total predictions were correct?}
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{When useful}}: Good for balanced datasets. But \textbf{not so good} if our classes are very imbalanced (e.g., 99\% zeros).
    \end{itemize}
    For example, if we have 80 correct out of 100, the accuracy is 80\%.
    
    \item \definition{Precision}
    \begin{equation}
        \text{Precision} = \dfrac{\text{TP}}{\text{TP} + \text{FP}}
    \end{equation}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{What does it tell us?}} \emph{Of all the times we predicted \textbf{1}, how many were actually 1?}
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{When useful}}: Important when \textbf{false positives} are expensive.
    \end{itemize}
    For example, with email spam filters, we don't want to mark good emails as spam.

    \item \definition{Recall}
    \begin{equation}
        \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
    \end{equation}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{What does it tell us?}} \emph{Of all the true 1s, how many did our model actually find?}
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{When useful}}: Important when \textbf{false negatives} are expensive.
    \end{itemize}
    For example, with medical tests, we don't want to miss sick patients.
\end{itemize}
This is why they're \textbf{core metrics} for \textbf{evaluating Logistic Regression} as a classifier: they tell us \textbf{how well our thresholding turns probabilities into real decisions}.

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l l l @{}}
            \toprule
            Metric              & Comes from        & What it answer                        & When useful \\
            \midrule
            \textbf{Accuracy}   & TP, TN, FP, FN    & Overall: how often right?             & Balanced data \\ [.3em]
            \textbf{Precision}  & TP, FP            & How trustworthy are positives?        & False positives costly \\ [.3em]
            \textbf{Recall}     & TP, FN            & How well do we find real positives?   & False negatives costly \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{These metrics were obtained from the Confusion Matrix.}
\end{table}

