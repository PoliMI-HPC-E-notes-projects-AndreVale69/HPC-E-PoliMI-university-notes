\subsubsection{Interpreting Logistic Regression Coefficients}

In Logistic Regression, each $\beta_j$ \textbf{does not} directly tell us the change in probability like a slope in Linear Regression. Instead, \textbf{each} $\beta_j$ tells us \textbf{how the log-odds change when that predictor increases by 1 unit}.

\highspace
Formally, \definition{Odds} is:
\begin{equation*}
    \text{odds} = \frac{p}{1 - p} \quad (\text{ratio of success to failure})
\end{equation*}
And \definition{Log-Odds} (logit) is:
\begin{equation*}
    \text{log-odds} = \log \left( \frac{p}{1 - p} \right)
\end{equation*}
The logistic regression model is naturally written in \textbf{log-odds}:
\begin{equation*}
    \text{log-odds} = X^T \beta
\end{equation*}
So $\beta_j$ \hl{shows how much the \textbf{log-odds} change per unit increase in $X_j$}. However, log-odds is not very intuitive for most people. For example, if $\beta_j = 0.5$, the log-odds goes up by $0.5$, but what does that mean in real life? So we \textbf{exponentiate} the log-odds change, and we get the \textbf{odds ratio}. This turns an \emph{additive} change on the log scale into a \emph{multiplicative} effect on the odds.

\highspace
If we exponentiate $\beta_j$, we get \definition{Odds Ratio}:
\begin{equation}
    \text{OR}_j = e^{\beta_j}
\end{equation}
For each 1-unit increase in $X_j$, the \textbf{odds of success} multiply by $\text{OR}_j$. This is much easier to explain:
\begin{itemize}
    \item ``The odds double'' (if $\text{OR} = 2$).
    \item ``The odds increase by 50\%'' (if $\text{OR} = 1.5$).
    \item ``The odds decrease by half'' (if $\text{OR} = 0.5$).
\end{itemize}
