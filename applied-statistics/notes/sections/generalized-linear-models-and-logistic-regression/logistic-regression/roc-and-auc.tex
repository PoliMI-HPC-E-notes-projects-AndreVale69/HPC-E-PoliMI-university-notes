\subsubsection{ROC and AUC}

A \definition{Threshold} is just a \textbf{cutoff number} we choose to decide \textbf{when to call a probability a ``yes'' (1) or a ``no'' (0)}. Logistic Regression always gives us a \textbf{probability}:
\begin{equation*}
    \hat{p} = P\left(Y = 1 | X\right)
\end{equation*}
But real-life decisions are not ``\emph{maybe 70\% yes}'', they're binary:
\begin{itemize}
    \item Do we send the marketing email? Yes or No.
    \item Is the patient sick? Yes or No.
    \item Do we approve the loan? Yes or No.
\end{itemize}
So we must \textbf{convert} that probability into an actual \textbf{class label}: 0 or 1. \textbf{To do that, we pick a cutoff, this is the threshold}. A threshold is just the line we draw to say: ``\emph{Above this number, predict 1. Below or equal, predict 0}''.

\highspace
Logistic Regression gives us \textbf{probabilities}, not just labels. We turn probabilities into 0 or 1 by setting a \textbf{threshold} (e.g., $0.5$). But Accuracy, Precision, Recall \textbf{depend} on the threshold we pick. If we change the threshold, these numbers change. So \emph{how do we evaluate a model \textbf{independent of a single threshold?}} That's where ROC and AUC come in.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{ROC curve, what is it?}}
\end{flushleft}
\textbf{ROC} stands for \definition{Receiver Operating Characteristic (ROC)}. A \textbf{ROC curve} shows how our classifier's performance \textbf{changes} as we sweep the threshold from 0 to 1. It plots:
\begin{itemize}
    \item \textbf{Y-axis}: \definition{True Positive Rate (TPR)}, same as Recall:
    \begin{equation}
        \text{TPR} = \dfrac{\text{TP}}{\text{TP} + \text{FN}}
    \end{equation}
    \item \textbf{X-axis}: \definition{False Positive Rate (FPR)}:
    \begin{equation}
        \text{FPR} = \dfrac{\text{FP}}{\text{FP} + \text{TN}}
    \end{equation}
\end{itemize}
The key idea is:
\begin{itemize}
    \item \textbf{Low} threshold $\rightarrow$ we classify \textbf{more things as positive} $\rightarrow$ TPR \textbf{increases}, as does FPR.
    \item \textbf{High} threshold $\rightarrow$ we classify \textbf{fewer things as positive} $\rightarrow$ TPR \textbf{decrease}, as does FPR.
\end{itemize}
So the ROC curve shows the \textbf{trade-off} between:
\begin{itemize}
    \item Catching true positives (TPR)
    \item Making false alarms (FPR)
\end{itemize}

\newpage

\noindent
A \textbf{perfect classifier} has $\text{TPR} = 1$ with $\text{FPR} = 0$, on top-left corner:
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.9\textwidth]{img/logistic-regression/perfect-classifier-roc.pdf}
\end{figure}

\noindent
A \textbf{random guess} falls along the diagonal line (TPR $=$ FPR). The \textbf{better our model}, the \textbf{closer the curve hugs the top-left corner}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{AUC, what does it mean?}}
\end{flushleft}
\definition{Area Under the ROC Curve (AUC)} condenses the ROC curve into a single number:
\begin{equation}
    0 \leq \text{AUC} \leq 1
\end{equation}
\begin{itemize}
    \item $\text{AUC} = 0.5 \rightarrow$ our model is no better than random guessing.
    \item $\text{AUC} = 1 \rightarrow$ perfect classifier.
    \item $\text{AUC} > 0.8 \rightarrow$ pretty good!
\end{itemize}
AUC is the probability that our model gives a higher score to a random positive than to a random negative. So it's a measure of how well our model \textbf{ranks} examples, regardless of threshold.

\highspace
\textcolor{Green3}{\faIcon{check-circle} \textbf{Why it's great:}} ROC \& AUC tell us how good our model is \textbf{even before we choose any single threshold}. ROC and AUC are core tools for comparing classifiers when our output is a probability. They help us understand the trade-off between catching true positives and avoiding false alarms, across all possible thresholds.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/logistic-regression/roc.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{
        A simple example ROC curve:
        \begin{itemize}
            \item The blue curve shows how TPR vs. FPR trade off as we vary the threshold.
            \item The gray dashed line is what we'd get with a totally random classifier ($\text{AUC} = 0.5$).
            \item The AUC shows how well the model ranks positives above negatives. In this toy example, it's around what we'd expect for random noise.
        \end{itemize}
        In real life, a good model's ROC curve bows up toward the top left, showing it finds positives with fewer false alarms!
    }
\end{figure}