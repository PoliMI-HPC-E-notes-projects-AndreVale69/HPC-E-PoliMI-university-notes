\subsection{Logistic Regression}

\subsubsection{What is Logistic Regression?}

\definition{Logistic Regression} is a \textbf{Generalized Linear Model (GLM)} used to model a \textbf{binary outcome} (i.e., when the response $Y$ can only be 0 or 1). It models the \textbf{probability} that $Y = 1$ as a \textbf{sigmoid function} of a linear combination of predictors. Formally:
\begin{equation}
    Y_i \sim \text{Bernoulli}\left(p_i\right) \quad \text{where} \quad p_i = P\left(Y_i = 1 | X_i\right)
\end{equation}
So we have an observation $i$ with input data $X_i$. The outcome $Y_i$ can be $0$ or $1$. The \textbf{probability} that $Y_i = 1$ is $p_i$. This means that the random variable $Y_i$ follows a Bernoulli distribution:
\begin{itemize}
    \item $Y_i = 1$ with probability $p_i$.
    \item $Y_i = 0$ with probability $1 - p_i$.
\end{itemize}
So we say: ``The \emph{distribution} of $Y_i$ is Bernoulli, with success probability $p_i$''.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How is $p_i$ determined?}}
\end{flushleft}
We don't just guess $p_i$! We \textbf{model it} using our predictors $X_i$ and some coefficients $\beta$. But we can't model $p_i$ directly with a linear predictor because probabilities must stay in $[0,1]$.

\highspace
So we \textbf{transform} $p_i$ using the \textbf{logit link} (page \pageref{eq: Logit Link}):
\begin{equation*}
    \log \left( \dfrac{p_i}{1 - p_i} \right) = X_i^T \beta \quad \left(\definition{Logit Link}\right)
\end{equation*}
Where:
\begin{itemize}
    \item The expression inside the log is called the \textbf{odds}:
    \begin{equation*}
        \text{odds} = \dfrac{p}{1 - p}
    \end{equation*}

    \item The \textbf{log of the odds} (log-odds) can be any real number, so it is perfect for linear modeling.
    
    \item So we model \textbf{log-odds} as a linear function of our predictors:
    \begin{equation*}
        \text{log-odds of success} = X_i^T \beta
    \end{equation*}
\end{itemize}
This means that if $X_i^T \beta$ goes up, the log-odds go up. And bigger log-odds, higher probability of success.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{undo} \textbf{Why is the \emph{sigmoid} version necessary?}}
\end{flushleft}
We've already seen why Inverse Link Functions exist (see page \pageref{subsubsection: Inverse Link Function}). However, the \emph{logit} equation shows how we model the probability $p_i$. But to get back to the \textbf{actual probability}, we invert the \emph{logit} using the \textbf{Sigmoid function}:
\begin{equation*}
    p_i = \dfrac{1}{1 + e^{-X_i^T \beta}} \quad \left(\definition{Sigmoid}\right)
\end{equation*}
In simple terms, take our linear combination $X_i^T \beta$, plug it into the \textbf{sigmoid function}, and we get $p_i$, which is guaranteed to be between $0$ and $1$. So the sigmoid ``squashes'' the real line to the unit interval.

\begin{takeawaysbox}
    So the big picture is:
    \begin{itemize}
        \item The \important{logit form} is how we \textbf{build} and \textbf{fit} the model.
        \item The \important{sigmoid form} is how we \textbf{interpret} it and \textbf{make probability predictions}.
    \end{itemize}
    They are just two sides of the same coin.
\end{takeawaysbox}