\subsection{Multiclass Logistic Regression}

Everything so far assumed:
\begin{equation*}
    Y_i \in \left\{0, 1\right\}
\end{equation*}
But what if we have:
\begin{equation*}
    Y_i \in \left\{1, 2, \dots, K\right\} \quad \left(K > 2\right)
\end{equation*}
For example:
\begin{itemize}
    \item Email topic: \emph{Work, Personal, Spam}.
    \item Image label: \emph{Cat, Dog, Bird}.
    \item Customer intent: \emph{Buy A, Buy B, Buy Nothing}.
\end{itemize}
Logistic Regression by itself only models:
\begin{equation*}
    P\left(Y = 1 | X\right) \quad \text{vs.} \quad P\left(Y = 0 | X\right)
\end{equation*}
So it's naturally \textbf{binary}.

\highspace
But when we have \textbf{more than 2 classes}, we need to adapt the idea:
\begin{itemize}
    \item Keep the basic logistic idea: \emph{probabilities modeled using log-odds}.
    \item But generalize the structure to handle multiple outcomes.
\end{itemize}
So the \textbf{goal} is to predict \textbf{which class} out of \emph{K} possibilities each observation belongs to.
\begin{itemize}
    \item \definition{One-vs-Rest (OvR) strategy}. Split our $K$-class problem into \textbf{$K$ separate binary} problems. For each class:
    \begin{itemize}
        \item Label that class as \textbf{1}.
        \item Label all other classes as \textbf{0}.
        \item Fir a standard Logistic Regression.
    \end{itemize}
    Example for Cat, Dog, Bird:
    \begin{itemize}
        \item Cat vs. Not Cat $\rightarrow$ 1 binary model.
        \item Dog vs. Not Dog $\rightarrow$ 1 binary model.
        \item Bird vs. Not Bird $\rightarrow$ 1 binary model.
    \end{itemize}
    For a new point, each binary model gives a probability. We pick the class with the \textbf{highest probability}.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] Simple, easy to implement.
        \item[\textcolor{Green3}{\faIcon{check}}] Works with any binary classifier.
        \item[\textcolor{Red2}{\faIcon{times}}] The probabilities may not sum to 1.
        \item[\textcolor{Red2}{\faIcon{times}}] Each binary classifier works independently, no global probability consistency.
    \end{itemize}
    \item \definition{Softmax Regression (Multinomial Logistic Regression)}. Handle \textbf{all classes together} in \textbf{one model} that directly estimates:
    \begin{equation}
        P(Y_i = k | X_i) \quad \text{for all } k = 1, \dots, K
    \end{equation}
    We extend the logit idea, but we use \textbf{softmax function} instead of sigmoid. For each class $k$:
    \begin{equation}
        P\left(Y_i = k | X_i\right) = \dfrac{
            e^{X_i^T \beta_k}
        }{
            \displaystyle\sum_{l=1}^{K} e^{X_i^T \beta_l}
        }
        =
        \dfrac{
            \exp\left(X_i^T \beta_k\right)
        }{
            \displaystyle\sum_{l=1}^{K} \exp\left(X_i^T \beta_l\right)
        }
    \end{equation}
    Each class gets its own $\beta_k$. The numerator says: ``\emph{how much weight does this class get for these X?}''. And the denominator ensures all probabilities add to \textbf{1}.

    This is called \definition{Multinomial Logistic Regression} or \definition{Softmax Regression}. 

    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] Probabilities for all classes are consistent and sum to 1.
        \item[\textcolor{Green3}{\faIcon{check}}] Single unified model.
        \item[\textcolor{Red2}{\faIcon{times}}] More parameters to estimate, then more complex than OvR.
    \end{itemize}
\end{itemize}
Both are standard extensions of Logistic Regression when our outcome has \textbf{more than two categories}.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Approach & How it works \\
        \midrule
        \textbf{1-vs-Rest}  & Split problem into multiple binary Logistic Regressions. \\ [.3em]
        \textbf{Softmax}    & One model directly predicting class probabilities jointly. \\
        \bottomrule
    \end{tabular}
\end{table}