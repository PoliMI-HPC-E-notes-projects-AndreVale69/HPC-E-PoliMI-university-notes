\subsubsection{Matrix Formulation}\label{subsubsection: Matrix Formulation}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why Use Matrix Form?}}
\end{flushleft}
Matrix notation gives us:
\begin{itemize}
    \item \textbf{Compact representation} of the model.
    \item A way to handle \textbf{any number of predictors} with one formula.
    \item Clean derivation of \textbf{OLS estimators}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Model in Matrix Form}}
\end{flushleft}
We rewrite:
\begin{equation*}
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \varepsilon
\end{equation*}
As:
\begin{equation}
    \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}    
\end{equation}
Where:
\begin{itemize}
    \item $\mathbf{y}$: $n \times 1$ column vector of observed responses.
    \item $\mathbf{X}$: $n \times (p+1)$ \textbf{design matrix}, including a column of ones for the intercept.
    \item $\boldsymbol{\beta}$: $(p+1) \times 1$ vector of coefficients.
    \item $\boldsymbol{\varepsilon}$: $n \times 1$ vector of errors (residuals)
\end{itemize}
For example, if we have two predictors (plus intercept):
\begin{equation*}
    \mathbf{X} =
    \begin{bmatrix}
    1 & x_{11} & x_{12} \\
    1 & x_{21} & x_{22} \\
    \vdots & \vdots & \vdots \\
    1 & x_{n1} & x_{n2}
    \end{bmatrix}
\end{equation*}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{OLS Solution Using Matrix Algebra}}
\end{flushleft}
To estimate the coefficients that minimize residual sum of squares:
\begin{equation}
    \hat{\boldsymbol{\beta}} = (\mathbf{X}^{T} \mathbf{X})^{-1} \mathbf{X}^{T} \mathbf{y}    
\end{equation}
This generalizes the simple regression formula to \textbf{any number of predictors}. It is the \definition{Ordinary Least Squares estimators in matrix form}.
\begin{itemize}
    \item $\mathbf{X}$: design matrix (rows are the data points, columns are the predictors $+1$).
    \item $\mathbf{X}^{T}$: transpose of $\mathbf{X}$.
    \item $\mathbf{X}^{T}\mathbf{X}$: matrix of sums of products, square and invertible if predictors are independent.
    \item $\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}$: inverse matrix, handles ``division'' in multidimensional space.
    \item $\mathbf{X}^{T}\mathbf{y}$: vector of dot products between predictors and output.
    \item $\hat{\beta}$: vector of estimated coefficients $\left[\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_p\right]$.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Predictions and Residuals}}
\end{flushleft}
\begin{itemize}
    \item \important{Fitted values}
    \begin{equation}
        \hat{\mathbf{y}} = \mathbf{X} \hat{\boldsymbol{\beta}}
    \end{equation}
    Where:
    \begin{itemize}
        \item $\hat{\mathbf{y}}$ is the vector of \textbf{fitted (predicted) values} for all our data points.
        \item $\mathbf{X}$ is the \textbf{design matrix} with all our predictors and a column of ones (for the intercept).
        \item $\hat{\beta}$ is the vector of \textbf{estimated coefficients}.
    \end{itemize}
    This gives us all the predicted outcomes using the regression line we just fit.

    \item \important{Residuals}
    \begin{equation}
        \hat{\boldsymbol{\varepsilon}} = \mathbf{y} - \hat{\mathbf{y}}
    \end{equation}
    Where:
    \begin{itemize}
        \item $\hat{\boldsymbol{\varepsilon}}$ is the vector of \textbf{residuals}, the errors the model made.
        \item The residuals is the difference between:
        \begin{itemize}
            \item $\mathbf{y}$ is the \textbf{true observed values}.
            \item $\hat{\mathbf{y}}$ is the \textbf{predicted values} from your model. It is used for evaluating how well the model fits.
        \end{itemize}
    \end{itemize}
    In other words: ``\emph{how far off was our prediction from the actual result?}''.
\end{itemize}
