\subsection{Multiple Linear Regression (MLR)}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why Move Beyond Simple Linear Regression?}}
\end{flushleft}
Up until now, we've modeled a response variable $Y$ as a function of \textbf{just one predictor} $X$:
\begin{equation*}
    \hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X
\end{equation*}
But in real-world problems, the outcome usually depends on \textbf{more than one factor}.

\highspace
For example, let's say we're predicting a house price.
\begin{itemize}
    \item One predictor might be: square footage
    \item But what about: number of bedrooms? location? year built?
\end{itemize}
That's where \textbf{Multiple Linear Regression} comes in.

\highspace
\definition{Multiple Linear Regression (MLR)} generalizes the model to include \textbf{multiple independent variables}:
\begin{equation}
    \hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \dots + \hat{\beta}_p X_p
\end{equation}
Where:
\begin{itemize}
    \item $Y$: outcome.
    \item $X_1, X_2, \dots, X_p$: predictors.
    \item $\beta_0$: intercept.
    \item $\beta_j$: effect of predictor $j$.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why use MLR?}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l | l @{}}
            \toprule
            Use Case & MLR Benefit \\
            \midrule
            We want better predictions & More predictors, more information \\ [.5em]
            We want to isolate effects & Estimate each variable's impact, \textbf{controlling for others} \\ [.5em]
            We want realism & Real systems depend on \textbf{many factors}, not just one \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\newpage

\subsubsection{Model Expansion}\label{subsubsection: Model Expansion}

So far, we've worked with:
\begin{equation*}
    Y = \beta_0 + \beta_1 X + \varepsilon
\end{equation*}
This is a \textbf{simple linear regression (SLR)} with only \textbf{one predictor} (page \pageref{subsection: Simple Linear Regression}).

\highspace
We now expand this to:
\begin{equation}
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \varepsilon
\end{equation}
This is \definition{Multiple Linear Regression (MLR)} with $p$ predictors.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{What's new here?}}
\end{flushleft}
We now have \textbf{multiple input variables}, not just one $X$, but many contributions. Each gets its own coefficient $\beta_j$, representing its \textbf{independent contribution} to predicting $Y$.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Intuition}}
\end{flushleft}
Each $\beta_j$ answers: ``\emph{if we increase $X_j$ by 1 unit, how much does $Y$ change, \textbf{holding all the other $X$'s constant}?}''. The last part is critical because, in MLR, we \hl{isolate the effect of each variable while controlling for the effects of the others}.

\highspace
For example, let's say we model student grades:
\begin{equation*}
    \hat{Y} = \beta_0 + \beta_1 \cdot \text{StudyTime} + \beta_2 \cdot \text{Sleep} + \beta_3 \cdot \text{Caffeine}
\end{equation*}
Then:
\begin{itemize}
    \item $\beta_1$: how grades change \textbf{per hour of study}, keeping sleep and caffeine \underline{constant}.
    \item $\beta_3$: how grades change \textbf{per mg of caffeine}, keeping study and sleep \underline{constant}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Beware: Correlation Between Predictors}}
\end{flushleft}
If predictors (e.g., Sleep and StudyTime) are \textbf{highly correlated}, it becomes harder to isolate the effect of one from the other. This is known as \textbf{multicollinearity}, and it can:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item Make coefficients unstable.
    \item Inflate standard errors.
    \item Confuse interpretation.
\end{itemize}
More formally, \definition{Multicollinearity} occurs when two or more predictor variables in a multiple regression model are \textbf{highly correlated} with each other. This means they carry \textbf{overlapping information} about the outcome $Y$, making it hard for the model to distinguish their individual effects.

\highspace
\textcolor{Red2}{\faIcon{question-circle} \textbf{Why Multicollinearity is a Problem?}} If $X_1$ and $X_2$ are strongly correlated, the model can't \textbf{tell who's responsible} for changes in $Y$. The estimated coefficients $\hat{\beta}_1$, $\hat{\beta}_2$ become \textbf{unstable}:
\begin{itemize}
    \item They can change drastically with small changes in data.
    \item They may have \textbf{unexpected signs} (positive/negative).
    \item Their \textbf{standard errors} become large, then \textbf{t-tests become unreliable}.
\end{itemize}
For example, image this: $X_1 = \text{StudyTime}$, $X_2 = \text{TimeAtLibrary}$. If students who study more also spend more time at the library, then $X_1 \approx X_2$. So the regression model says: ``\emph{wait... both variables kind of say the same thing. Who gets the credit?}''. The answer is that the model doesn't know, and the estimates become inaccurate.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How to detect Multicollinearity?}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l p{25em} @{}}
            \toprule
            Method & Description \\
            \midrule
            Correlation matrix & Check if predictors are highly correlated \\ [.3em]
            Variance Inflation Factor (VIF) & Measures how much a variable is explained by other predictors \\ [.3em]
            Unstable coefficients & Coefficients flip sign or have huge standard errors \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{When is it \emph{not} a problem?}}
\end{flushleft}
Multicollinearity \textbf{does not hurt prediction accuracy} (if all predictors are used). But it \underline{does} hurt Interpretability, and Statistical Inference (t-test and p-values).