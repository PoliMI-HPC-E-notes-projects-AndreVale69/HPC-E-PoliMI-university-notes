\subsubsection{Geometric Interpretation of Linear Regression}

Understanding OLS \textbf{geometrically} gives deep intuition about what's really happening during the model fitting process.

\highspace
\textcolor{Red2}{\faIcon{tools} \textbf{Setup.}} Suppose we have a dataset:
\begin{itemize}
    \item We observe $n$ responses and save them in a vector of observed responses $\mathbf{Y} \in \mathbb{R}^{n}$:
    \begin{equation*}
        \mathbf{Y} = \left[Y_1, Y_2, \dots, Y_n\right]^{T}
    \end{equation*}
    \item We have one predictor $X$, but the model includes a constant (intercept) too. So the \textbf{design matrix} $\mathbf{X} \in \mathbb{R}^{n \times 2}$ has:
    \begin{equation*}
        \mathbf{X} = \begin{bmatrix}
            1       & X_{1}     \\
            1       & X_{2}     \\
            \vdots  & \vdots    \\
            1       & X_{n}     \\
        \end{bmatrix}
    \end{equation*}
    The column of ones is fundamental because, to calculate the regression line $\hat{\mathbf{Y}}$ in matrix form, we need to $\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$, which translates to:
    \begin{equation*}
        \hat{\mathbf{Y}} =
        \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon} =
        \begin{bmatrix}
            1 & X_{1} \\
            1 & X_{2} \\
            \vdots & \vdots \\
            1 & X_{n} \\
        \end{bmatrix}
        \begin{bmatrix}
            \beta_0 \\ \beta_1
        \end{bmatrix} + \boldsymbol{\varepsilon} =
        \begin{bmatrix}
            \beta_0 + \beta_1 X_1 \\
            \beta_0 + \beta_1 X_2 \\
            \vdots + \vdots \\
            \beta_0 + \beta_1 X_n \\
        \end{bmatrix} + \boldsymbol{\varepsilon}
    \end{equation*}
    So the column of ones ensures the \textbf{intercept} $\beta_0$ is part of every prediction. Without the column of ones, we would be \textbf{forcing the line to pass through the origin}, i.e., no intercept!
\end{itemize}
Now, the column space of $\mathbf{X}$ is all combinations of the two vectors: the all-ones vector (for the intercept), and the values of $X$:
\begin{equation*}
    \text{Col}\left(\mathbf{X}\right) = \left\{\beta_0 \cdot \mathbf{1} + \beta_1 \cdot X\right\}
\end{equation*}
This is a \textbf{2D plane inside} $\mathbb{R}^{n}$.

\highspace
\textcolor{Red2}{\faIcon{check-circle} \textbf{Goal.}} The goal of OLS is to \textbf{project} the vector $\mathbf{Y}$ onto the \textbf{space spanned by the predictors} in $\mathbf{X}$. In other words, since it is impossible to fit all our observed values ($\mathbf{Y}$) perfectly with a line, \textbf{OLS selects a vector} $\hat{\mathbf{Y}}$ \textbf{in the column space of} $\mathbf{X}$ \textbf{that is as close as possible to} $\mathbf{Y}$. This is done by \hl{orthogonal projection}:
\begin{equation*}
    \begin{array}{rcl}
        \hat{\mathbf{Y}} &=& \text{projection of } \mathbf{Y} \text{ onto the space spanned by the columns of } \mathbf{X} \\ [.3em]
        &=& \mathbf{X}\hat{\beta} \\ [.3em]
        &=& \text{projection of } \mathbf{Y} \text{ onto Col}\left(\mathbf{X}\right)
    \end{array}
\end{equation*}
It is important because the \textbf{projection minimizes the distance} from $\mathbf{Y}$ (our observed values) to $\hat{\mathbf{Y}}$ (regression line to find), exactly what OLS does.

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/linear-regression/slr-geometric-explanation.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{Blue dots are the actual observed $Y_i$ values, the full response vector $\mathbf{Y}$. The red dots are the projected values $\hat{Y}_i$, they lie in the space spanned by the columns of $\mathbf{X}$. The gray dashed lines are the residuals, the vertical distance from each observed $Y_i$ to its projected $\hat{Y}_i$. The black line is the fitted regression line (through the red points), this lies in the \textbf{column space} of the design matrix.
    
    \highspace
    The design matrix $\mathbf{X}$ has two columns: one column of ones (to include the intercept) and one column with the values of $X$. The red points from the best approximation $\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta}$. The black line shows the \textbf{vector space of all possible} $\hat{Y}$ values using linear combinations of the intercept and $X$. \textbf{OLS chooses} the projection of $\mathbf{Y}$ \textbf{onto that line}, minimizing the \textbf{sum of squared residuals}.}
\end{figure}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{square-root-alt} \textbf{Vector Space View}}
\end{flushleft}
In Simple Linear Regression, the design matrix is, as we have seen, as follows:
\begin{equation*}
    \mathbf{X} =
    \begin{bmatrix}
        1 & X_{1} \\
        1 & X_{2} \\
        \vdots & \vdots \\
        1 & X_{n} \\
    \end{bmatrix}
\end{equation*}
OLS finds the \textbf{orthogonal projection} of $\mathbf{Y}$ onto this space:
\begin{equation}
    \hat{\mathbf{Y}} = \mathbf{P}_X \mathbf{Y}
\end{equation}
Where $\mathbf{P}_X$ is called \definition{Projection Matrix}, or \definition{Hat Matrix} because it maps $\mathbf{Y}$ to its fitted values $\hat{\mathbf{Y}}$. It is defined as follows:
\begin{equation}\label{eq: Projection Matrix}
    \mathbf{P}_X = \mathbf{X}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T}
\end{equation}
It \textbf{projects} any vector in $\mathbb{R}^{n}$ (like $\mathbf{Y}$) onto the \textbf{column space of} $\mathbf{X}$, that is, onto the space of all possible predicted values. 

\highspace
Finally, the residual vector is:
\begin{equation}
    \mathbf{e} = \mathbf{Y} - \hat{\mathbf{Y}}
\end{equation}
That is \textbf{orthogonal} to the fitted vector $\hat{\mathbf{Y}}$.