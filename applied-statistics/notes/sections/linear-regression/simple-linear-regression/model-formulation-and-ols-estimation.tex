\subsection{Simple Linear Regression (SLR)}

\subsubsection{Model Formulation and OLS Estimation}

The \definition{Simple Linear Regression (SLR)} is a \textbf{statistical method} that models the relationship between one \textbf{independent variable} (also called predictor or input) $X$ and one \textbf{dependent variable} (also called response or output) $Y$, using a \textbf{straight line}. The model assumes:
\begin{equation}
    Y_{i} = \beta_{0} + \beta_{1} X_{i} + \varepsilon_{i}
\end{equation}
Where:
\begin{itemize}
    \item $Y_i$: \textbf{observed response} (output) for sample $i$. It is the value we want to predict.
    \item $X_i$: \textbf{predictor} (input). It is the value we use to predict.
    \item $\beta_0$: \textbf{intercept} (value of $Y$ when $X = 0$).
    \item $\beta_1$: \textbf{slope} (change in $Y$ for a unit increase in $X$). It indicates how much $Y$ increases when $X$ increases by 1.
    \item $\varepsilon_i$: \textbf{random error} (assumed to be $\sim \mathcal{N}\left(0, \sigma^{2}\right)$, in other words normally distributed).
\end{itemize}
It can also be viewed in matrix form:
\begin{equation*}
    \mathbf{Y} = X \boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{equation*}
It's called \textbf{simple} because there is only one predictor ($X$) and if there were multiple predictors $X_{1}, X_{2}, \dots$, we would call it \textbf{multiple linear regression}.

\highspace
\textcolor{Green3}{\faIcon{check-circle} \textbf{Goal}}. We want to \textbf{predict a continuous outcome} $Y \in \mathbb{R}$ based on a single input variable $X \in \mathbb{R}$ using a \textbf{linear relationship}. For example, predicting house price $Y$ based on house size $X$.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Key Idea}}
\end{flushleft}
We want to find the \textbf{best straight line}:
\begin{equation*}
    \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X
\end{equation*}
That minimizes the \textbf{total squared error} between the actual $Y$ and the predicted $\hat{Y}$.

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/linear-regression/slr.pdf}
    \caption{This is an example of an SLR model in action. The X-axis represents the independent variable, hours studied, the Y-axis represents the dependent variable, exam score, the orange dots represent the actual observed data points, and the black line represents the best-fit regression line $\hat{Y} = \hat{\beta}_{0} + \hat{\beta}_{1} X$. The OSL finds the straight line that best represents the trend in the data; here, students who study more tend to get higher exam scores. The line summarize that linear relationship with just 2 numbers: intercept and slope.}
\end{figure}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is OSL Estimation (Ordinary Least Squares)?}}
\end{flushleft}
\definition{Ordinary Least Squares Estimation (OSL Estimation)} is a method \textbf{to find the best-fitting line} for our data by \textbf{minimizing the total squared errors} between observed values and predicted values.

\highspace
Given the SLR model, we don't know $\beta_{0}$ and $\beta_{1}$. We want to \textbf{estimate them} using data $\left(x_{1}, y_{1}\right), \left(x_{2}, y_{2}\right), \dots, \left(x_{n}, y_{n}\right)$. We define the prediction as:
\begin{equation*}
    \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i
\end{equation*}
And the \textbf{error (residual)} for point $i$ is:
\begin{equation}
    e_i = Y_i - \hat{Y}_i
\end{equation}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What does OLS Minimize?}}
\end{flushleft}
OLS finds $\hat{\beta}_0$ and $\hat{\beta}_1$ that \textbf{minimize the sum of squared residuals}:
\begin{equation}
    \text{RSS} = \displaystyle\sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i)^2
\end{equation}
This is called the \definition{Residual Sum of Squares (RSS)}. 

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Core Idea of RSS}}
\end{flushleft}
We want our regression line to be \textbf{as close as possible} to the observed data points. But \emph{how do we measure ``closeness''?} By the \textbf{vertical distance} between each true point $\left(X_{i}, Y_{i}\right)$ and the predicted point $(X_{i}, \hat{Y}_{i})$, that is:
\begin{equation*}
    e_i = Y_i - \hat{Y}_i \quad \text{(residual)}
\end{equation*}
But some residuals are negative, some positive. So \textbf{we square them to avoid cancellation and emphasize large errors}:
\begin{equation*}
    \text{RSS} = \displaystyle\sum e_{i}^{2}
\end{equation*}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/linear-regression/ols-regression.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{In this plot, we highlight the residual and the residual sum of squares (RSS). The blue dots are the actual data point $(X_i, Y_i)$, the red line is the regression line $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$, the gray dashed lines are the residuals $e_{i} = Y_{i} - \hat{Y}_{i}$, i.e., vertical distances from each point to the line, and the RSS value is displayed in the title, it's the sum of the squares of these residuals.
    
    OLS chooses the red line that \textbf{minimizes the total gray distances squared (RSS)}. A different line (e.g., tilted more or less) would \textbf{increase the sum of squared errors} (see Figure \ref{fig: poor ols regression}, page \pageref{fig: poor ols regression}). This is \textbf{why} OLS minimizes RSS, it leads to the line that best balances all errors.}
\end{figure}

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/linear-regression/poor-ols-regression.pdf}
    \includegraphics[width=\textwidth]{img/linear-regression/poor-ols-regression-2.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{This is an example of a poor linear fit, which is also highlighted by the RSS. The purple dots are the true data points, which follow a \textbf{nonlinear (sinusoidal) trend}, green line is the best-fitting \textbf{straight line} found by OLS, the gray dashed lines are the \textbf{residuals} (the vertical errors between data points and the regression line), and the RSS is very high because the straight line fails to capture the curve in the data.
    
    OLS always minimizes the \textbf{sum of squared residuals}, but if the model is misspecified (e.g., fitting a line to curved data), the \textbf{RSS will still be high}.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check-circle}}] Good fit $\Rightarrow$ \textbf{low RSS}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] Bad fit $\Rightarrow$ \textbf{high RSS}
    \end{itemize}
    OLS chooses the line that makes RSS as small as possible, even if the fit is poor due to model limitations.}
    \label{fig: poor ols regression}
\end{figure}
