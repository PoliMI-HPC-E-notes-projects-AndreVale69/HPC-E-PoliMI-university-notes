\section{Model Selection and Regularization}

\subsection{Motivation \& Problem Setup}

\textcolor{Green3}{\faIcon{check-circle} \textbf{The Goal.}} We learned \emph{how} to evaluate a model and which key factors to consider. Now, we want \textbf{\emph{to build}} a model that is as simple as possible, but still accurate when predicting new data. In other words, to \textbf{find a linear model that is accurate}, \textbf{stable}, and \textbf{easy to interpret}. More precisely, we want to:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textbf{Make Good Predictions}. \hl{Not just on training data, but on new unseen data}. This is why we care about generalization and not just high $R^{2}$ (coefficient of determination, page \pageref{eq: coefficient of determination}).
    \item \textbf{Avoid Overfitting}. Including too many predictors, our model fits noise. We want to choose only the useful variables.
    \item \textbf{Reduce Variance of Coefficients}. If variables are correlated, coefficients jump around a lot. We want stable estimates even if the data changes slightly.
    \item \textbf{Interpret the Model Easily}. Simpler models are easier to explain. Sparse models (with some $\beta_{j} = 0$) highlight the important variables.
\end{itemize}

\highspace
\textcolor{Green3}{\faIcon{tools} \textbf{Setup: Linear Regression.}} We have data:
\begin{itemize}
    \item $Y$: outcome (like house price, disease risk, exam score)
    \item $X_1, X_2, \dots, X_p$: features (like square footage, age, sleep hours)
\end{itemize}
We want to model:
\begin{equation*}
    Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \varepsilon
\end{equation*}
Using Ordinary Least Square (OLS, page \pageref{def: Ordinary Least Squares Estimation}), we estimate:
\begin{equation*}
    \hat{\beta} = \left(X^{T} X\right)^{-1} X^T Y
\end{equation*}
It gives us the ``best'' coefficients to fit the data (minimize squared error).

\highspace
\textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Problems.}} However, this approach has some problems:
\begin{itemize}
    \item \important{Too Many Variables?} The more variables we add, the higher $R^{2}$ goes. But adding variables blindly causes \textbf{overfitting}, and the model fits noise, not the true signal.

    \item \important{Multicollinearity} (page \hqpageref{def: multicollinearity}). If two variables $X_1 \approx X_2$, the OLS estimate becomes unstable. Interpretation of $\beta_j$ becomes meaningless. The \textbf{variance} of the coefficients explodes:
    \begin{equation*}
        \text{Var}\left(\hat{\beta}\right) = \sigma^{2} \left(X^{T} X\right)^{-1}
    \end{equation*}
    If $X^TX$ is close to singular (not invertible), $\hat{\beta}$ becomes extremely variable!

    \item \important{High Variance, Low Bias}. OLS has \textbf{low bias} (accurate on average), but if predictors are correlated or too many, \textbf{variance} is high, then \textbf{poor generalization}.
\end{itemize}