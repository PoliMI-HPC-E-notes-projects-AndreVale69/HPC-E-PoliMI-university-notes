\subsubsection{Backward Stepwise}\label{subsubsection: Backward Stepwise}

The key idea of \definition{Backward Stepwise Selection} is to start with \textbf{all predictors} in the model. At each step, \textbf{remove} the \textbf{least useful} predictor, the one whose removal \textbf{improves} (or least worsens) the model based on a criterion like RSS, AIC, BIC, etc. Conceptually, Backward Stepwise Selection is the inverse of Forward Stepwise Selection. However, even though they move in opposite directions, they \textbf{don't necessarily reach the same model}. Since they are greedy algorithms, different paths can potentially lead to different final models.

\highspace
It is \textbf{used} when the \textbf{number of predictors} ($p$) is \textbf{not too large}, or when we want a simpler model but \textbf{don't want to try all possible combinations} (as in best subset selection).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How it works}}
\end{flushleft}
Assume we have 5 predictors: $X_1, X_2, X_3, X_4, X_5$.
\begin{enumerate}
    \item Start with the \textbf{full model}:
    \begin{equation*}
        Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_5 X_5 + \varepsilon
    \end{equation*}

    \item Fit \textbf{5 reduced models}, each omitting one predictor:
    \begin{itemize}
        \item $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4$
        \item $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_5 X_5$
        \item $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_4 X_4 + \beta_5 X_5$
        \item $Y = \beta_0 + \beta_1 X_1 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5$
        \item $Y = \beta_0 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5$
    \end{itemize}
    Choose the one with \textbf{best score} (e.g., lowest AIC). Suppose omitting $X_4$ is best. Now the model is:
    \begin{equation*}
        Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_5 X_5
    \end{equation*}

    \item Try removing one of the 4 remaining predictors:
    \begin{itemize}
        \item $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3$
        \item $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_5 X_5$
        \item $Y = \beta_0 + \beta_1 X_1 + \beta_3 X_3 + \beta_5 X_5$
        \item $Y = \beta_0 + \beta_2 X_2 + \beta_3 X_3 + \beta_5 X_5$
    \end{itemize}
\end{enumerate}
Repeat until stopping rule is triggered (e.g., no further improvement, or reaching a minimum number of variables).

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Assumptions and Limitations}}
\end{flushleft}
The Backward Stepwise Selection \textbf{cannot be used if} $p > n$ (\textbf{more variables than observations}), since the full model cannot be fit. Also, the algorithm is Greedy, so when \textbf{once a variable is remove, it's gone forever}, even if it would be useful with a different combination.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why can't the number of variables be greater than the number of observations?}}
\end{flushleft}
Linear regression solves for:
\begin{equation*}
    \boldsymbol{\hat{\beta}} = \left(X^{T} X\right)^{-1} X^{T} y
\end{equation*}
Where:
\begin{itemize}
    \item $X$ is the \textbf{design matrix}: it has $n$ rows (observations) and $p$ columns (predictors).
    \item $\hat{\beta}$ is the vector of estimated coefficients.
    \item $\left(X^{T} X\right)^{-1}$ is the \textbf{inverse} of the $p \times p$ matrix $X^\top X$
\end{itemize}
But this formula only works if $X^{T} X$ is invertible.

\highspace
\textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{The problem.}} An invertible matrix must be \textbf{full-rank}. Also, a matrix $X \in \mathbb{R}^{n \times p}$ can have:
\begin{equation*}
    \text{at most rank} = \min\left(n, p\right)
\end{equation*}
Since the size of the inverse matrix is determined by that of the design matrix and its transpose $X^T \in \mathbb{R}^{p \times n}$, we must ensure that the rank of the design matrix is equal to or less than that of its inverse.

\highspace
For example, let's say we have:
\begin{itemize}
    \item $n = 2$ observations
    \item $p = 3$ predictors
\end{itemize}
So $X$ might look like:
\begin{equation*}
    X = \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6
    \end{bmatrix} \quad \text{(2 rows, 3 columns)}
\end{equation*}
Then:
\begin{equation*}
    X^\top X = \begin{bmatrix}
        1 & 4 \\
        2 & 5 \\
        3 & 6
    \end{bmatrix} \cdot \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6
    \end{bmatrix} = \begin{bmatrix}
        17 & 22 & 27 \\
        22 & 29 & 36 \\
        27 & 36 & 45
    \end{bmatrix} \Rightarrow \text{a 3$\times$3 matrix with rank $\le$ 2}
\end{equation*}
The rank is less than or equal to two because the rank of a matrix product cannot exceed that of its original matrices. More precisely:
\begin{equation*}
    \text{rank}\left(X^{T} X\right) \leq \text{rank}(X)
\end{equation*}
So if $p > n$, then the product $X^{T} X$ is \textbf{singular} and \textbf{not invertible} (because $\text{rank}(X) = \text{rank}\left(X^{T} X\right)$ and it is not full rank for $X^{T} X$), and the regression coefficients are \textbf{not computable}:
\begin{equation*}
    \hat{\beta} = \left(X^{T} X\right)^{-1}X^{T} y
\end{equation*}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{Forward vs Backward}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l l @{}}
        \toprule
        Feature                             & Forward Stepwise      & Backward Stepwise \\
        \midrule
        \textbf{Starts with}                & No predictors         & All predictors                    \\ [.3em]
        \textbf{Adds / Removes}             & Adds one at a time    & Removes one at a time             \\ [.3em]
        \textbf{Requires full model fit?}   & No                    & Yes (so needs $p \le n$)          \\ [.3em]
        \textbf{Greedy?}                    & Yes                   & Yes                               \\ [.3em]
        \textbf{Typical usage}              & When $p$ is \hl{large}& When $p$ is \hl{small} ($p < n$)  \\
        \bottomrule
    \end{tabular}
\end{table}