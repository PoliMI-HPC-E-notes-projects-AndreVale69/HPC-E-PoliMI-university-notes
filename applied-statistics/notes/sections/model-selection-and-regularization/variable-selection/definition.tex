\subsection{Variable Selection}

\subsubsection{Definition}

\definition{Variable Selection} (or \definition{Feature Selection}) is the process of choosing \textbf{which predictors ($X$'s)} to include in our model.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why do we need it?}}
\end{flushleft}
In real-world data, we often have \textbf{many predictors} and some are:
\begin{itemize}
    \item Irrelevant
    \item Redundant
    \item Weakly related to $Y$
    \item Strongly correlated with others (collinearity)
\end{itemize}
Then including all variables can:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item Inflate variance (\emph{remember multicollinearity?}).
    \item Make the model \textbf{overfit} to noise.
    \item Reduce interpretability.
    \item Add unnecessary complexity.
\end{itemize}
So we want to \textbf{select a subset of variables} such that:
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item Gives good predictive performance.
    \item Keeps the model simple and interpretable.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{book} \textbf{Model Selection Problem}}
\end{flushleft}
Given a full set of $p$ predictors:
\begin{equation*}
    Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \varepsilon
\end{equation*}
We want to find a \textbf{smaller model} (called \emph{Model Selection Problem}):
\begin{equation*}
    Y = \beta_0 + \sum_{j \in S} \beta_j X_j + \varepsilon
\end{equation*}
Where $S$ is a subset of the predictors (i.e., the best combination). The \definition{Model Selection Problem} is the challenge of choosing \textbf{the best model} from a \textbf{collection of candidate models}. We want to \textbf{fine the one} that best explains or predicts the data without overfitting.

\highspace
Formally, given a set of candidate models $\mathcal{M}_1, \dots, \mathcal{M}_K$, find the model $\mathcal{M}_j$ that \textbf{minimizes a chosen criterion}, such as: Validation error, AIC / BIC, Cross-validation error, Mallows' $C_p$, Adjusted $R^2$.

\highspace
\textcolor{Green3}{\faIcon{question-circle} \textbf{Why is this so important? We can do it by hand, right?}} Say you have $p = 10$ predictors: $X_1, X_2, \dots, X_{10}$. There are $2^{10} = 1024$ possible subsets of predictors! Each subset defines a \textbf{different model}:
\begin{equation*}
    Y = \beta_0 + \sum_{j \in S} \beta_j X_j + \varepsilon
\end{equation*}
Where $S \subset \{1, 2, \dots, 10\}$. So the \textbf{model selection problem} becomes: \emph{which subset $S$ of predictors gives the best model?}

\highspace
\textcolor{Green3}{\faIcon{question-circle} \textbf{What makes a ``Good'' subset?}} That depends on our goal. We might want the model that:
\begin{itemize}
    \item Has the \textbf{lowest test error} (prediction accuracy).
    \item Uses the \textbf{fewest predictors} (simplicity).
    \item Balances both (e.g., via AIC, BIC, cross-validation).
\end{itemize}

\highspace
\textcolor{Green3}{\faIcon{balance-scale} \textbf{Tradeoff: Bias vs. Variance.}} Choosing too many variables:
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check}}] Low bias (flexible).
    \item[\textcolor{Red2}{\faIcon{times}}] \textbf{High variance}, then overfits noise.
\end{itemize}
But, choosing too few:
\begin{itemize}
    \item[\textcolor{Red2}{\faIcon{times}}] High bias (underfits).
    \item[\textcolor{Green3}{\faIcon{check}}] Low variance.
\end{itemize}
So model selection is about finding the \textbf{right tradeoff}.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/model-selection-and-regularization/intro-to-variable-selection.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{\textbf{Summary of the bias-variance tradeoff}.
    \begin{itemize}
        \item \textbf{High Bias (Underfit)} (top). Linear model (degree 1) too simple to capture the true pattern (a sine wave).
        \begin{itemize}
            \item[\textcolor{Red2}{\faIcon{times}}] \textbf{High bias}: Misses key features.
            \item[\textcolor{Green3}{\faIcon{check}}] \textbf{Low variance}: Prediction is stable.
        \end{itemize}
        \item \textbf{Just Right} (middle). Polynomial of degree 3 that captures the main structure without chasing noise. Good generalization: \textbf{low bias}, \textbf{low variance}.
        \item \textbf{High Variance (Overfit)} (bottom). Polynomial of degree 15 that fits noise in the training data perfectly.
        \begin{itemize}
            \item[\textcolor{Red2}{\faIcon{times}}] \textbf{High variance}: Prediction is unstable.
            \item[\textcolor{Red2}{\faIcon{times}}] \textbf{Too low bias}: Too flexible, will perform poorly on new data.
        \end{itemize}
        \textcolor{Green3}{\faIcon{question-circle} \textbf{Why does the true function appear as a line rather than a sine wave?}} Because the overfit model (a high-degree polynomial) produces such large, wiggly oscillations that it stretches the vertical axis to fit them. As a result:
        \begin{itemize}
            \item The \textbf{true function}, which is a smooth sine wave, has relatively small amplitude (between $-1$ and $1$).
            \item The \textbf{fitted curve} might swing up to $+10$ or down to $-10$ or more.
            \item This \textbf{resizes the entire plot}, making the sine wave appear almost flat, like a straight line.
        \end{itemize}
    \end{itemize}}
\end{figure}

\highspace
\textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{How hard is it to find a \emph{good subset}?}} Short answer: \textbf{hard}. Because the number of models grows \textbf{exponentially} with the number of predictors. Also, test error isn't known in advance, we must estimate it. And finally, some variables are \textbf{marginally useless} but \textbf{jointly helpful}, and vice versa. So it's not so free.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Common Strategies}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l @{}}
            \toprule
            Method & Idea \\
            \midrule
            \textbf{Best Subset Selection} (page \pageref{subsubsection: Best Subset Selection})    & Try all possible combinations and choose the best.           \\ [.3em]
            \textbf{Forward Stepwise} (page \pageref{subsubsection: Forward Stepwise})              & Start with none, add one at a time.                        \\ [.3em]
            \textbf{Backward Stepwise} (page \pageref{subsubsection: Backward Stepwise})            & Start with all, remove one at a time.                      \\ [.3em]
            \textbf{Ridge Regression} (page \pageref{paragraph: Ridge Regression})                  & Shrink coefficients, never removes variables (L2 norm).    \\ [.3em]
            \textbf{Lasso} (page \pageref{paragraph: Lasso})                                        & Shrink and set some coefficients to 0 (L1 norm).       \\ [.3em]
            \textbf{Elastic Net}            & Combines Ridge and Lasso.                                    \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}