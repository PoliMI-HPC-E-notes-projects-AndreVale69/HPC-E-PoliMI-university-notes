\subsubsection{Forward Stepwise}\label{subsubsection: Forward Stepwise}

The goal of \definition{Forward Stepwise Selection} is to find a parsimonious model that \textbf{balances predictive power} with \textbf{simplicity}, \hl{without evaluating} \textbf{all possible} combinations (as in best subset selection, page \pageref{subsubsection: Best Subset Selection}, which is computationally expensive).

\highspace
The key idea is to \textbf{start with no predictors}. At each step, \textbf{add the predictor} that improves the model the most, \textbf{given the current set of predictors}. Continue until:
\begin{itemize}
    \item A maximum number of predictors is reached;
    \item No significant improvement occurs;
    \item A criterion (like BIC or cross-validation error) suggests stopping.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How it works}}
\end{flushleft}
Assume we have 5 predictors: $X_1, X_2, X_3, X_4, X_5$.
\begin{enumerate}
    \item Start with the \textbf{null model}: $\hat{Y} = \bar{Y}$. It is the mean of the response variable $Y$, that is:
    \begin{equation*}
        \bar{Y} = \dfrac{1}{n}\displaystyle\sum_{i=1}^{n} Y_i
    \end{equation*}
    \item Fit \textbf{5 models}, each with one predictor:
    \begin{itemize}
        \item Model 1: $Y = \beta_0 + \beta_1 X_1$
        \item Model 2: $Y = \beta_0 + \beta_2 X_2$
        \item Model 3: $Y = \beta_0 + \beta_3 X_3$
        \item Model 4: $Y = \beta_0 + \beta_4 X_4$
        \item Model 5: $Y = \beta_0 + \beta_5 X_5$
    \end{itemize}
    Choose the one with \textbf{lowest RSS} or \textbf{best score}. For example, we suppose it's $X_3$.
    \item Now fit \textbf{4 models}, each adding one of the remaining variables to $X_3$:
    \begin{itemize}
        \item Model 1: $Y = \beta_0 + \beta_3 X_3 + \beta_1 X_1$
        \item Model 2: $Y = \beta_0 + \beta_3 X_3 + \beta_2 X_2$
        \item Model 3: $Y = \beta_0 + \beta_3 X_3 + \beta_4 X_4$
        \item Model 4: $Y = \beta_0 + \beta_3 X_3 + \beta_5 X_5$
    \end{itemize}
    Choose the best again. For example, we suppose it's $X_4$. Now the model is: $Y = \beta_0 + \beta_3 X_3 + \beta_4 X_4$.
\end{enumerate}
We repeat until a stopping rule is met (e.g. validation error increases).

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{\speedIcon}}] \textbf{Much faster} than best subset selection (linear vs exponential).
    \item Often yields very similar results.
    \item Can be combined with information criteria (AIC/BIC), validation, etc.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{times-circle} \textbf{Disadvantages}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Greedy} method\footnote{%
        A Greedy method is an algorithm that at each step, makes the best immediate choice without considering the long-term consequences.
    }: once a variable is added, it \textbf{stays}, even if a better subset is possible without it.
    \item Might \textbf{miss the best model} if combinations are more powerful than incremental gains.
\end{itemize}