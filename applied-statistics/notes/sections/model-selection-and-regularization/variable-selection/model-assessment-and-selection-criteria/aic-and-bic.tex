\paragraph{AIC and BIC}

\definition{Akaike Information Criterion (AIC)}, as named by its inventor \href{https://en.wikipedia.org/wiki/Hirotugu_Akaike}{Hirotugu Akaike}, and \definition{Bayesian Information Criterion (BIC)}, as named by its inventor \href{https://www.jstor.org/stable/2958889}{Gideon Schwarz}\cite{bicGideonSchwarz}, are \textbf{model selection criteria} that balance two goals:
\begin{enumerate}
    \item \textbf{How well the model fits the data} (like low RSS, page \pageref{eq: RSS}, or high likelihood).
    \item \textbf{How simple the model is} (fewer predictors).
\end{enumerate}
In simple terms, they answer this question: ``\emph{among many models, which one likely generalizes best to new data?}''.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{square-root-alt} \textbf{Formulas}}
\end{flushleft}
Both are based on the \textbf{log-likelihood} of the mode, how well it fits, and then \textbf{penalize} it based on the number of parameters. So, they both start with the \textbf{log-likelihood} of the model (i.e., how likely is the data under this model), and then \textbf{penalize} it for how many parameters the model uses.

\highspace
For a linear model with Gaussian errors:
\begin{equation}
    \text{AIC} = n \cdot \log\left(\dfrac{\text{RSS}}{n}\right) + \underbrace{2d}_{\text{penalty}}
\end{equation}
\begin{equation}
    \text{BIC} = n \cdot \log\left(\dfrac{\text{RSS}}{n}\right) + \underbrace{d \cdot \log(n)}_{\text{penalty}}
\end{equation}
Where:
\begin{itemize}
    \item $n$: number of observations.
    \item $d$: number of predictors (model complexity).
    \item $\text{RSS}$: residual sum of squares (fit quality, see page \pageref{eq: RSS}).
    \item[\textcolor{Green3}{\faIcon{check-circle}}] Both \textbf{reward} good fit (\textbf{low RSS}).
    \item[\textcolor{Red2}{\faIcon{times-circle}}] Both \textbf{penalize} large $d$ (\textbf{complexity}, too many predictors).
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{What's the difference?}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l l @{}}
        \toprule
        Criterion & Penalty for complexity & Tends to favor...  \\
        \midrule
        AIC & $2d$          & Slightly more complex models      \\ [.3em]
        BIC & $d \cdot \log(n)$   & Simpler models, stronger penalty  \\
        \bottomrule
    \end{tabular}
    \caption{A simple comparison between AIC and BIC.}
\end{table}

\newpage

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l l @{}}
            \toprule
            & AIC & BIC \\
            \midrule
            \textbf{Penalty strength}   & Mild: $2d$                    & Stronger: $d \cdot \log(n)$           \\ [.3em]
            \textbf{Tends to choose}    & Slightly more complex models  & Simpler models                        \\ [.3em]
            \textbf{Best for}           & Prediction (lower test error) & Identifying true model (if exists)    \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Interpretation of AIC and BIC.}
\end{table}

\noindent
So:
\begin{itemize}
    \item \important{AIC} is better when we're focused on \textbf{prediction}.
    \item \important{BIC} is better when we're trying to \textbf{find the true model} (especially when $n$ is large).
\end{itemize}
\textbf{BIC} is \textbf{more stringent} than AIC because its penalty is stronger, especially as $n$ increases.

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l l @{}}
            \toprule
            Situation   & AIC says...   & BIC says... \\
            \midrule
            Slightly better fit, more parameters    & \textcolor{Green3}{\faIcon{check-circle}} ``Okay, that's fine''   & \textcolor{Red2}{\faIcon{times-circle}} ``Nope, not worth it.'' \\ [.3em]
            Big dataset with noisy improvements     & \textcolor{Green3}{\faIcon{thumbs-up}} ``Still acceptable''       & \textcolor{Red2}{\faIcon{exclamation-triangle}} ``We're overfitting'' \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How to use them?}}
\end{flushleft}
\begin{enumerate}
    \item Compute AIC and/or BIC for each candidate model.
    \item \textbf{Choose the model with the lowest value}.
\end{enumerate}
They are \textbf{not interpretable on their own}, they're only useful \textbf{in comparison} between models.

\highspace
\begin{examplebox}[: What AIC and BIC do]
    Imagine we have 10 models. AIC and BIC will:
    \begin{itemize}
        \item Look at how well each model fits the data (small RSS $=$ good).
        \item Subtract points if the model is \textbf{too complex}.
        \item Pick the model with the \textbf{lowest score}.
    \end{itemize}
    They answer: ``\emph{Which model fits well, \textbf{without overdoing it}?}''.
\end{examplebox}

\newpage

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{How do AIC and BIC relate to overfitting?}}
\end{flushleft}
Overfitting occurs when a model is too complex and fits the noise instead of the signal. AIC and BIC can \textbf{prevent overfitting} because they \textbf{penalize unnecessary complexity}. However, they do so in different ways and with different levels of strictness, as we have seen above.
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{shield-alt}}] \textbf{BIC} is \textbf{more resistant to overfitting} than AIC.
    \item[\textcolor{Red2}{\faIcon{exclamation}}] \textbf{AIC} still helps, better than Adjusted $R^2$, but may \textbf{accept a few extra variables} if they improve training fit slightly.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{If BIC is more robust, why can't we always use it instead of AIC?}}
\end{flushleft}
Because \textbf{avoiding overfitting isn't the only goal}. Sometimes, \textbf{being too strict} can cause the opposite problem: \textbf{underfitting}.

\highspace
BIC is \textbf{very conservative}. It heavily penalizes additional predictors, and often selects \textbf{very simple models}, especially as $n$ (sample size) grows.
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check-circle}}] This is good when our goal is to \textbf{identify the true underlying model}, such as in scientific discovery.
    \item[\textcolor{Red2}{\faIcon{times-circle}}] But BIC can \textbf{miss useful predictors} that would improve \textbf{predictive accuracy}, especially:
    \begin{itemize}
        \item When the true model is complex.
        \item When some variables are only \textbf{mildly helpful}.
        \item When sample size is small (so penalty is not balanced by enough data).
    \end{itemize}
\end{itemize}

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l l p{16.5em} @{}}
            \toprule
            Method & Immune? & Why/Why not \\
            \midrule
            $R^2$               & \textcolor{Red2}{\faIcon{times-circle}} \textcolor{Red2}{\faIcon{times-circle}}                     & Always increases with more variables, pure training fit.                    \\ [.32em]
            Adjusted $R^2$      & \textcolor{Red2}{\faIcon{times-circle}}                                                             & Penalizes complexity, but still based on training data.                     \\ [.32em]
            AIC                 & \textcolor{Green3}{\faIcon{balance-scale}}                                                          & Penalizes complexity, but might allow slight overfit for predictive gain.   \\ [.32em]
            BIC                 & \textcolor{Green3}{\faIcon{check-circle}} (almost)                                                  & Strong penalty, favors simpler models, but still uses training data.        \\ [.32em]
            Cross-Validation    & \textcolor{Green3}{\faIcon{check-circle}} (best in practice)                                        & Tests the model on unseen data, closest thing to a real-world check.        \\ [.32em]
            Real test set       & \textcolor{Green3}{\faIcon{check-circle}} \textcolor{Green3}{\faIcon{check-circle}} (gold standard) & Directly measures generalization, but only if we have one!                  \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{No method is 100\% immune to overfitting, but some are much \textbf{more resistant}. This table shows the hierarchy of \textbf{resilience to overfitting}.}
\end{table}