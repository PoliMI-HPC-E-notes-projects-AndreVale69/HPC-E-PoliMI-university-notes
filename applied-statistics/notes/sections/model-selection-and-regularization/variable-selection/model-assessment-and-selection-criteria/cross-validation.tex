\paragraph{Cross-Validation (CV)}\label{paragraph: Cross-Validation}

We will present a brief overview of the cross-validation method here, as it will be explained in more detail in future sections.

\highspace
\definition{Cross-Validation (CV)} is a technique to estimate a model's test error \textbf{more reliably} than the simple validation set method. Instead of holding out just one subset of the data, CV \textbf{rotates} the validation role across \textbf{multiple subsets}. This reduces variability and makes better use of the data.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{Types of Cross-Validation}}
\end{flushleft}
\begin{itemize}
    \item \important{k-Fold CV}: Split into $k$ parts, train on $k - 1$, validate on the remaining part. Repeat $k$ times.
    \item \important{Leave-One-Out CV (LOOCV)}: Extreme case of k-fold where $k = n$; each fold has one data point.
    \item \important{Repeated k-Fold CV}: Repeat k-fold CV multiple times with different splits, then average results.
    \item \important{Stratified k-Fold}: Like k-fold but ensures each fold has a balanced class distribution (important in classification).
    \item \important{Monte Carlo CV} / \important{Shuffle-split}: Randomly split the data into training and test sets multiple times.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Most common form: k-Fold Cross-Validation}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Split the data} into $k$ roughly equal-sized \textbf{folds} (groups).
    \item For each fold $i = 1, \dots, k$:
    \begin{itemize}
        \item Use all the data \textbf{except} fold $i$ to \textbf{train} the model.
        \item Use fold $i$ as the \textbf{validation set}.
        \item Compute the prediction error on fold $i$.
    \end{itemize}
    \item \textbf{Average} the $k$ errors to get the \textbf{cross-validation estimate} of test error.
\end{enumerate}

\begin{examplebox}[: 5-Fold CV]
    We split the dataset into 5 parts ($F1$ to $F5$). We run 5 iterations:
    \begin{itemize}
        \item Train on $F2 + F3 + F4 + F5$, test on $F1$.
        \item Train on $F1 + F3 + F4 + F5$, test on $F2$.
        \item And so on.
    \end{itemize}
    We average the 5 validation errors, and the result is the Cross-Validation error estimate.
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why not use just one validation set?}}
\end{flushleft}
Because the estimate would depend heavily on which data are held out. CV reduces this \textbf{variance} by \textbf{reusing all the data} for both training and validation (just not at the same time).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Special k-Fold CV cases}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l p{22.8em} @{}}
        \toprule
        Type & Description \\
        \midrule
        \textbf{Leave-One-Out CV}    & $k = n$: each fold is one single observation. \\ [.5em]
        \textbf{Repeated k-Fold CV}  & Repeat k-fold CV multiple times with different splits, then average. \\ [.5em]
        \textbf{Stratified k-Fold}   & Ensure each fold has balanced classes (used in classification). \\
        \bottomrule
    \end{tabular}
\end{table}

\highspace
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Feature & k-Fold CV \\
        \midrule
        \emph{Estimates test error?}        & \textcolor{Green3}{\faIcon{check-circle}} Yes \\ [.5em]
        \emph{Uses all data for training?}  & \textcolor{Green3}{\faIcon{check-circle}} Eventually, each point is used $k-1$ times \\ [.5em]
        \emph{Reduces variance?}            & \textcolor{Green3}{\faIcon{check-circle}} Compared to single validation split \\ [.5em]
        \emph{More computation?}            & \textcolor{Red2}{\faIcon{exclamation-triangle}} Yes, model is fit $k$ times! \\
        \bottomrule
    \end{tabular}
    \caption{Summary of k-Fold Cross-Validation.}
\end{table}