\paragraph{Elastic Net}\label{paragraph: Elastic Net}

\definition{Elastic Net} is a regression (hybrid) method that combines the strengths of \textbf{Ridge} and \textbf{Lasso} by using \textbf{both the L1 and L2 penalties}. It encourages \textbf{sparsity} like Lasso, and stabilizes the model like Ridge. The loss function is:
\begin{equation}
    \hat{\beta}^{\text{EN}} = \arg\min_{\beta} \left\{
        \displaystyle\sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2
        + \lambda_1 \sum_{j=1}^p \left|\beta_j\right|
        + \lambda_2 \sum_{j=1}^p \beta_j^2
    \right\}
\end{equation}
Where:
\begin{itemize}
    \item The first term
    \begin{equation*}
        \displaystyle\sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2
    \end{equation*}
    is the \textbf{Residual Sum of Squares (RSS)}.
    \item The second term
    \begin{equation*}
        \lambda \sum_{j=1}^p |\beta_j|
    \end{equation*}
    is the \textbf{L1 penalty} (\textbf{Lasso}). It is the sum of the absolute values of the coefficients.
    \item The third term
    \begin{equation*}
        \lambda \sum_{j=1}^p \beta_j^2
    \end{equation*}
    is the \textbf{L2 penalty} (\textbf{Ridge penalty}). It is the sum of squared coefficients.
\end{itemize}
Sometimes it's written more compactly as:
\begin{equation}
    \text{RSS} + \lambda \left( \alpha \sum \left|\beta_j\right| + \left(1 - \alpha\right) \sum \beta_j^2 \right)
\end{equation}
Where:
\begin{itemize}
    \item $\lambda \geq 0$: total regularization strength
    \item $\alpha \in \left[0, 1\right]$: balance between Lasso (L1) and Ridge (L2).
    \begin{itemize}
        \item $\alpha = 0$: Elastic Net becomes the \textbf{Ridge} method.
        \item $\alpha = 1$: Elastic Net becomes the \textbf{Lasso} method.
        \item $ 0 < \alpha < 1$: Elastic Net is a mix of both.
    \end{itemize}
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why use Elastic Net?}}
\end{flushleft}
\begin{itemize}
    \item We have \textbf{many correlated predictors}.
    \item Lasso selects \textbf{only one} from a group of correlated variables and ignores the rest.
    \item We want \textbf{sparse output} (like Lasso) but also \textbf{grouped selection} and \textbf{stability} (like Ridge).
\end{itemize}
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} p{10em} | c | c | c @{}}
            \toprule
            Feature & Lasso & Ridge & Elastic Net \\
            \midrule
            Shrinks coefficients            & \textcolor{Green3}{\faIcon{check-circle}}                     & \textcolor{Green3}{\faIcon{check-circle}}                         & \textcolor{Green3}{\faIcon{check-circle}}                     \\ [.3em]
            Sets coefficients to 0          & \textcolor{Green3}{\faIcon{check-circle}}                     & \textcolor{Red2}{\faIcon{times-circle}}                           & \textcolor{Green3}{\faIcon{check-circle}}                     \\ [.3em]
            Handles correlated predictors   & \textcolor{Red2}{\faIcon{times-circle}} (selects one)   & \textcolor{Green3}{\faIcon{check-circle}} (spreads weight)  & \textcolor{Green3}{\faIcon{check-circle}} (can select group)  \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When to use which?}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        If we care about... & Use \\
        \midrule
        Prediction accuracy \& multicollinearity, keep all predictors   & \textbf{Ridge}        \\ [.5em]
        Simplicity, variable selection, interpretability                & \textbf{Lasso}        \\ [.5em]       
        Both: want sparsity \textbf{and} handle correlation well        & \textbf{Elastic Net}  \\ 
        \bottomrule
    \end{tabular}
\end{table}