\paragraph{Lasso}\label{paragraph: Lasso}

\definition{Lasso Regression (Least Absolute Shrinkage and Selection Operator)} is a linear regression method that adds an \textbf{L1 penalty} to the loss function. This penalty encourages sparsity by shrinking some coefficients \textbf{exactly to zero}, effectively performing \textbf{variable selection}.

\highspace
Mathematically, Lasso solves:
\begin{equation}
    \hat{\beta}^{\text{lasso}} = \arg\min_{\beta} \left\{ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j \right)^2 + \lambda \sum_{j=1}^p |\beta_j| \right\}
\end{equation}
Where:
\begin{itemize}
    \item The first term
    \begin{equation*}
        \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j \right)^2
    \end{equation*}
    is the \textbf{Residual Sum of Squares (RSS)}.
    \item The second term
    \begin{equation*}
        \lambda \sum_{j=1}^p |\beta_j|
    \end{equation*}
    is the \textbf{L1 penalty}. It is the sum of the absolute values of the coefficients.
    \item $\lambda \geq 0$ controls the strength of the penalty.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is the L1 Penalty?}}
\end{flushleft}
The \textbf{L1 penalty} is the \textbf{sum of the absolute values of the regression coefficients}:
\begin{equation}
    \text{L1 penalty} = \sum_{j=1}^{p} |\beta_j| = \| \boldsymbol{\beta} \|_1
\end{equation}
This is called the \textbf{L1 norm} of the coefficient vector. It's simply a measure of \textbf{how large the coefficients are}, in terms of their \textbf{total absolute magnitude}. The L1 penalty \textbf{set coefficients to exactly zero} because the \textbf{absolute value function has a sharp corner at 0}, the optimizer finds ti ``cheap'' to \textbf{set coefficients to zero} to reduce penalty. This is what allows \textbf{Lasso to do variable selection}. In other words, it allows to \textbf{select important variables} and discard the rest.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l l @{}}
        \toprule
        Penalty Type        & Formula                                   & Behavior \\
        \midrule
        \textbf{L2 (Ridge)} & $\displaystyle\sum \beta_j^2$             & Shrinks all coefficients smoothly \\ [.3em]
        \textbf{L1 (Lasso)} & $\displaystyle\sum \left|\beta_j\right|$  & Can shrink some to \textbf{exactly 0} \\
        \bottomrule
    \end{tabular}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Role of $\lambda$}}
\end{flushleft}
Just like Ridge:
\begin{itemize}
    \item $\lambda = 0$: Lasso is OLS.
    \item Larger $\lambda$: more shrinkage.
    \item \textbf{But with Lasso}, some coefficients become \textbf{exactly 0} when $\lambda$ is big enough.
\end{itemize}
So Lasso not only regularizes but also \textbf{drops unimportant variables}. So during training, it \textbf{assigns a value of exactly zero to some coefficients}.

\highspace
In other words, Lasso is like \textbf{automatic feature selection}. It keeps only the most important predictors, and discards the rest by setting their coefficients to zero. This is something Ridge Regression \textbf{cannot do}, because it \textbf{shrinks} coefficients, but \textbf{never eliminates them}.

\begin{examplebox}[: Lasso]
    Suppose we have 5 predictors:
    \begin{equation*}
        Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 + \epsilon
    \end{equation*}
    After fitting Lasso, we might get:
    \begin{equation*}
        \hat{\beta}_1 = 3.1,\quad \hat{\beta}_2 = 0,\quad \hat{\beta}_3 = 0,\quad \hat{\beta}_4 = 2.5,\quad \hat{\beta}_5 = 0
    \end{equation*}
    This means:
    \begin{itemize}
        \item $X_1$ and $X_4$ are \textbf{kept}.
        \item $X_2$, $X_3$, and $X_5$ are \textbf{removed} from the model.
    \end{itemize}
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why use Lasso?}}
\end{flushleft}
\begin{itemize}
    \item To reduce \textbf{overfitting} by controlling model complexity.
    \item To get a \textbf{simpler, more interpretable model}.
    \item To automatically \textbf{select important variables}.
    \item Works well when \textbf{some predictors are irrelevant}.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.9\textwidth]{img/model-selection-and-regularization/ridge-vs-lasso.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{A visual comparison between \textbf{Ridge} and \textbf{Lasso} regression.
    \begin{itemize}
        \item \textbf{Ridge} (top): As $\lambda$ increases, all coefficients \textbf{shrink smoothly toward zero}, but \textbf{none become exactly zero}.
        \item \textbf{Lasso} (bottom): As $\lambda$ increases, some coefficients \textbf{shrink to zero} and stay there, the model \textbf{drops predictors} entirely.
    \end{itemize}
    This shows how Ridge keeps all variables but controls their influence, while Lasso both regularizes and performs \textbf{feature selection}.}
\end{figure}