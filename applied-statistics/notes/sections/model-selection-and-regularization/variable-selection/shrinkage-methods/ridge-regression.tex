\subsubsection{Shrinkage Methods}

In \definition{Shrinkage Methods}, we \textbf{penalize the size} of the regression coefficients. Instead of minimizing only the \textbf{residual sum of squares (RSS)}, we minimize:
\begin{equation*}
    \text{RSS} + \lambda \cdot \text{Penalty}
\end{equation*}
\begin{itemize}
    \item \textbf{RSS} ensures a good fit to the data.
    \item The \textbf{penalty} prevents overfitting by shrinking the coefficients \textbf{toward 0}.
    \item The \textbf{tuning parameter} $\lambda \geq 0$ controls the amount of shrinkage.
\end{itemize}
So:
\begin{itemize}
    \item If $\lambda = 0$: no shrinkage, then same as ordinary least squares (OLS).
    \item If $\lambda \to \infty$: coefficients shrink toward 0.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{cube} \textbf{Geometric View}}
\end{flushleft}
In OLS, we find the point that minimizes RSS (ellipse contour). In shrinkage, we \textbf{constrain the solution} to be within a certain shape (circle, diamond, etc.). The Shrinkage is the \textbf{constrained optimization}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why Shrink?}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l p{22em} @{}}
        \toprule
        \textbf{Benefit} & \textbf{How Shrinkage Helps} \\
        \midrule
        Reduce variance             & Shrinking coefficients prevents wild oscillation.             \\ [.3em]
        Handle multicollinearity    & Penalty makes solution stable when predictors are correlated. \\ [.3em]
        Interpretability            & Lasso even sets some coefficients exactly to 0.               \\ [.3em]
        Works when $p > n$          & OLS fails, shrinkage still works.                             \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
The \textbf{General Optimization Form} for all shrinkage methods is:
\begin{equation}
    \min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \cdot \text{Penalty}(\beta) \right\}
\end{equation}
\begin{itemize}
    \item The first term is \textbf{RSS}.
    \item The second term is a \textbf{penalty} on the size of $\beta$.
    \item The \textbf{form of the penalty} determines the method.
\end{itemize}

\newpage

\paragraph{Ridge Regression}\label{paragraph: Ridge Regression}

\definition{Ridge Regression} is a linear regression technique that adds an \textbf{L2 penalty} to the loss function (page \pageref{def: Loss Function}) in order to \textbf{shrink} the regression coefficients and reduce model complexity. It is especially useful when the predictors are \textbf{highly correlated} or when there are \textbf{more predictors than observations} ($p > n$).

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{What problems does ordinary linear regression have?}}
\end{flushleft}
In \textbf{linear regression}, we find the best coefficients $\beta_j$ to predict $Y$ from $X$:
\begin{equation*}
    \hat{\beta}^{\text{OLS}} = \arg\min_{\beta} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \arg\min_{\beta} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2
\end{equation*}
We're just minimizing the total squared error between what the model predicts and the actual data. But when predictors are \textbf{highly correlated}, or there are \textbf{many predictors}, or $p > n$ (more variables than observations), then OLS estimates become:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textbf{Unstable}: coefficients bounce around a lot.
    \item \textbf{Large}: some $\beta_j$ become huge to compensate.
    \item \textbf{Overfit}: fits training data well, but fails on new data.
\end{itemize}
\textcolor{Green3}{\faIcon{check-circle} \textbf{Ridge Regression: the fix.}} The key idea is: let's stop the coefficients from growing too large. So we modify the loss function:
\begin{equation*}
    \text{Ridge Loss} = \text{RSS} + \lambda \sum_{j=1}^p \beta_j^2
\end{equation*}
We still try to make predictions accurate (low RSS), \textbf{but now we also penalize large} $\beta_j$ values.

\highspace
Mathematically, Ridge Regression is a modified version of linear regression, it estimates the coefficients $\hat{\beta}$ by solving:
\begin{equation}
    \hat{\beta}^{\text{ridge}} = \arg\min_{\beta} \left\{ \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\}
\end{equation}
\begin{itemize}
    \item The first term:
    \begin{equation*}
        \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2
    \end{equation*}
    is the \textbf{residual sum of squares (RSS)}.
    \item The second term:
    \begin{equation*}
        \lambda \sum_{j=1}^p \beta_j^2
    \end{equation*}
    is the \textbf{ridge penalty}. It is the sum of squared coefficients.
    \item $\lambda \geq 0$ is the \textbf{tuning parameter} that controls the strength of shrinkage.
\end{itemize}
\hl{When $\lambda = 0$, Ridge Regression is equivalent to ordinary least squares (OLS).} As $\lambda$ increases, the coefficients are increasingly \textbf{shrunk toward zero}, but \hl{never exactly zero}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What does L2 penalty mean?}}
\end{flushleft}
In Ridge Regression, the \textbf{L2 penalty} refers to the \textbf{sum of the squares of the regression coefficients}:
\begin{equation}
    \text{L2 penalty} = \sum_{j=1}^{p} \beta_j^2 = \| \boldsymbol{\beta} \|_2^2
\end{equation}
It's called \textbf{L2} because it's based on the \textbf{L2 norm} (also known as the Euclidean norm). The term $\| \boldsymbol{\beta} \|_2$ is the \textbf{distance} from the origin in coefficient space:
\begin{equation*}
    \| \boldsymbol{\beta} \|_2 = \sqrt{ \beta_1^2 + \beta_2^2 + \dots + \beta_p^2 }
\end{equation*}
So the \textbf{L2 penalty} is just the \textbf{square} of that distance:
\begin{equation*}
    \| \boldsymbol{\beta} \|_2^2 = \beta_1^2 + \beta_2^2 + \dots + \beta_p^2
\end{equation*}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why square the coefficients?}}
\end{flushleft}
Because squaring, the penalty:
\begin{itemize}
    \item \textbf{Grows fast} and it \textbf{heavily punishes large values} (for example, $10^2 = 100$).
    \item Is \textbf{smooth and differentiable}, making it easy to optimize.
    \item Makes the penalty \textbf{zero} when all $\beta_j = 0$.
\end{itemize}
So the model is \textbf{encouraged} to fit the data well and \textbf{keep coefficients small} (unless they're really helpful).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What does $\lambda$ do?}}
\end{flushleft}
The $\lambda$ term is called \textbf{weight} (or \textbf{tuning parameter}) and controls how much importance we give to the \textbf{penalty} compared to the \textbf{prediction error}. In other words, it is a balancing weight between fit (RSS) and simplicity (ridge penalty).

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Value of $\lambda$      & What happens \\
        \midrule
        $\lambda = 0$           & No penalty, same as OLS (possibly overfitting)                \\ [.3em]
        Small $\lambda$         & Light shrinkage, keeps fit close to OLS                       \\ [.3em]
        Medium $\lambda$        & Shrinks coefficients significantly, improves generalization   \\ [.3em]
        Large $\lambda$         & Strong penalty, forces all $\beta_j \to 0$, risk underfitting \\ [.3em]
        $\lambda \to \infty$    & Model ignores all predictors, then predicts the mean          \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{So, how do we choose the best $\lambda$?}}
\end{flushleft}
We \textbf{don't guess it manually}. Instead, we use \textbf{cross-validation}:
\begin{enumerate}
    \item Split our data into training and validation sets;
    \item Try many values of $\lambda$;
    \item For each, compute prediction error on validation data;
    \item Choose the $\lambda$ that gives the \textbf{lowest validation error}.
\end{enumerate}
We use cross-validation because we want to find the value of $\lambda$ that gives the best prediction performance on new, unseen data (and cross-validation simulates that).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When should we use ridge regression, and when should we not?}}
\end{flushleft}
\begin{enumerate}
    \item We have \important{many predictors}, especially when $p \sim n$ or $p > n$. Because OLS becomes unstable and Ridge can controls the complexity.
    \item Our predictors are \important{highly correlated (multicollinearity)}. Because OLS coefficients can become huge and erratic. Ridge \textbf{shrinks correlated coefficients smoothly}, improving stability.
    \item We want \important{better prediction accuracy}, not interpretability. Ridge keeps \textbf{all variables}, it doesn't eliminate any. Great when our goal is \textbf{prediction}, not explanation.
    \item We \important{don't need variable selection}. Ridge shrinks all coefficients, but \textbf{never sets them exactly to 0}. So it's not ideal when we want to identify the most important variables.
\end{enumerate}
\textbf{Don't use Ridge if}:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item We want to \textbf{remove irrelevant features}, then use Lasso.
    \item We want a model that's \textbf{easy to interpret}, then Ridge keeps everything in.
\end{itemize}