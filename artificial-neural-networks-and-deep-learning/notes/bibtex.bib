@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@unpublished{course-slides-polimi,
	title  = {Artificial Neural Networks and Deep Learning},
	author = {Matteucci Matteo},
	note   = {Slides from the HPC-E master's degree course on Politecnico di Milano},
	year   = {2025-2026}
}
@article{HORNIK1989359,
    title = {Multilayer feedforward networks are universal approximators},
    journal = {Neural Networks},
    volume = {2},
    number = {5},
    pages = {359-366},
    year = {1989},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
    url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
    author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
    keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
    abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}