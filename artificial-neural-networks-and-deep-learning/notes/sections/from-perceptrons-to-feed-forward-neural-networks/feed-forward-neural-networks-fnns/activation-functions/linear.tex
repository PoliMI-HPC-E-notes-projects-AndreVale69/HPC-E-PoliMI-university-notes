\subsubsection{Activation Functions}

Every neuron computes a \textbf{weighted sum} of its inputs and bias:
\begin{equation*}
    a = w^{T} x + b
\end{equation*}
And then applies an \textbf{activation function} $f(a)$ to produce its final output:
\begin{equation*}
    y = f(a)
\end{equation*}
The \definition{Activation Function} defines \textbf{how the neuron ``fires''}, i.e., how it transforms the raw input signal into an output that will be passed to the next layer. Exist many different activation functions, each with its own characteristics and use cases. The choice of activation function can significantly impact the performance and capabilities of a neural network. In the following, we will explore some of the most commonly used activation functions in neural networks.

\highspace
In the next sections, we will mention the derivative result and the range of each activation function:
\begin{itemize}
    \item During training, neural networks learn by \textbf{minimizing a loss function}, and this requires \textbf{backpropagation}, which is based entirely on \important{derivatives}. We will explain backpropagation later, but for now, here is a brief overview of how it works: (1) each neuron has parameters $w_i$ (weights) and $b$ (bias); (2) to adjust them, we compute how the \textbf{loss} changes if we slightly change each parameter; (3) mathematically, that's done through \textbf{gradients}, the derivatives of the loss with respect to the weights. When we apply the \textbf{chain rule} to compute these gradients, we get something like:
    \begin{equation*}
        \frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial a} \cdot \frac{\partial a}{\partial w_i}
    \end{equation*}
    Where $\frac{\partial y}{\partial a}$ is the derivative of the activation function $f(a)$. Therefore, having an activation function: \textbf{too small} ($=0$), means gradients vanish and the learning stops; \textbf{too large} gradients can cause instability. Hence, the derivative of the activation function is crucial for effective learning.

    \item The \important{range} of an activation determines what kind of outputs each neuron can produce, and this affects: (1) \textbf{how the next layer receives data}, and \textbf{how easy it is to train} the network. For example, if an activation function outputs values in a limited range (like between 0 and 1), it can help keep the network's outputs stable and prevent extreme values that could lead to numerical issues during training.
\end{itemize}

\newpage

\paragraph{Linear}

The \definition{Linear Activation Function} is the simplest activation function, defined as:
\begin{equation}
    f(a) = a
\end{equation}
That means the neuron's output equals its input; there's no distortion or thresholding. So the neuron is just a \textbf{weighted sum} followed by nothing.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{lightbulb} \textbf{Intuitive interpretation}}
\end{flushleft}
If all neurons in a network use $f(a) = a$, then every layer just performs a \textbf{linear transformation} of the input. Stacking \textbf{multiple linear layers doesn't add any expressive power}; the entire \textbf{network can be reduced to a single linear transformation}. In general, for a network with $n$ layers, each represented by a weight matrix $W_i$, the overall transformation is:
\begin{equation*}
    f\left(W_n \left(W_{n-1} \left( \ldots W_2 \left(W_1 x\right) \ldots \right)\right)\right) = \left(W_n W_{n-1} \ldots W_2 W_1\right) x
\end{equation*}
However, if the \hl{network only uses linear activation functions}, then it simplifies to:
\begin{equation*}
    f\left(W_2 \left(W_1 x\right)\right) = \left(W_2 W_1\right) x
\end{equation*}
Therefore, linear activation functions are rarely used in practice for hidden layers, as they cannot capture complex patterns in data. In other words, a \hl{purely linear network} cannot learn anything more complex than a straight boundary; it is basically a big matrix multiplication because all the layers collapse into one.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Property & Description \\
        \midrule
        Formula     & $f(a) = a$ \\[.3em]
        Derivative  & $f'(a) = 1$ \\[.3em]
        Range       & $\left(-\infty, +\infty\right)$ \\[.3em]
        Nonlinear?  & \textcolor{Red2}{\faIcon{times}}\\[.3em]
        Typical use & Regression output layers (not hidden neurons) \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When to use it}}
\end{flushleft}
Even though a \textbf{linear activation} is useless inside hidden layers (because it doesn't add nonlinearity), it's still \textbf{important at the output layer} of certain models:
\begin{itemize}
    \item \textbf{Regression problems}: when we want a real-valued output (like predicting house prices), a linear activation allows the network to produce any value in the range $(-\infty, +\infty)$. However, the linear activation is applied only at the output layer, while hidden layers use nonlinear activations to capture complex patterns.
    \item \textbf{Autoencoders or embedding layers}: sometimes the linear activation helps maintain continuous representations of data.
\end{itemize}
In summary, the \textbf{linear activation} keeps the output proportional to the input. It's mathematically simple and differentiable, but \textbf{does not allow the network to model nonlinear relationships}. Hence, not used in hidden layers.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/fnns/linear-activation-function.pdf}
    \caption{Linear activation $f(a) = a$ and its derivative. The function is the identity (a straight line, showing that the neuron outputs exactly what it receives), and its constant derivative $f'(a) = 1$ allows perfect gradient flow. However, being linear, it adds no expressive power to the network.}
\end{figure}