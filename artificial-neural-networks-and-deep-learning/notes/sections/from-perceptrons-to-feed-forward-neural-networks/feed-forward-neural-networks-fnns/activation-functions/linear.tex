\subsubsection{Activation Functions}\label{sec:activation-functions}

Every neuron computes a \textbf{weighted sum} of its inputs and bias, called \definition{Net Input} or \definition{Activation Potential}:
\begin{equation}\label{eq:net-input}
    a = w^{T} x + b
\end{equation}
Up to this point, everything is \textbf{linear}. If we stopped here and used $y = a$ as the output, the neuron would just perform a \emph{linear transformation}. So, we need to add some \textbf{non-linearity} to the neuron's output (a sort of \emph{magic ingredient}) to allow the network to learn complex patterns. Without it, the entire neural network would collapse into a single linear operation: a big matrix multiplication. Remember that we come from the perceptron, which used a step function as activation and complex patterns, like XOR, could not be learned without non-linearity.

\highspace
So we introduce the concept of \definition{Activation Function}. A neuron takes the net input $a$ computed above, and then applies an \textbf{activation function} $f(a)$ to produce its final output:
\begin{equation*}
    y = f(a)
\end{equation*}
The \definition{Activation Function} defines \textbf{how the neuron ``fires''}, i.e., how it transforms the raw input signal into an output that will be passed to the next layer.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why do we really need activation functions?}}
\end{flushleft}
Activation functions are what give neural networks their power and flexibility. Three main reasons why they are essential:
\begin{enumerate}
    \item \textbf{To \emph{break linearity}}. Without it, the entire neural network would collapse into a single linear operation. Mathematically:
    \begin{equation*}
        \text{If } g(a) = a, \text{ then } g(W^{(2)} g(W^{(1)}x)) = W^{(2)}W^{(1)}x
    \end{equation*}
    This is \emph{still linear}, no matter how many layers we stack. We'd just have one big matrix multiplication. So, without $g\left(\cdot\right)$, the network couldn't model \emph{curves}, \emph{XOR}, \emph{images}, \emph{language}, or anything nonlinear.

    \item \textbf{To allow \emph{complex decision boundaries}}. Think of a perceptron. With only linear operations, it can only separate data using a straight line (or plane, or hyperplane). By inserting a nonlinear $g(a)$, hidden neurons can \textbf{bend} the space (combine multiple lines to form curved boundaries). For example, a simple linear neuron can only separate classes with a single line, while nonlinear activations (like sigmoid or ReLU) allow the network to create complex, curved, multi-region separations.

    \item \textbf{To \emph{control output range}}. Activation functions can also \textbf{squash} values:
    \newpage
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l c c l @{}}
            \toprule
            Function & Formula & Output Range & Typical Use \\
            \midrule
            Sigmoid & $\dfrac{1}{1 + e^{-a}}$ & $(0, 1)$ & Binary classification output layers. \\[1.2em]
            Tanh    & $\dfrac{e^{a} - e^{-a}}{e^{a} + e^{-a}}$ & $(-1, 1)$ & Hidden layers in some networks. \\[1.2em]
            Linear  & $a$ & $(-\infty, +\infty)$ & Regression output layers. \\
            \bottomrule
        \end{tabular}
    \end{table}

    That's why we use sigmoid/tanh in hidden layers or output layers for classification (to get probabilities between 0 and 1), while linear activations are used in regression tasks (to allow any real-valued output).
\end{enumerate}

\highspace
Exist many different activation functions, each with its own characteristics and use cases. The choice of activation function can significantly impact the performance and capabilities of a neural network. In the following, we will explore some of the most commonly used activation functions in neural networks.

\highspace
In the next sections, we will mention the derivative result and the range of each activation function:
\begin{itemize}
    \item During training, neural networks learn by \textbf{minimizing a loss function}, and this requires \textbf{backpropagation}, which is based entirely on \important{derivatives}. We will explain backpropagation later, but for now, here is a brief overview of how it works: (1) each neuron has parameters $w_i$ (weights) and $b$ (bias); (2) to adjust them, we compute how the \textbf{loss} changes if we slightly change each parameter; (3) mathematically, that's done through \textbf{gradients}, the derivatives of the loss with respect to the weights. When we apply the \textbf{chain rule} to compute these gradients, we get something like:
    \begin{equation*}
        \frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial a} \cdot \frac{\partial a}{\partial w_i}
    \end{equation*}
    Where $\frac{\partial y}{\partial a}$ is the derivative of the activation function $f(a)$. Therefore, having an activation function: \textbf{too small} ($=0$), means gradients vanish and the learning stops; \textbf{too large} gradients can cause instability. Hence, the derivative of the activation function is crucial for effective learning.

    \item The \important{range} of an activation determines what kind of outputs each neuron can produce, and this affects: (1) \textbf{how the next layer receives data}, and \textbf{how easy it is to train} the network. For example, if an activation function outputs values in a limited range (like between 0 and 1), it can help keep the network's outputs stable and prevent extreme values that could lead to numerical issues during training.
\end{itemize}

\newpage

\paragraph{Linear}\label{par:linear-activation-function}

The \definition{Linear Activation Function} is the simplest activation function, defined as:
\begin{equation}
    f(a) = a
\end{equation}
That means the neuron's output equals its input; there's no distortion or thresholding. So the neuron is just a \textbf{weighted sum} followed by nothing.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{lightbulb} \textbf{Intuitive interpretation}}
\end{flushleft}
If all neurons in a network use $f(a) = a$, then every layer just performs a \textbf{linear transformation} of the input. Stacking \textbf{multiple linear layers doesn't add any expressive power}; the entire \textbf{network can be reduced to a single linear transformation}. In general, for a network with $n$ layers, each represented by a weight matrix $W_i$, the overall transformation is:
\begin{equation*}
    f\left(W_n \left(W_{n-1} \left( \ldots W_2 \left(W_1 x\right) \ldots \right)\right)\right) = \left(W_n W_{n-1} \ldots W_2 W_1\right) x
\end{equation*}
However, if the \hl{network only uses linear activation functions}, then it simplifies to:
\begin{equation*}
    f\left(W_2 \left(W_1 x\right)\right) = \left(W_2 W_1\right) x
\end{equation*}
Therefore, linear activation functions are rarely used in practice for hidden layers, as they cannot capture complex patterns in data. In other words, a \hl{purely linear network} cannot learn anything more complex than a straight boundary; it is basically a big matrix multiplication because all the layers collapse into one.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Property & Description \\
        \midrule
        Formula     & $f(a) = a$ \\[.3em]
        Derivative  & $f'(a) = 1$ \\[.3em]
        Range       & $\left(-\infty, +\infty\right)$ \\[.3em]
        Nonlinear?  & \textcolor{Red2}{\faIcon{times}}\\[.3em]
        Typical use & Regression output layers (not hidden neurons) \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When to use it}}
\end{flushleft}
Even though a \textbf{linear activation} is useless inside hidden layers (because it doesn't add nonlinearity), it's still \textbf{important at the output layer} of certain models:
\begin{itemize}
    \item \textbf{Regression problems}: when we want a real-valued output (like predicting house prices), a linear activation allows the network to produce any value in the range $(-\infty, +\infty)$. However, the linear activation is applied only at the output layer, while hidden layers use nonlinear activations to capture complex patterns.
    \item \textbf{Autoencoders or embedding layers}: sometimes the linear activation helps maintain continuous representations of data.
\end{itemize}
In summary, the \textbf{linear activation} keeps the output proportional to the input. It's mathematically simple and differentiable, but \textbf{does not allow the network to model nonlinear relationships}. Hence, not used in hidden layers.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/fnns/linear-activation-function.pdf}
    \caption{Linear activation $f(a) = a$ and its derivative. The function is the identity (a straight line, showing that the neuron outputs exactly what it receives), and its constant derivative $f'(a) = 1$ allows perfect gradient flow. However, being linear, it adds no expressive power to the network.}
\end{figure}