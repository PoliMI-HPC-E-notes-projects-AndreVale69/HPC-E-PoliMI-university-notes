\paragraph{Sigmoid}

The \definition{Sigmoid Activation Function} (or \definition{Logistic Activation Function}) is defined as:
\begin{equation}
    f(a) = \dfrac{1}{1 + e^{-a}}
\end{equation}
It ``\textbf{squashes}'' any real-valued \textbf{input} $a$ \textbf{into a range} between \textbf{0 and 1}.

\highspace
The Sigmoid converts its input into something that looks like a \textbf{smooth threshold}:
\begin{itemize}
    \item[\textcolor{Green3}{$\boldsymbol{\upuparrows}$}] Large positive inputs $a$ produce outputs close to 1;
    \item[\textcolor{Red2}{$\boldsymbol{\downdownarrows}$}] Large negative inputs $a$ produce outputs close to 0;
    \item[\textcolor{DarkOrange3}{$\boldsymbol{\approx}$}] Inputs $a$ close to 0 produce outputs close to 0.5
\end{itemize}
It's often described as giving a ``\textbf{firing probability}'' to a neuron, mimicking how biological neurons activate gradually rather than with a hard step.

\highspace
Graphically, the Sigmoid function is a smooth \textbf{S-shaped} curve (sigmoidal). It's \textbf{continuous} and \textbf{differentiable everywhere}. It has a gentle slope around $0$ and saturates near the extremes (0 or 1).

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/fnns/sigmoid.pdf}
    \caption{Sigmoid Activation Function and its derivative. The sigmoid introduces smooth nonlinearity and maps inputs into $(0,1)$, but its derivative vanishes for large inputs, causing slow learning in deep networks.}
\end{figure}

\noindent
The \textbf{derivative} tells us how sensitive the neuron's output is to changes in its input. For the Sigmoid function, the derivative is given by:
\begin{equation}
    f'(a) = f(a) - 2f(a) = f(a) \cdot \left[1 - f(a)\right]
\end{equation}
This means:
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{\speedIcon}}] When $f(a) \approx 0.5$, the derivative is maximized at $0.25$, allowing for significant weight updates during training (\textbf{neuron is responsive}).
    \item[\textcolor{Red2}{\faIcon{exclamation-triangle}}] When $f(a) \approx 0$ or $f(a) \approx 1$, the derivative approaches $0$, leading to very small weight updates (\textbf{neuron is saturated} and gradients vanish).
    
    This \important{vanish gradient problem} makes deep networks with Sigmoid activations \textbf{hard to train}, as gradients become very small in earlier layers during backpropagation. In other words, the Sigmoid function can cause \textbf{slow learning} in deep networks due to its saturating behavior at extreme input values.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/fnns/sigmoid-saturation.pdf}
    \caption{Sigmoid activation and saturation regions.}
    \label{fig:sigmoid-saturation}
\end{figure}

\noindent
In figure \ref{fig:sigmoid-saturation} we can see the \textbf{vanish gradient problem} in action: when neurons saturate, their gradients vanish, making it hard for the network to learn from data during training.
\begin{itemize}
    \item The blue curve shows the \textbf{sigmoid activation} $f(a)$. That smooth \textbf{S-shaped curve} (in blue) represents:
    \begin{equation*}
        f(a) = \dfrac{1}{1 + e^{-a}}
    \end{equation*}
    In the center (around $a=0$), the output is about $0.5$, and the curve is \textbf{steepest}; for large positive $a>5$, the curve \textbf{flattens near 1}; for large negative $a<-5$, it \textbf{flattens near 0}. Those flat tails are the \textbf{saturation regions} (highlighted in orange). These regions mean that when the neuron receives very strong positive or negative inputs, its output doesn't change much anymore; it has reached its ``max'' or ``min'' activation.
    \item The orange curve are the parts of the curve where the output is \textbf{almost constant}:
    \begin{itemize}
        \item On the left (for large negative $a$), the output is very close to $0$ (saturated low);
        \item On the right (for large positive $a$), the output is very close to $1$ (saturated high).
    \end{itemize}
    In those regions:
    \begin{equation*}
        \dfrac{\partial f}{\partial a} = f'(a) \approx 0
    \end{equation*}
    So the neuron has \textbf{stopped responding}, even big changes in $a$ cause almost no change in the output $f(a)$.
    \item The red curve shows the \textbf{derivative} $f'(a)$:
    \begin{equation*}
        f'(a) = f(a) \cdot (1 - f(a))
    \end{equation*}
    The derivative is only significant in a \textbf{small central region} (roughly between $-3$ and $3$). Outside this range, the derivative \textbf{drops to near zero}, indicating that the neuron is \textbf{saturated} and \textbf{not learning effectively}.
\end{itemize}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l p{20em} @{}}
        \toprule
        \textbf{Property} & \textbf{Value / Meaning} \\
        \midrule
        Formula & $f(a) = \dfrac{1}{1 + e^{-a}}$ \\[1em]
        Range & $(0, 1)$ \\[.5em]
        Derivative & $f'(a) = f(a) \cdot (1 - f(a))$ \\[.5em]
        Output interpretation & Probability or ``firing strength''. \\[.5em]
        Pros & Smooth, differentiable, bounded output, probabilistic interpretation. \\[.5em]
        Cons & Vanishing gradients for large $a$, outputs not zero-centered, computationally expensive. \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When to use it}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Output layer of binary classification} networks, where outputs represent probabilities:
    \begin{equation*}
        \mathbb{P}\left( y = 1 \mid \mathbf{x} \right) = f(w^{T} \mathbf{x} + b)
    \end{equation*}
    \item Historically used in hidden layers (in early networks), but now often replaced by ReLU or its variants due to vanishing gradient issues.
\end{itemize}

\highspace
In summary, the Sigmoid activation function is essentially a \textbf{soft version} of the perceptron's step function:
\begin{equation*}
    \text{step: } f(a) = \begin{cases}
        1 & \text{if } a \geq 0 \\
        0 & \text{if } a < 0
    \end{cases}
    \quad \longrightarrow \quad
    \text{sigmoid: } f(a) = \dfrac{1}{1 + e^{-a}}
\end{equation*}
So the sigmoid allowed neural networks to become \textbf{differentiable}, which made \textbf{gradient-based learning (backpropagation)} possible. However, its tendency to \textbf{saturate} and cause \textbf{vanishing gradients} has led to the adoption of alternative activation functions (like ReLU) in modern deep learning architectures.
