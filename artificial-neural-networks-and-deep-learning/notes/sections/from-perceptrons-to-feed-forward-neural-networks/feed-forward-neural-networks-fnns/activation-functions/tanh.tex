\paragraph{Hyperbolic Tangent (tanh)}

The \definition{Hyperbolic Tangent (tanh) Activation Function}, commonly known as \textbf{tanh}, is defined as:
\begin{equation}
    f(a) = \tanh(a) = \dfrac{e^{a} - e^{-a}}{e^{a} + e^{-a}}
\end{equation}
And its derivative is:
\begin{equation}
    f'(a) = 1 - \tanh^2(a) = 1 - f^2(a)
\end{equation}
The tanh function maps input values to an output range between -1 and 1. It is a scaled version of the sigmoid function, centered around zero.

\highspace
The tanh activation function looks \textbf{very similar to the sigmoid}, but it is \textbf{symmetric around zero}: outputs range from $-1$ to $1$, which helps in centering the data and can lead to faster convergence during training. This makes it \textbf{zero-centered}, which is a big advantage.
\begin{itemize}
    \item When the input is zero ($a=0$), the output is also zero ($f(0)=0$).
    \item For large positive inputs, the output approaches 1 ($f(a) \to 1$ as $a \to +\infty$).
    \item For large negative inputs, the output approaches -1 ($f(a) \to -1$ as $a \to -\infty$).
\end{itemize}
That means hidden neurons can have both positive and negative activations, which helps later layers learn faster because the data stays \textbf{balanced} around zero.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/fnns/tanh-activation-functions.pdf}
    \includegraphics[width=\textwidth]{img/fnns/tanh-derivatives.pdf}
    \caption{Hyperbolic Tangent (tanh) Activation Function and its Derivative.}
    \label{fig:tanh-activation-function}
\end{figure}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why it's better than sigmoid}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Range}: The tanh function outputs values between -1 and 1, while the sigmoid function outputs values between 0 and 1. This means that tanh is zero-centered, which can help with convergence during training.
    \item \textbf{Gradient around zero}: The derivative of the tanh function is $\approx 1$ around zero, while the derivative of the sigmoid function is $\approx 0.25$ around zero. This means that the tanh function has a steeper gradient\footnote{%
        A ``steeper gradient'' means that small changes in the input lead to larger changes in the output, which can help the model learn more effectively.
    } around zero, which can help with learning.
    \item \textbf{Saturates?} Both functions can saturate for large positive or negative inputs, leading to the vanishing gradient problem. However, because tanh is zero-centered, it can help mitigate this issue to some extent.
    \item \textbf{Training speed}: In practice, models using the tanh activation function often converge faster than those using the sigmoid function, especially in deep networks.
\end{itemize}
The \textbf{zero-centered} output means activations can cancel each other out more easily, so the network doesn't get a constant ``positive bias'' in its gradient (a problem with sigmoid). This often leads to \textbf{faster convergence} during training.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When to use tanh}}
\end{flushleft}
The tanh activation function is often preferred over the sigmoid function in hidden layers of neural networks, especially when the data is centered around zero. It is particularly useful in scenarios where:
\begin{itemize}
    \item The \textbf{input} data is \textbf{normalized} to have a \textbf{mean of zero}.
    \item The \textbf{model requires faster convergence} during training.
    \item The \textbf{network is deep}, and the benefits of zero-centered activations help mitigate issues like vanishing gradients.
\end{itemize}
However, it's important to note that while tanh can be advantageous in many situations, it \textbf{still suffers from the vanishing gradient problem for very large or very small input values}. Therefore, in very deep networks, other activation functions like ReLU (Rectified Linear Unit) are often preferred.