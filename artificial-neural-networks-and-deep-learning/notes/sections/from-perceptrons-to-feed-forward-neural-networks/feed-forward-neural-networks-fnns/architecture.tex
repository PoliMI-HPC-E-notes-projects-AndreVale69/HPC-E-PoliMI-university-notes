\subsection{Feed-Forward Neural Networks (FNNs)}

\subsubsection{Architecture}

After discovering that a single perceptron can only draw \textbf{one straight boundary}, researchers realized that we need \textbf{multiple layers} of neurons to build \textbf{non-linear decision surfaces}. That's how \textbf{Feed-Forward Neural Networks (FNNs)} were born.

\highspace
\begin{definitionbox}[: Feed-Forward Neural Networks (FNNs)]
    A \definition{Feed-Forward Neural Network (FNN)} is an artificial neural network where information \textbf{flows in one direction only}, from the input layer, through any hidden layers, to the output layer. There are no cycles or loops in the network.
    \begin{equation*}
        x \, \rightarrow \, \text{Layer 1} \, \rightarrow \, \text{Layer 2} \, \rightarrow \, \dots \, \rightarrow \, \text{Output Layer}
    \end{equation*}

    \highspace
    Each layer receives signals from the previous layer, processes them using weighted connections and activation functions, and passes the output to the next layer.
\end{definitionbox}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Structure of a Feed-Forward Network}}
\end{flushleft}
An FNN is composed of:
\begin{enumerate}
    \item \important{Input Layer}: one neuron per input feature (e.g., pixel value, sensor reading). Does not perform computation, it simply distributes inputs to the next layer.
    \item \important{Hidden Layer(s)}: Contain neurons that each compute:
    \begin{equation*}
        a_{j} = f\left(w_{j}^{T} x + b_{j}\right)
    \end{equation*}
    where $w_{j}$ are the weights, $b_{j}$ is the bias, $x$ is the input vector from the previous layer, and $f$ is the nonlinear activation function (sigmoid, tanh, ReLu, etc.). The index $j$ identifies the specific neuron in the hidden layer. Each neuron learns different \textbf{intermediate features} of the data.
    \item \important{Output Layer}: Produces the network's final result. Activation depends on the task, we mean for classification we often use \textbf{softmax} or \textbf{sigmoid}, while for regression we use a \textbf{linear} activation.
\end{enumerate}
The connections between neurons are \textbf{weighted}:
\begin{itemize}
    \item Each neuron in layer $l$ is connected to \textbf{all} neurons in the previous layer $l-1$. This is called a \textbf{fully connected} or \textbf{dense} layer.
    \item Every connection has its own \textbf{weight}, which is learned during training. Every neuron also has a own \textbf{bias} term.
    \item During training, all these weights and biases are adjusted to minimize the difference between the predicted output and the actual target values.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.92\textwidth]{img/fnns/3-layer-fnn.pdf}
    \caption{Architecture of a simple feed-forward neural network. Each layer is fully connected to the next one. Signals flow in one direction (input $\to$ hidden $\to$ output) without feedback connections.}
\end{figure}

\noindent
FNNs can be represented as graphs based on this architecture:
\begin{itemize}
    \item \textbf{Nodes} represent neurons.
    \item \textbf{Edges} represent weighted connections between neurons.
\end{itemize}
This graph representation helps visualize the network's structure and understand how information propagates through it.

\highspace
Mathematically, for a layer $l$:
\begin{equation*}
    \begin{cases}
        a^{(l)} = f\left(W^{(l)} a^{(l-1)} + b^{(l)}\right) & \text{for hidden layers} \\
        x^{(l)} = f\left(a^{(l)}\right)                     & \text{for output layer}
    \end{cases}
\end{equation*}
where:
\begin{itemize}
    \item $a^{(l)}$ is the activation vector of layer $l$.
    \item $W^{(l)}$ is the weight matrix connecting layer $l-1$ to layer $l$.
    \item $b^{(l)}$ is the bias vector for layer $l$.
    \item $f$ is the activation function.
    \item $x^{(l)}$ is the final output of the network.
\end{itemize}
This formalism allows us to \textbf{compute the output of the network given an input vector by sequentially applying these transformations layer by layer}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How FNNs learn hierarchical features}}
\end{flushleft}
Adding layers lets the network learn \textbf{hierarchical representations}:
\begin{itemize}
    \item The first layers capture \textbf{simple patterns} (e.g., edges in images).
    \item Deeper layers combine these simple patterns into \textbf{more complex features} (e.g., shapes, objects).
\end{itemize}
This ability to build abstractions through depth is the essence of \textbf{Deep Learning}.