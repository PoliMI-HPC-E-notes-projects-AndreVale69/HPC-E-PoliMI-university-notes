\subsubsection{Neural Networks as Universal Approximators}

In 1989, Kurt Hornik, Maxwell Stinchcombe, and Halbert White published a seminal paper titled ``Multilayer Feedforward Networks are Universal Approximators'' \cite{HORNIK1989359}. This groundbreaking work established that Feed-Forward Neural Networks (FNNs) with at least one hidden layer and non-linear activation functions can approximate any continuous function on compact subsets of $\mathbb{R}^n$ to any desired degree of accuracy, given sufficient neurons in the hidden layer:

\begin{center}
    ``\emph{A single hidden layer feed-forward neural network with S-shaped activation functions can approximate any measurable function to any desired degree of accuracy on a compact set.}''
\end{center}

\noindent
This theorem establishes the \emph{theoretical power} of neural networks: given enough hidden neurons, an FNN can approximate \textbf{any continuous function} $f(x)$ over a bounded input domain. Let's define this more formally.

\begin{theorem}[Universal Approximation Theorem]\index{Universal Approximation Theorem}
    Let:
    \begin{equation}
        f: \, \mathbb{R}^{n} \rightarrow \mathbb{R}
    \end{equation}
    By any continuous function on a compact subset of $\mathbb{R}^{n}$ ($K \subset \mathbb{R}^{n}$).

    Then, for any $\varepsilon > 0$, there exists:
    \begin{itemize}
        \item A \textbf{single-hidden-layer neural network}
        \item With \textbf{finite} number of neurons $J$
        \item And \textbf{non-linear activation} $\sigma(\cdot)$ (e.g., sigmoid, tanh, ReLU, etc.)
    \end{itemize}
    Such that for all $x \in K$:
    \begin{equation}
        \left|
            f(x) - \displaystyle\sum_{j=1}^{J}
                w_{j}^{(2)} \cdot \sigma \cdot
                    \displaystyle\sum_{i=1}^{n} \left(
                        w_{ji}^{(1)} x_{i} + b_{j}
                    \right)
        \right| < \varepsilon
    \end{equation}
    Where:
    \begin{itemize}
        \item $w_{ji}^{(1)}$ are the \textbf{weights} from \textbf{input layer to hidden layer}.
        \item $b_{j}$ are the \textbf{biases} of the \textbf{hidden layer} neurons.
        \item $\displaystyle\sum_{i=1}^{n} \left( w_{ji}^{(1)} x_{i} + b_{j} \right)$ is the \textbf{input to the hidden layer} neuron $j$.
        \item $\sigma(\cdot)$ is the \textbf{non-linear activation function} applied at \textbf{hidden} layer neurons.
        \item $w_{j}^{(2)}$ are the \textbf{weights from hidden layer to output layer}.
        \item The output is the result of the neural network for input $x$.
    \end{itemize}
\end{theorem}

\noindent
In simpler terms, any continuous can be represented by a neural network with just \textbf{one hidden layer}, if that layer has enough neurons and uses a non-linear activation function.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why it works (intuition)}}
\end{flushleft}
Each hidden neuron with non-linear activation acts like a \textbf{basis function} (similar to how sine and cosine functions can approximate any waveform in Fourier series). By combining enough of these basis functions (hidden neurons), the neural network can approximate complex functions by adjusting the weights and biases.

\highspace
Imagine we have an unknown function $f(x) = \sin(x)$. And we want our neural network to \textbf{learn} this function. So, in other words, we want our neural network to approximate $f(x)$ as closely as possible ($\hat{f}(x) \approx f(x)$). Let's take a \textbf{single neuron} with a non-linear activation function (e.g., sigmoid):
\begin{equation*}
    \sigma(a) = \dfrac{1}{1 + e^{-a}}
\end{equation*}
If we plot this, it looks like an S-shaped curve (page \pageref{fig:sigmoid-activation-function}):
\begin{itemize}
    \item Almost 0 for large negative inputs.
    \item Almost 1 for large positive inputs.
    \item Smoothly transitions between 0 and 1 around input 0.
\end{itemize}
When we apply this neuron to a \textbf{linear combination of $x$}:
\begin{equation*}
    \sigma(w \cdot x + b)
\end{equation*}
We get a \emph{shifted and stretched S-curve} along the $x$-axis. Now imagine we have \textbf{many hidden neurons}, each with their own weights and biases:
\begin{equation*}
    \hat{f}(x) = \displaystyle\sum_{j=1}^{J} w_{j}^{(2)} \cdot \sigma(w_{j}^{(1)} \cdot x + b_{j})
\end{equation*}
Each neuron produces its own ``bump'' or ``S-step'' at a different location. When we \textbf{add them together}, those bumps \textbf{stack up and blend}, creating any curve shape we want. And that's the whole trick! Just like adding sinusoids can approximate any periodic signal (Fourier series), adding non-linear S-shaped functions can approximate any continuous curve. Note that non-linearity is crucial. If we used only linear activations and stacked them, the result would still be a linear function. This would collapse the network's expressive power.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Important note}}
\end{flushleft}
The \hl{Universal Approximation Theorem guarantees that a neural network} \textbf{can} \hl{approximate any continuous function}, but it does not tell us \textbf{how to find} the right \hl{weights and biases to do so}. In practice, training a neural network to approximate a specific function requires effective optimization algorithms (like gradient descent) and sufficient training data. Additionally, while a single hidden layer is theoretically sufficient, deeper networks (with more hidden layers) often learn more efficiently and generalize better in practice.