\subsubsection{Output Layer}

The \textbf{output layer} is the \emph{last} layer of the network, the one that produces the model's \textbf{final prediction}. Up to this point, the \textbf{hidden layers} have been learning to extract useful features (patterns, relationships, hierarchies). But the \textbf{output layer} translates all of that internal representation into the final, human-meaningful result. For example:
\begin{itemize}
    \item A \textbf{continuous number} (e.g., house price) in \textbf{regression tasks} (e.g., predicting a numerical value).
    \item Or a \textbf{class label} (e.g., cat vs. dog) in \textbf{classification tasks} (e.g., categorizing images).
\end{itemize}
So, the \textbf{choice of activation function} in the output layer depends on the \textbf{type of output we want}. Exist several options:
\begin{itemize}
    \item For \textbf{regression tasks}, where we want to predict a continuous value, we often use a \textbf{linear activation function} (or no activation function at all) in the output layer. This allows the network to produce a wide range of values.
    \item For \textbf{binary classification tasks}, where we want to classify inputs into two classes, we typically use the \textbf{sigmoid activation function} in the output layer. This squashes the output to a value between 0 and 1, which can be interpreted as a probability.
    \item For \textbf{multi-class classification tasks}, where we want to classify inputs into more than two classes, we often use the \textbf{softmax activation function} in the output layer. This produces a probability distribution over the classes, ensuring that the sum of the outputs equals 1.
\end{itemize}
The \textbf{design of the output layer} is crucial because it directly affects how well the network can perform its intended task. Choosing the appropriate activation function and structure for the output layer ensures that the network's predictions are meaningful and useful for the specific problem at hand.

\newpage

\paragraph{Regression}

In \textbf{regression problems}, we want the network to predict \textbf{a real-valued quantity}, something that can take \emph{any} number, positive or negative. For example, predicting the price of a house based on its features (size, location, number of rooms, etc.) is a regression task. In this case, the \textbf{output layer} of the neural network typically consists of a \textbf{single neuron} that produces a \textbf{continuous output}, not categorical labels (we want a number, not a class like ``expensive'' or ``cheap'').

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{The output function}}
\end{flushleft}
For regression tasks, we don't want to limit or distort the network's output. Therefore, the last layer simply uses a \textbf{linear activation} (page \pageref{par:linear-activation-function}):
\begin{equation*}
    f(a) = a \quad \text{or equivalently} \quad y = w^{T} x + b
\end{equation*}
This means the output neuron just returns the raw weighted sum of its inputs, no squashing or thresholding.

\highspace
If we used a \textbf{sigmoid} or \textbf{tanh} activation in the output layer, the output would be forced into $\left(0,1\right)$ or $\left(-1,1\right)$ ranges, respectively. This would be problematic for regression tasks where the target variable can take on a wide range of values. For example, if we're predicting house prices, we want the output to be able to represent any price, not just values between 0 and 1 (e.g., a house could cost $\$250,000$, which is far outside the range of a sigmoid output, or a temperature could be $-10$ degrees Celsius, which is outside the range of tanh). The \textbf{linear} activation allows any real number to be output, making it suitable for regression tasks.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Typical network setup for regression}}
\end{flushleft}
A typical neural network for regression tasks has the following structure:

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l p{24em} @{}}
        \toprule
        \textbf{Component} & \textbf{Example} \\
        \midrule
        Hidden layers   & Several, with nonlinear activations (e.g., ReLU, tanh). \\[.3em]
        Output layer    & One neuron (for single output) with \textbf{linear activation}. \\[.3em]
        Loss function   & \textbf{Mean Squared Error (MSE)} or \textbf{Mean Absolute Error (MAE)}. \\
        \bottomrule
    \end{tabular}
\end{table}