\section{From Perceptrons to FNNs}

\subsection{Historical Context}

When Artificial Intelligence first emerged as a field in the 1940s and 1950s, researchers were fascinated by the idea of creating machines that could \emph{think}, \emph{adapt}, and \emph{learn} as the human brain does. At that time, traditional computers were already capable of executing precise, deterministic instructions with incredible speed. However, these \textbf{early machines lacked flexibility}: they \hl{could not interpret noisy or ambiguous input}, \hl{nor could they modify their behavior from experience}.

\highspace
This limitation led scientists to look beyond the rigid Von Neumann architecture\footnote{The sequential model where computation and memory are separated} and toward the \textbf{brain} as an alternative computational paradigm. The human brain, with its billions of interconnected neurons, represented a radically different kind of machine: \textbf{massively parallel}, \textbf{distributed}, \textbf{redundant}, and \textbf{fault-tolerant}. Each neuron is \emph{simple}, yet together they form a system capable of extraordinary complexity and adaptability.

\highspace
From this inspiration arose the idea of \textbf{neural networks}: mathematical models built from simple interconnected units that imitate, in a highly abstract way, the behavior of biological neurons. Interestingly, neural networks are not a recent invention of the deep learning era: they have existed since the birth of AI itself. In fact, the phrase ``\emph{Deep Learning is not AI, nor Machine Learning}'' emphasizes that \textbf{deep learning is a later evolution within this larger historical continuum}. Neural networks have been a foundational approach to artificial intelligence from its inception, long before modern computational power and data made them successful.

\highspace
In summary, the reason researchers in the 1940s and 1950s looked ``beyond Von Neumann'' was that they sought to create machines that could \textbf{learn from experience} and \textbf{adapt to new situations}, capabilities that traditional computers lacked:
\begin{itemize}
    \item \textbf{1940s motivation}: classic computers excelled at precise, fast arithmetic but researches wanted systems that could \textbf{interact with noisy data}, be \textbf{parallel and fault-tolerant}, and \textbf{adapt}.
    \item \textbf{Brain as a computational model}: the brain offers a radically different architecture that is massively parallel, distributed, redundant system. These properties are an appealing template for computation, which inspired artificial neurons and later full neural networks.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{The inception of AI}}
\end{flushleft}
In the years immediately following the Second World War, a new scientific dream began to take shape: the \hl{idea that intelligence could be recreated in a machine}. Early pioneers such as \textbf{Alan Turing}, \textbf{John von Neumann}, \textbf{Warren McCulloch}, and \textbf{Walter Pitts} laid the foundations of what would soon be called \emph{Artificial Intelligence}. Computers had just proven they could follow precise instructions and perform huge calculations at incredible speed, yet these machines were nothing more than rigid automata: they obeyed every command literally, unable to perceive, reason, or learn.

\highspace
The emerging field of AI was born from the desire to bridge that gap, to make machines that could \textbf{adapt}, \textbf{generalize from experience}, and \textbf{interact intelligently} with the world. The 1940s and 1950s were therefore an era of conceptual excitement: \emph{could the brain's mechanisms be modeled mathematically and implemented in hardware or software?} The \hl{earliest experiments sought to replicate the nervous system's structure}, creating computational units that mimicked neurons and synapses. These units could, in principle, activate or remain silent depending on the inputs they received, a primitive form of reasoning.

\highspace
At this stage, AI and neural networks were inseparable: \textbf{to build an intelligent machine meant to build an artificial brain}. Over the next decades, this vision would split into two main traditions. One emphasized \emph{symbolic} reasoning (manipulating explicit rules and logic) while the other, the \emph{connectionist} approach, pursued learning from examples through networks of simple computational nodes. The second line, though overshadowed for many years, would eventually resurface as what we now call \important{Deep Learning}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{brain} \textbf{From Von Neumann Machines to Brain-Inspired Models}}
\end{flushleft}
In the 1940s, the \textbf{Von Neumann architecture} defined what we still call a \emph{classical computer}: a machine with a central processor (CPU) that executes instructions stored in memory, step by step, following a deterministic sequence. This design is extremely powerful for arithmetic and logic, but it has key limitations when the goal is to emulate intelligence.

\highspace
A Von Neumann computer is \textbf{serial}, \textbf{rigid}, and \textbf{exact}: it does exactly what it's told, line by line. Intelligence, however, requires something different, the ability to handle \textbf{noisy or incomplete data}, \textbf{recover from errors}, \textbf{adapt to change}, and \textbf{operate in parallel} on many signals at once. The human brain, in contrast, is a \textbf{massively parallel} and \textbf{distributed} system made of roughly $10^{11}$ neurons, each connected to thousands of others through $10^{14}$ to $10^{15}$ synapses.

\highspace
This comparison motivated the idea of a \textbf{computational model inspired by the brain}. Instead of a single central processor, the brain uses huge numbers of simple processing units (neurons) working together. \hl{Each neuron performs a small, nonlinear operation, but their collective behavior gives rise to perception, reasoning, and learning.}

\highspace
Researchers realized that if intelligence in humans comes from these interactions, perhaps \hl{machines could become intelligent by simulating} \textbf{networks of artificial neurons}, each following simple rules, but collectively capable of complex, adaptive computation.

\newpage

\noindent
In short:
\begin{itemize}
    \item Von Neumann: deterministic, sequential, rigid.
    \item Brain-inspired: parallel, adaptive, fault-tolerant.
\end{itemize}
This shift marks the conceptual birth of \textbf{neural networks} as a new computational paradigm.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{history} \textbf{Neural Networks in the Early AI Era}}
\end{flushleft}
The idea of taking the \textbf{human brain} as a model for computation stems from its extraordinary complexity and efficiency. A typical adult brain contains around \textbf{100 billion neurons} ($10^{11}$), and each neuron is connected to roughly \textbf{7'000} others, forming an estimated $10^{14}$ - $5 \times 10^{14}$ \textbf{synapses}, even reaching $10^{15}$ in a three-year-old child.

\highspace
Despite being slow compared to digital processors (neurons fire in milliseconds, not nanoseconds), the brain's power lies in its \textbf{massive parallelism} and \textbf{redundancy}. Each \textbf{neuron is a simple processing element}, but \hl{together} they \hl{create} a \hl{distributed, nonlinear, and fault-tolerant system} capable of perception, reasoning, adaptation, and learning; functions that no single algorithmic machine of the 1940s could perform.

\highspace
From a computational viewpoint, this means:
\begin{itemize}
    \item \important{Processing is distributed}: no central control; intelligence arises from interactions.
    \item \important{Information is encoded collectively}: a concept survives even if some neurons fail.
    \item \important{Parallelism ensures speed and robustness}: thousands of operations occur simultaneously.
    \item \important{Adaptivity}: synaptic strengths (connections) change with experience, enabling learning.
\end{itemize}
These characteristics inspired the \textbf{first attempts to formalize ``neurons'' mathematically}, giving rise to the \important{perceptron} and to the field of \emph{artificial neural networks}. The \hl{perceptron} is, in essence, a \hl{simplified abstraction of how a biological neuron integrates inputs, applies a threshold, and produces an output}. An idea that we'll explore in the following section.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{What about the computation of biological versus artificial neurons?}}
\end{flushleft}
\textcolor{Red2}{\faIcon{brain}} In a \textbf{biological neuron}, information is transmitted through \textbf{electrochemical signals}:
\begin{itemize}
    \item The \textbf{dendrites} receive inputs from other neurons through \emph{synapses}.
    \item Each input can be \textbf{excitatory} (it increases activation) or \textbf{inhibitory} (it decreases activation).
    \item The neuron \textbf{integrates} all these signals in the \textbf{cell body (soma)}.
    \item When the total accumulated signal exceeds a \textbf{threshold}, the neuron \textbf{fires}, sending an output through its \textbf{axon} to other neurons.
\end{itemize}
Although this process is complex and involves various biochemical mechanisms, it can be summarized as:
\begin{equation*}
    \text{collect inputs} \rightarrow \text{integrate} \rightarrow \text{compare with threshold} \rightarrow \text{fire}
\end{equation*}
\textcolor{Red2}{\faIcon{cogs}} But how to model this computationally? In the \textbf{artificial version}, we simplify this biological process into a mathematical model:
\begin{equation*}
    h_{j}\left(x, w, b\right) = f\left(\sum_{i=1}^{I} w_{i} x_{i} - b\right) = f\left(w^{T} x\right)
\end{equation*}
Where:
\begin{itemize}
    \item $x_{i}$ are the input values (analogous to signals received by dendrites). They are like the neurotransmitter signals that a biological neuron receives from other neurons.
    \item $w_{i}$ are the weights (analogous to synaptic strengths). They represent how strongly each input influences the neuron's activation.
    \item $b$ is the bias (analogous to the threshold). It determines the level of input required for the neuron to activate.
    \item $f\left(\cdot\right)$ is the activation function (analogous to the firing mechanism). It decides whether the neuron fires based on the integrated input.
\end{itemize}
Each artificial neuron thus performs three main steps:
\begin{enumerate}
    \item \important{Weighted sum} of its inputs (integration): $\displaystyle\sum_{i=1}^{I} w_{i} x_{i}$.
    \item \important{Subtracts the bias} (thresholding): $\displaystyle\sum_{i=1}^{I} w_{i} x_{i} - b$.
    \item \important{Applies the activation function} (firing decision): $f\left(\displaystyle\sum_{i=1}^{I} w_{i} x_{i} - b\right)$.
\end{enumerate}

\newpage

\begin{definitionbox}[: Artificial Neuron]\label{def:artificial-neuron}
    An \definition{Artificial Neuron} is a \textbf{mathematical model} inspired by the way a biological neuron works. It's the \textbf{basic computation unit} of a neural network.

    \highspace
    While a real neuron collects electrical signals from thousands of connections (synapses) and ``fires'' if the total signal passes a threshold, an artificial neuron does the same thing, but with numbers.

    \highspace
    Formally, it takes several inputs $\left(x_1, x_2, \ldots, x_I\right)$, multiplies each by a \textbf{weight} $w_i$, sums them, adds a \textbf{bias} $b$, and passes the result through an \textbf{activation function} $f(\cdot)$:
    \begin{equation}
        h_{j}\left(x, w, b\right) = f\left(\sum_{i=1}^{I} w_{i} x_{i} - b\right) = f\left(w^{T} x\right)
    \end{equation}
    Where:
    \begin{itemize}
        \item \textbf{Inputs} $\left(x_i\right)$: the signals coming from other neurons or from data (e.g., pixel values).
        \item \textbf{Weights} $\left(w_i\right)$: how strong each input connection is (analogous to synaptic strength).
        \item \textbf{Bias} $\left(b\right)$: shifts the activation threshold up or down.
        \item \textbf{Activation function} $\left(f\right)$: decides whether the neuron ``fires'' (outputs a strong signal) or stays quiet.
    \end{itemize}
    In essence, the pipeline of an artificial neuron is:
    \begin{equation*}
        \text{Weighted sum} \rightarrow \text{Threshold/Bias} \rightarrow \text{Nonlinear activation} \rightarrow \text{Output}
    \end{equation*}
\end{definitionbox}

\begin{definitionbox}[: Bias]
    The \definition{Bias} is an additional parameter in an artificial neuron that allows the activation function $f$ to be shifted horizontally, providing the model with the ability to represent patterns that do not pass through the origin.

    \highspace
    Mathematically, it appears as the constant term $b$ in the neuron's activation equation (see page \pageref{def:artificial-neuron}):
    \begin{equation*}
        a = w^T x + b
    \end{equation*}
    The bias represents the \textbf{intrinsic tendency of a neuron to activate}, even in the absence of input. It acts like a tunable threshold that controls \emph{when} the neuron fires.

    \highspace
    Think of the bias as the neuron's \textbf{default tendency to fire}, it decides \emph{how easy or hard} it is for the neuron to activate:
    \begin{itemize}
        \item A \textbf{large positive bias} $\to$ neuron tends to fire even with small input.
        \item A \textbf{large negative bias} $\to$ neuron needs strong evidence (large input sum) to fire.
    \end{itemize}
    In other words, the bias \emph{shifts the activation threshold} left or right along the input axis, allowing the neuron to learn more complex decision boundaries.

    \highspace
    Imagine a simple rule: ``\emph{if the weighted sum of our inputs is greater than 0, we output 1}''. Now suppose all our inputs are zero ($x_1 = x_2 = 0$). If we want the neuron to still fire in that case, we need a \textbf{bias} to ``push'' it over the threshold. Bias gives the neuron a \emph{baseline activity}, like saying: ``\emph{even if there's not input, we are slightly inclined to fire}''.
\end{definitionbox}

\highspace
So, an \textbf{artificial neuron} mimics the logical essence of a biological one: a small computing unit that combines multiple inputs into one output, depending on the learned connection strengths (weights) and a bias term. This is the foundation of the \textbf{perceptron}, the first neural network model, the topic of the next section.