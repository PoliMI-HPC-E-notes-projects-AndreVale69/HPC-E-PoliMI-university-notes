\subsubsection{Error Minimization and Loss Function (SSE)}\label{sec:error-minimization-and-loss-function}

To teach a neural network, we need a way to \textbf{measure how wrong} it is. That measure is the \textbf{error (loss) function}. Once defined. we can \textbf{minimize} it by adjusting the weights, and the process \textbf{is} the essence of learning.

\highspace
\begin{definitionbox}[: Error Function]
    Given a \textbf{training set}:
    \begin{equation}
        \mathcal{D} = \Biggl\{\left(x_{n}, t_{n}\right)\Biggr\}_{n=1}^{N}
    \end{equation}
    And the network's predictions:
    \begin{equation}
        y_{n} = g\left(x_{n}; w\right)
    \end{equation}
    The \definition{Error Function $E(w)$} measures the total discrepancy between all predictions and their true targets:
    \begin{equation}
        E(w) = \displaystyle\sum_{n=1}^{N} \text{Loss}(t_{n}, y_{n})
    \end{equation}
    Where $\text{Loss}\left(\cdot\right)$ is the per-sample difference between the predicted and actual value.
\end{definitionbox}

\begin{definitionbox}[: Loss Function]
    A \definition{Loss Function} (sometimes called \textbf{error} or \definition{Cost Function}) is a \textbf{mathematical function that quantifies how wrong a model's predictions are} compared to the true (target) values. In other words:
    \begin{equation}
        \text{Loss}(t, y) = \text{scalar measure of discrepancy between } t \text{ and } y
    \end{equation}
    Where $t$ is the true target value, and $y$ is the model's prediction. The loss function \textbf{outputs a single number representing how bad the prediction is}; \hl{lower values indicate better predictions}. During training/learning, the network tries to \textbf{minimize} this loss by adjusting its weights, to reduce its mistakes.

    \highspace
    It is strictly related to the \textbf{Error Function} $E(w)$, which aggregates the loss over the entire training set to give a total measure of how well the model is performing. Indeed, for a single training example $\left(x_{n}, t_{n}\right)$, we have:
    \begin{equation}
        L_{n} = \text{Loss}\left(t_{n}, g\left(x_{n}; w\right)\right)
    \end{equation}
    Where $L_{n}$ is the loss for sample $n$, $t_{n}$ is the true target, and $g\left(x_{n}; w\right)$ is the model's prediction for input $x_{n}$ with weights $w$. And the total error function over all $N$ samples is:
    \begin{equation*}
        E(w) = \displaystyle\sum_{n=1}^{N} L_{n} = \displaystyle\sum_{n=1}^{N} \text{Loss}\left(t_{n}, g\left(x_{n}; w\right)\right)
    \end{equation*}
    So, the \textbf{loss function measures the error for one sample}, while the \textbf{Error Function sums these losses over the entire dataset} to give a total error measure.
\end{definitionbox}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{The simplest and most classic choice: SSE}}
\end{flushleft}
Exists many choices for the loss function. The simplest and most classic is the \textbf{Sum of Squared Errors (SSE)}. In early neural networks (and still often in regression problems), the \definition{Sum of Squared Errors (SSE)} was the standard loss function:
\begin{equation}
    E(w) = \displaystyle\sum_{n=1}^{N} \text{Loss}(t_{n}, y_{n}) = \displaystyle\sum_{n=1}^{N} \left[t_{n} - g\left(x_{n}; w\right)\right]^{2}
\end{equation}
\begin{itemize}
    \item $t_{n}$ is the true target for sample $n$.
    \item $g\left(x_{n}; w\right)$ is the network's prediction for input $x_{n}$.
\end{itemize}
Similar to the Mean Squared Error (MSE, page \pageref{eq:mse}), the SSE squares the differences to makes all errors \textbf{positive} (so under- and over-predictions both count) and \textbf{emphasizes large errors} (penalizes them more heavily). Also, squaring makes the error function \textbf{differentiable}, which is crucial for optimization algorithms like gradient descent.

\highspace
Minimizing the SSE means finding the weights $w$ that make the network's predictions as close as possible to the true targets across the entire training set. Formally, learning becomes finding the set of weights $w^{*}$ that minimize the total error:
\begin{equation*}
    w^{*} = \arg\min_w E(w)
\end{equation*}
Each weight $w_{i}$ acts like a small knob controlling part of the network's behavior. We tweak these knobs slightly so that the network's predictions move closer to the true outputs. When all knobs are adjusted such that $E(w)$ is as small as possible, the network has \textbf{learned} the function.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Geometric Interpretation}}
\end{flushleft}
Minimizing the error means \textbf{finding a point in parameter space} $w$ \textbf{where the error surface} $E(w)$ \textbf{reaches its minimum}. We can visualize $E(w)$ as a \textbf{landscape}: valleys represent low error (good predictions), and hills represent high error (bad predictions). The learning process is like navigating this landscape to find the lowest valley, which corresponds to the optimal weights $w^{*}$ that minimize the error.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.85\textwidth]{img/learning-and-optimization/error-geometric-interpretation-1.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{How the \textbf{Sum of Squared Errors (SSE)} behaves as a function of the \textbf{model parameters (weights)}, and how \textbf{gradient descent} moves step by step toward the minimum error point. The x-axis $w_{1}$ and y-axis $w_{2}$ represent two weights (parameters) of the model/ Each contour line shows \textbf{all combinations of weights} ($w_{1}, w_{2}$) that produce the \textbf{same total error} $E(w_{1}, w_{2})$. The loss function we use:
    \begin{equation*}
        E\left(w_1, w_2\right) = \left(w_1 - 2\right)^2 + 2\left(w_2 + 1\right)^2
    \end{equation*}
    Those small points connected by a line represent the \textbf{path that gradient descent follows} over time. Starting from an initial guess, each step moves downhill toward lower error, reaching the minimum point where the error is lowest (point $(2, -1)$ in this case). In a real network, the number of weights isn't just two but can be thousands or millions, making the error surface a high-dimensional landscape. We can't visualize that directly, but this contour map is an \textbf{analogy} to help understand how optimization algorithms like gradient descent navigate the error surface to find the best weights.}
\end{figure}

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.85\textwidth]{img/learning-and-optimization/error-geometric-interpretation-2.pdf}
    \caption{The \textbf{error function} for \textbf{a single training example} using the \textbf{Sum of Squared Errors (SSE)} loss: $E(y) = \left(t-y\right)^{2}$, where $t$ is the true target and $y$ is the model's prediction. The x-axis represents the model's prediction $y$ (all possible output values the model could predict for this input sample), and the y-axis shows the corresponding error $E(y)$ (how wrong the model would be for each possible prediction). This graph shows \textbf{how the error changes} as the model output moves away from the correct answer. It is a \textbf{parabola}, because the error grows quadratically as we move away from the correct value. At $y = t$, the error is zero (the model predicts perfectly) because $E(t) = (t - t)^2 = 0$. As $y$ moves away from $t$, the difference grows, and squaring makes it \textbf{positive and larger}. The dashed vertical line marks the \textbf{minimum point} where the prediction perfectly matches the target ($y = t$, zero error).}
\end{figure}

\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{chart-bar} \textbf{Relation to Statistical Foundations}}
\end{flushleft}
The squared error has a nice \textbf{probabilistic interpretation}. If we assume that the target values $t_{n}$ are generated by a model with \textbf{Gaussian noise}:
\begin{equation*}
    t_{n} = g(x_{n}; w) + \epsilon_{n}, \quad \epsilon_{n} \sim \mathcal{N}(0, \sigma^{2})
\end{equation*}
Then minimizing the sum of squared errors (SSE) is equivalent to \textbf{Maximum Likelihood Estimation (MLE)} of the weights $w$. In other words, by minimizing SSE, we are finding the weights that make the observed data most probable under the assumed Gaussian noise model. So the SSE is not arbitrary choice; it has a solid statistical basis when the noise in the data is Gaussian.