\subsection{Learning and Optimization}

Let's retrace what we've built so far step by step:
\begin{enumerate}
    \item We started with the \textbf{historical context}, understanding why we want machines to ``learn'' like brains, transitioning from symbolic AI to data-driven learning.
    \item Next, we explored the \textbf{Perceptron}, the simplest computational neuron, which introduced us to linear decision boundaries and Hebbian learning.
    \item However, we also learned about the \textbf{limitations of the Perceptron}, particularly its inability to solve non-linear problems like XOR.
    \item To overcome these limitations, we delved into \textbf{Feed-Forward Neural Networks (FNNs)}, discovering how multi-layer networks with hidden layers and nonlinear activations can model complex functions.
    \item Finally, we touched on the \textbf{Universal Approximation Theorem}, which assures us that even a single hidden layer is theoretically sufficient to approximate any function (though in practice, deeper networks often perform better).
\end{enumerate}
Now we know \textbf{what} the architecture can represent. But we haven't yet learned \textbf{how} to \emph{find the right weights} that make it represent what we want. And that's \emph{exactly} why this section begins.

\highspace
After defining the structure of a neural network, we must \textbf{teach it} to perform a task, such as classifying images or predicting values. This teaching process is called \textbf{learning} or \textbf{training} (or simply \emph{learning by optimization}). Think of the journey like this:
\begin{equation*}
    \text{Architecture} \, \rightarrow \,
    \text{Function Space} \, \rightarrow \,
    \text{Optimization} \, \rightarrow \,
    \text{Learning}
\end{equation*}
We've defined the \textbf{function space} (what kinds of functions the network can represent), and now, we must \textbf{search inside that space} for the specific function that matches our data. This search is done through \textbf{optimization algorithms} that adjust the network's weights based on the data we provide. So, in this section, we'll answer three big questions:
\begin{enumerate}
    \item \emph{\textbf{How does a neural networks learn?}} By comparing predictions with known targets (supervised learning).
    \item \emph{\textbf{How do we measure ``how wrong'' it is?}} Through \emph{loss functions} (some of which we have already encountered in the output layer design).
    \item \emph{\textbf{How do we improve it?}} Through \emph{optimization algorithms} like gradient descent and (later) backpropagation.
\end{enumerate}