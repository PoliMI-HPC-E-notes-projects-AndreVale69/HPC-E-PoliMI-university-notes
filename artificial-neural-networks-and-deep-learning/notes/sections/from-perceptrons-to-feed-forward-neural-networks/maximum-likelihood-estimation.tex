\subsection{Maximum Likelihood Estimation (MLE)}\label{sec:maximum-likelihood-estimation}

In the previous sections, we discussed how to \emph{compute} gradients (backpropagation) and \emph{optimize} weights (gradient descent) in neural networks. But \textbf{why} do we minimize a loss function in the first place? What's the \textbf{statistical justification} for this approach? Is there a \textbf{probabilistic interpretation} of learning in neural networks? And why is it important?

\highspace
We've already seen that when we train a neural network, we \emph{minimize a loss function} $E(w)$ (page \pageref{eq:optimal-weights}). MLE gives a \textbf{statistical justification} for that loss: it's equivalent to \textbf{maximizing the probability} of observing our data given the model parameters. In other words, \textbf{MLE turns learning into a probability problem}.

\highspace
\begin{definitionbox}[: Maximum Likelihood Estimation (MLE)]
    Let our model have parameters $\theta$ (or $w$ in neural networks), and our dataset be:
    \begin{equation*}
        \mathcal{D} = \left\{ x_{1}, x_{2}, \ldots, x_{N} \right\}
    \end{equation*}
    We assume that each data point is drawn from a probability distribution that depends on those parameters:
    \begin{equation*}
        p\left(x_{n} \, \mid \, \theta\right)
    \end{equation*}
    If all samples are \textbf{i.i.d.} (independent and identically distributed) then the probability of the entire dataset is:
    \begin{equation*}
        p\left(\mathcal{D} \, \mid \, \theta\right) = \displaystyle\prod_{n=1}^{N} p\left(x_{n} \, \mid \, \theta\right)
    \end{equation*}
    This is called the \textbf{likelihood function}:
    \begin{equation}
        L\left(\theta\right) = p\left(\mathcal{D} \, \mid \, \theta\right) = \displaystyle\prod_{n=1}^{N} p\left(x_{n} \, \mid \, \theta\right)
    \end{equation}
    Then the \definition{Maximum Likelihood Estimation (MLE)} is the value of $\theta$ that maximizes this likelihood:
    \begin{equation}
        \hat{\theta}_{\text{MLE}} = \arg\max_{\theta} L\left(\theta\right) = \arg\max_{\theta} p\left(\mathcal{D} \, \mid \, \theta\right)
    \end{equation}
    In practice, we often maximize the \textbf{log-likelihood} instead:
    \begin{equation}
        \hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \log L\left(\theta\right) = \arg\max_{\theta} \log p\left(\mathcal{D} \, \mid \, \theta\right)
    \end{equation}
    Because the logarithm is a monotonic function, maximizing the log-likelihood is equivalent to maximizing the likelihood itself. Also, the logarithm allows us to turn products into sums, which are easier to work with.
\end{definitionbox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What does i.i.d. mean?}}\label{def:iid}
\end{flushleft}
The acronym \definitionWithSpecificIndex{i.i.d.}{Independent and Identically Distributed (i.i.d.)}{} stands for \emph{independent and identically distributed}. It's a \textbf{fundamental assumption} in statistics that simplifies how we model data. When we say that our samples are i.i.d., we mean two things:
\begin{enumerate}
    \item \important{Independent}. Each observation (data point) does \textbf{not depend} on the others. Formally:
    \begin{equation*}
        p\left(x_{1}, x_{2}, \dots, x_{N}\right) = \displaystyle\prod_{n=1}^{N} p\left(x_{n}\right)
    \end{equation*}
    So, knowing $x_{1}$ tells us nothing about $x_{2}$, and so on. Each is separate, independent draw from the underlying process. For example, if we toss a coin 10 times, each toss is independent of the others, assuming the coin is fair and the tosses don't influence each other.

    \item \important{Identically Distributed}. All samples come from the \textbf{same probability distribution}. Formally:
    \begin{equation*}
        x_{n} \sim p\left(x \mid \theta\right) \quad \forall n
    \end{equation*}
    This means they are generated by the \textbf{same model parameters} ($\theta$) (e.g., same mean and variance in a Gaussian). For example, each coin toss has the same probability ($P(\text{Head}) = 0.5$).
\end{enumerate}
So, when we say:
\begin{equation*}
    x_{1}, x_{2}, \ldots, x_{N} \overset{\text{are i.i.d.}}{\sim} p\left(x \mid \theta\right)
\end{equation*}
We mean that each sample $x_{n}$ is an independent draw from the \textbf{same} distribution underlying probability parameterized by $\theta$. This is the \textbf{standard assumption} when deriving the Likelihood:
\begin{equation*}
    p\left(\mathcal{D} \, \mid \, \theta\right) = \displaystyle\prod_{n=1}^{N} p\left(x_{n} \, \mid \, \theta\right)
\end{equation*}
Without the i.i.d. assumption, we couldn't factor the joint probability into a product of individual probabilities, making the analysis much more complex.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{lightbulb} \textbf{Core Idea of MLE}}
\end{flushleft}
The core idea of MLE is to find the parameter values that make the \textbf{observed data most probable} under our assumed statistical model. By maximizing the likelihood function, we identify the parameters that best explain the data we have collected. Let's illustrate this with a simple example.

\highspace
Imagine a \textbf{Gaussian distribution} (bell curve) centered around some unknown true mean $\hat{\mu}$. Each black point below the curve represents one \textbf{sample} (observation): $x_{1}, x_{2}, \ldots, x_{N}$. These samples:
\begin{itemize}
    \item Come from the \textbf{same distribution} $\mathcal{N}\left(\mu, \sigma^{2}\right)$, so they are \textbf{identically distributed}. It means they share the same mean $\mu$ and variance $\sigma^{2}$.
    \item Are drawn \textbf{independently} from each other, so they are \textbf{independent}.
\end{itemize}
If we were to repeat the sampling many times, we'd get different sets of samples, but all coming from \emph{the same bell curve}.

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/maximum-likelihood-estimation/gaussian-samples.pdf}
\end{figure}

\noindent
Now, MLE asks: ``\emph{which value of $\mu$ would make these observed points most likely under the Gaussian model?}'' If we shift the curve too far left or right, the points no longer lie near its center, reducing their likelihood. The ``best'' $\mu$ is the one centering the curve on the data cloud, called $\hat{\mu}_{\text{MLE}}$. This $\hat{\mu}_{\text{MLE}}$ is the \textbf{Maximum Likelihood Estimate} of the mean.

\highspace
Let's derive the MLE for the mean of a Gaussian distribution step-by-step.
\begin{enumerate}
    \item \important{Model assumption}. We assume our data points are i.i.d. samples from a Gaussian distribution:
    \begin{equation*}
        x_1, x_2, \dots, x_N \sim \mathcal{N}(\mu, \sigma^2)
    \end{equation*}
    Where $\sigma^2$ is known, and $\mu$ is the parameter we want to estimate. Each point has probability density:
    \begin{equation*}
        p\left(x_n \mid \mu, \sigma^{2}\right) = \dfrac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\dfrac{\left(x_n - \mu\right)^2}{2 \sigma^2}\right)
    \end{equation*}
    The \textbf{probability density} is a function that describes the likelihood of a random variable taking on a specific value. For continuous variables, it indicates how dense the probability is around that value.

    \item \important{Likelihood of the dataset}. The likelihood of the entire dataset, assuming i.i.d. samples, is:
    \begin{equation*}
        L\left(\mu\right) = p\left(\mathcal{D} \mid \mu, \sigma^{2}\right) = \prod_{n=1}^{N} p\left(x_n \mid \mu, \sigma^{2}\right)
    \end{equation*}

    \item \important{Log-likelihood}. We take the logarithm of the likelihood to simplify calculations:
    \begin{equation*}
        \log \ell\left(\mu\right) = -\dfrac{N}{2}\log\left(2 \pi \sigma^{2}\right) - \dfrac{1}{2 \sigma^{2}} \sum_{n=1}^{N} \left(x_n - \mu\right)^2
    \end{equation*}

    \important{Derivative with respect to $\mu$}. We differentiate the log-likelihood with respect to $\mu$ because we want to find the value of $\mu$ that maximizes it (and the gradient tells us where the maximum is):
    \begin{align*}
        \dfrac{\partial \ell}{\partial \mu} = & -\dfrac{1}{2\sigma^{2}} \cdot \dfrac{\partial}{\partial \mu} \sum_{n=1}^{N} \left(x_n - \mu\right)^{2} \\[.3em]
        = & -\dfrac{1}{2\sigma^{2}} \cdot \sum_{n=1}^{N} 2\left(\mu - x_n\right) \\[.3em]
        \Rightarrow & \, \dfrac{1}{\sigma^{2}} \cdot \displaystyle \sum_{n=1}^{N} \left(x_n - \mu\right)
    \end{align*}

    \item \important{Set derivative to zero}. To find the maximum, we set the derivative equal to zero and solve for $\mu$, because at the maximum point, the slope of the function is zero:
    \begin{align*}
        0 = & \, \dfrac{1}{\sigma^{2}} \cdot \displaystyle \sum_{n=1}^{N} \left(x_n - \mu\right) \\[.3em]
        \Rightarrow & \, \sum_{n=1}^{N} x_n - N \mu = 0 \\[.3em]
        \Rightarrow & \, N \mu = \sum_{n=1}^{N} x_n \\[.3em]
        \Rightarrow & \, \mu = \dfrac{1}{N} \cdot \sum_{n=1}^{N} x_n
    \end{align*}
    Thus, the MLE for the mean of a Gaussian distribution is the \textbf{sample mean}:
    \begin{equation}
        \hat{\mu}_{\text{MLE}} = \dfrac{1}{N} \cdot \sum_{n=1}^{N} x_n
    \end{equation}
\end{enumerate}
This derivation shows \hl{how MLE provides a principled way to estimate model parameters by maximizing the likelihood of the observed data}. In this case, it leads us to the intuitive result that the \hl{best estimate for the mean of a Gaussian is simply the average of the observed samples}. In simple terms, Gaussian is the geometric intuition behind the MLE of the mean.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Applying MLE to Neural Networks}}
\end{flushleft}
In neural networks we do something that looks like this:
\begin{equation*}
    \text{Find } w^{*} = \arg\min_{w} E\left(w\right)
\end{equation*}
Where $E(w)$ is the \textbf{loss function} (e.g., Mean Squared Error, Binary Cross-Entropy). MLE gives the \textbf{statistical justification} for this optimization: minimizing these loses is equivalent to maximizing the likelihood of the observed data under the model defined by the neural network with parameters $w$. So, \textbf{MLE defines \emph{what} loss we should minimize}, and \textbf{gradient descent} plus \textbf{backpropagation} define \textbf{\emph{how} to minimize it}.
\begin{enumerate}
    \item \important{Assume a probabilistic model for our targets}. We assume each training example $\left(x_{n}, t_{n}\right)$ was generated by an unknown process:
    \begin{equation*}
        t_{n} \sim p\left(t_{n} \mid x_{n}, w\right)
    \end{equation*}
    Where:
    \begin{itemize}
        \item $x_{n}$ is the input (features).
        \item $t_{n}$ is the target (label).
        \item $w$ are the model parameters (weights of the neural network).
        \item $p\left(t_{n} \mid x_{n}, w\right)$ is the probability of observing target $t_{n}$ given input $x_{n}$ and model parameters $w$.
    \end{itemize}

    \item \important{Apply the Maximum Likelihood principle}. We want to find weights that make our observed dataset as probable as possible:
    \begin{equation*}
        \hat{w}_{\text{MLE}} = \arg\max_{w} p\left(\mathcal{D} \mid w\right) = \arg\max_{w} \prod_{n=1}^{N} p\left(t_{n} \mid x_{n}, w\right)
    \end{equation*}
    Taking the logarithm:
    \begin{equation*}
        \hat{w}_{\text{MLE}} = \arg\max_{w} \sum_{n=1}^{N} \log p\left(t_{n} \mid x_{n}, w\right)
    \end{equation*}
    Or equivalently:
    \begin{equation*}
        \hat{w}_{\text{MLE}} = \arg\min_{w} E(w)
    \end{equation*}
    Where:
    \begin{equation*}
        E(w) = -\sum_{n=1}^{N} \log p\left(t_{n} \mid x_{n}, w\right)
    \end{equation*}
    Is the \textbf{negative log-likelihood loss}, in neural networks often called simply the \textbf{loss function} (used in training).

    \item \important{Depending on the probabilistic assumptions, we get different loss functions}. For example:
    \begin{itemize}
        \item For \hl{Regression tasks}, the probabilistic model is often Gaussian:
        \begin{equation*}
            t_{n} \sim \mathcal{N}\left(y\left(x_{n}, w\right), \sigma^{2}\right)
        \end{equation*}
        With logarithm of the likelihood:
        \begin{equation*}
            -\dfrac{1}{2 \sigma^{2}} \cdot \sum_{n=1}^{N} \left(t_{n} - y\left(x_{n}, w\right)\right)^{2}
        \end{equation*}
        This leads to the \textbf{Mean Squared Error (MSE)} loss (page \pageref{eq:mse}):
        \begin{equation*}
            E(w) = \dfrac{1}{N} \cdot \sum_{n=1}^{N} \left(t_{n} - y\left(x_{n}, w\right)\right)^{2} \qquad \underbrace{w^{*} = \arg\min_{w} E(w)}_{\text{optimize weights}}
        \end{equation*}

        \item For \hl{Binary Classification tasks}, the probabilistic model is often\break Bernoulli:
        \begin{equation*}
            t_{n} \sim \text{Bernoulli}\left(y\left(x_{n}, w\right)\right)
        \end{equation*}
        With logarithm of the likelihood:
        \begin{equation*}
            \sum_{n=1}^{N} \left[t_{n} \log y + \left(1 - t_{n}\right) \log\left(1 - y\right)\right]
        \end{equation*}
        This leads to the \textbf{Binary Cross-Entropy (BCE)} loss (page \pageref{eq:bce-loss}):
        \begin{equation*}
            E(w) = -\dfrac{1}{N} \cdot \sum_{n=1}^{N} \left[t_{n} \cdot \ln \left(y\left(x_{n}, w\right)\right) + \left(1 - t_{n}\right) \cdot \ln\left(1 - y\left(x_{n}, w\right)\right)\right]
        \end{equation*}
        \begin{equation*}
            \underbrace{w^{*} = \arg\min_{w} E(w)}_{\text{optimize weights}}
        \end{equation*}

        \item For \hl{Multi-Class Classification} tasks, the probabilistic model is often Categorical:
        \begin{equation*}
            t_{n} \sim \text{Categorical}\left(\mathrm{softmax}(y\left(x_{n}, w\right))\right)
        \end{equation*}
        With logarithm of the likelihood:
        \begin{equation*}
            \sum_{n=1}^{N} \sum_{c=1}^{C} t_{n,c} \log y_{c}
        \end{equation*}
        This leads to the \textbf{Categorical Cross-Entropy (CCE)} loss (page \pageref{eq:cce-loss}):
        \begin{equation*}
            E(w) = -\dfrac{1}{N} \cdot \sum_{n=1}^{N} \sum_{c=1}^{C} t_{n,c} \cdot \ln\left(y_{c}\left(x_{n}, w\right)\right)
        \end{equation*}
        \begin{equation*}
            \underbrace{w^{*} = \arg\min_{w} E(w)}_{\text{optimize weights}}
        \end{equation*}
    \end{itemize}
    So if we assume \textbf{Gaussian noise}, the MLE leads to MSE; if we assume \textbf{Bernoulli labels}, it leads to binary cross-entropy; and if we assume \textbf{Categorical labels}, it leads to categorical cross-entropy. This shows how \textbf{the choice of loss function is directly tied to our probabilistic assumptions about the data}.
\end{enumerate}
In summary, MLE provides a \textbf{statistical foundation} for training neural networks by showing that minimizing common loss functions is equivalent to maximizing the likelihood of the observed data under appropriate probabilistic models. This connection helps us understand \textbf{why} we use certain loss functions and guides us in choosing the right one based on the nature of our data and task.

\begin{deepeningbox}[: How to Choose the Error Function?]
    Now that we understand where loss functions come from via MLE, the next question is: \textbf{How do we choose the right loss function for a given problem?} The choice depends on the \textbf{type of task} (regression vs. classification) and the \textbf{underlying probabilistic assumptions} about the data.

    \highspace
    \begin{enumerate}
        \item \important{Linear}. If our model is linear and data are separable, we can use \textbf{Perceptron Loss}. This loss focuses on maximizing the margin between classes.

        \item \important{Regression Tasks}. If we're predicting continuous values, we often assume Gaussian noise in the targets. This leads us to use the \textbf{Mean Squared Error (MSE)} loss, which corresponds to maximizing the likelihood under a Gaussian model.

        \item \important{Binary Classification Tasks}. If we're classifying inputs into two classes, we typically model the targets as Bernoulli-distributed. This results in using the \textbf{Binary Cross-Entropy (BCE)} loss, which maximizes the likelihood under a Bernoulli model.

        \item \important{Multi-Class Classification Tasks}. For problems with more than two classes, we assume a Categorical distribution for the targets. This leads us to use the \textbf{Categorical Cross-Entropy (CCE)} loss, which maximizes the likelihood under a Categorical model.
    \end{enumerate}

    \highspace
    In practice, it's essential to align our choice of loss function with our assumptions about the data generation process. This ensures that our training procedure is statistically sound and that we're optimizing for the most appropriate objective given our specific problem.
\end{deepeningbox}