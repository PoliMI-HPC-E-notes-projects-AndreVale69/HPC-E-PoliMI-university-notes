\subsection{Summary}

This section starts with the \textbf{simplest neuron} (the perceptron, page \pageref{sec:the-perceptron}) and ends with the \textbf{first learning algorithm} (the Perceptron Learning Rule), showing how these early ideas evolved into the modern \textbf{feed-forward networks} that can learn complex, non-linear patterns.
\begin{enumerate}
    \item \important{Where we started: the Perceptron model} (page \pageref{sec:the-perceptron}). At the beginning of the chapter, we see historical context: the perceptron as the first trainable neural network model, invented by Frank Rosenblatt in 1957. It introduced the idea of adjusting weights based on errors to learn from data:
    \begin{equation*}
        y = \text{sign}\left(w^{T} x + b\right)
    \end{equation*}
    This was the \textbf{first artificial neuron}, a linear classifier with a hard threshold activation function. However, it could only solve linearly separable problems and had limitations (e.g., XOR problem).

    \item \important{Multilayer networks (FNNs)}. To overcome this limitation, we introduced \textbf{hidden layers}, \textbf{differentiable activations} (sigmoid, tanh), and \textbf{continuous outputs}. Now the model can approximate \textbf{any continuous function}, not just linear boundaries. This is what transforms the perceptron into a \textbf{Feed-Forward Neural Network (FNN)}.

    \item \important{How do we train these networks?} That lead to:
    \begin{itemize}
        \item \textbf{Gradient descent}: an optimization algorithm to minimize the loss function by iteratively updating weights in the direction of the steepest descent.
        \item \textbf{Backpropagation}: an efficient way to compute gradients for all weights in the network using the chain rule of calculus (a sort of cache mechanism to avoid redundant calculations).
        \item \textbf{Maximum Likelihood Estimation (MLE)}: a statistical framework to derive loss functions (e.g., cross-entropy for classification, mean squared error for regression) based on the likelihood of the observed data given the model parameters.
    \end{itemize}
    We saw how modern NNs \emph{learn}, layer by layer, using data and gradients.~

    \item \important{Return to the perceptron learning algorithm}. Once we understood \emph{how modern networks learn}, we revisited the \textbf{Perceptron Learning Algorithm} as a simple case of these principles. It uses a Stochastic Gradient Descent (SGD) approach to update weights based on individual training examples.
\end{enumerate}
