\subsubsection{Boolean Operators \& Linear Separability}

Once we've seen that a perceptron can learn \textbf{logical functions} (like AND, OR), the next natural question is:
\begin{center}
    ``\emph{Can it learn \textbf{all} possible logical operators?}''
\end{center}
Short answer: \textbf{No}. And understanding why leads to the crucial idea of \textbf{linear separability}: the key limitation of the perceptron model.

\highspace
Let's summarize the four fundamental binary logical functions (i.e., functions with two binary inputs and one binary output):

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} c l c @{}}
        \toprule
        \textbf{Operator} & \textbf{Output $=1$ when...} & \textbf{Linearly separable?} \\
        \midrule
        AND     & both inputs are 1         & \textcolor{Green3}{\faIcon{check}} Yes \\[.3em]
        OR      & at least one input is 1   & \textcolor{Green3}{\faIcon{check}} Yes \\[.3em]
        NAND    & at least one input is 0   & \textcolor{Green3}{\faIcon{check}} Yes \\[.3em]
        NOR     & both inputs are 0         & \textcolor{Green3}{\faIcon{check}} Yes \\[.3em]
        XOR     & exactly one input is 1    & \textcolor{Red3}{\faIcon{times}} No \\[.3em]
        XNOR    & both inputs are the same  & \textcolor{Red3}{\faIcon{times}} No \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
Note that the first four operators (AND, OR, NAND, NOR) are all \textbf{linearly separable}, while the last two (XOR, XNOR) are \textbf{not}. But what does ``linearly separable'' mean in this context?

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{The game changer: \emph{Linear Separability}}}
\end{flushleft}
\begin{definitionbox}[: Linearly Separable]
    A dataset is \definition{Linearly Separable} if there \textbf{exists} a straight line (in 2D), plane (in 3D), or \textbf{hyperplane} (in higher dimensions) that \textbf{perfectly divides} the \textbf{two classes of data points}. That is, all points of one class lie on one side, and all points of the other class lies on the opposite side.

    \highspace
    Formally, given a dataset with two classes, it is linearly separable if there exist weights $ w_1, w_2, \ldots, w_n $ and a bias $ b $ such that for every data point $ (x_1, x_2, \ldots, x_n) $:
    \begin{equation*}
        \begin{cases}
            w_1 x_1 + w_2 x_2 + \ldots + w_n x_n + b > 0 & \text{if the point belongs to Class 1} \\
            w_1 x_1 + w_2 x_2 + \ldots + w_n x_n + b < 0 & \text{if the point belongs to Class 2}
        \end{cases}
    \end{equation*}
    If such weights and bias exist, the dataset is linearly separable. Otherwise, no single perceptron can solve it (i.e., classify it correctly).
\end{definitionbox}

\newpage

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{The XOR problem - The classic example of non-linear separability}}
\end{flushleft}
Until now, we've seen that perceptrons can learn linearly separable functions like AND and OR. However, the linear separability limitation becomes evident when we consider some logical functions, such as XOR (exclusive OR). A little reminder of the XOR truth table:
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} c c c @{}}
        \toprule
        $ x_1 $ & $ x_2 $ & $ \text{XOR}(x_1, x_2) $ \\
        \midrule
        0 & 0 & 0 \\
        0 & 1 & 1 \\
        1 & 0 & 1 \\
        1 & 1 & 0 \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
The XOR function outputs 1 only when exactly one of its inputs is 1. If we plot the input-output pairs of the XOR function on a 2D plane, we get the following points:

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.7\textwidth]{img/perceptron/xor.pdf}
\end{figure}

\noindent
Here, the points are arranged in an ``X'' pattern:
\begin{itemize}
    \item Class 1 points are at (0, 1) and (1, 0) (opposite corners).
    \item Class 0 points are at (0, 0) and (1, 1) (remaining corners).
\end{itemize}
No single straight line can separate the Class 1 points from the Class 0 points. We'd need \emph{two lines} forming a region (a non-linear boundary). Hence, the XOR function is \textbf{not linearly separable}, and a single-layer perceptron cannot learn it.

\newpage

\noindent
In summary, the perceptron can only create \textbf{linear decision boundaries}, so:
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check-circle}}] It perfectly models \textbf{linearly separable} problems (like AND, OR, simple threshold rules).
    \item[\textcolor{Red3}{\faIcon{times-circle}}] If fails for \textbf{non-linearly separable} problems (like XOR, parity, circle-vs-ring, etc.).
\end{itemize}
This realization in the 1960s led to what's often called the ``\textbf{AI winter},'' as researchers recognized the limitations of single-layer perceptrons. However, this challenge also paved the way for the development of \textbf{multi-layer neural networks} (and backpropagation), which can overcome these limitations by creating complex, non-linear decision boundaries, combining multiple perceptrons in layers.