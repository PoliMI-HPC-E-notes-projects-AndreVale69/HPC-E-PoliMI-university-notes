\subsubsection{Hebbian Learning Rule}

Now that we understand what the Perceptron does and who invented it, let's explore \textbf{how it learns} from data. When the first artificial neurons were proposed, researchers wanted them not just to compute, but to \textbf{learn from experience}, as biological neurons do. The earliest and most influential idea for this was the \definition{Hebbian Learning Rule}, introduced by psychologist \textbf{Donald Hebb} in 1949.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{brain} \textbf{The biological intuition}}
\end{flushleft}
Donald Hebb was a psychologist, not a mathematician. In 1949, he was trying to explain \textbf{how the brain learns from experience}, without having explicit ``teachers'' or formulas. He observed that, in biological brains, learning seems to happen \textbf{through association}. That's the origin of his famous sentence:
\begin{center}
    ``\emph{Cells that fire together, wire together.}''
\end{center}
This means that if \textbf{two neurons} are \textbf{active at the same time} (one sending a signal and the other firing) then the \textbf{connection} (synapse) between them should \textbf{become stronger}. Over time, the brain reinforces useful associations automatically.

\highspace
In other words, \hl{if neuron \emph{A} consistently helps activate neuron \emph{B}, the connection from \emph{A} to \emph{B} should be strengthened}. This principle is thought to underlie learning and memory formation in the brain.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{square-root-alt} \textbf{The Artificial Version: Mathematical Formulation}}
\end{flushleft}
Now, we translate this biological intuition into a mathematical rule that can be applied to the Perceptron. In artificial neurons, ``firing'' means \emph{output} is active (e.g., output is 1). So if both input and output are active at the same time, that's equivalent to ``they fired together''. The Hebbian learning rule says:
\begin{itemize}
    \item \textbf{Increase} the weight of connections that are active when the neuron fires.
    \item \textbf{Decrease} or leave unchanged the connections that are inactive or misaligned.
\end{itemize}
To translate this into a mathematical rule for a Perceptron, we \textbf{express the weight update} as follows:
\begin{equation}
    \Delta w_i = \eta \cdot x_i \cdot t
\end{equation}
\begin{itemize}
    \item If $x_{i} > 0$ (input is active) and $t > 0$ (target output is active), both are active, then $\Delta w_i$ is positive, so the weight $w_i$ \textbf{increases}, the \hl{connection strengthens}.
    \item If $x_{i} > 0$ (input is active) but $t \leq 0$ (target output is inactive), mismatch, then $\Delta w_i$ is zero or negative, so the weight $w_i$ \textbf{decreases} or remains the same, the \hl{connection weakens}.
    \item If $x_{i} \leq 0$ (input is inactive), regardless of $t$, then no update occurs since $\Delta w_i$ is zero, the \hl{connection remains unchanged}.
\end{itemize}
Where:
\begin{itemize}
    \item $\eta$ is the \important{learning rate}, a small \hl{positive constant that controls how much the weights are adjusted during each update}. It ensures that learning is gradual and stable. To make an analogy, think of $\eta$ as the \textbf{speed limit} on a road: it prevents the learning process from speeding ahead too quickly and potentially crashing (i.e., diverging).
    \item $x_{i}$ is the $i^{th}$ \important{input value} to the Perceptron.
    \item $t$ is the \important{target output} (desired response) for the given input.
    \item $\Delta w_i$ is the \important{change in weight} for the $i^{th}$ input. This change is added to the current weight $w_i$ to get the new weight.
\end{itemize}
The full update rule becomes:
\begin{equation}
    w_{i}^{(k+1)} = w_i^{(k)} + \Delta w_i = w_i^{(k)} + \eta \cdot x_i \cdot t
\end{equation}
This is the \definitionWithSpecificIndex{Weight Update Rule}{Weight Update Rule in Hebbian Learning}{}. It tells us \emph{how to modify} each connection $w_{i}$ after seeing one training example. Conceptually, at each learning step (each training example):
\begin{enumerate}
    \item \textbf{Take the current weights} $w_{i}^{(k)}$.
    \item \textbf{Compute} how much they should change $\Delta w_{i} = \eta \cdot x_{i} \cdot t$.
    \item \textbf{Add that change} to get the new weights $w_{i}^{(k+1)}$.
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How it works}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Initialize} all weights $w_i$ to small random values (or zeros).
    \item \textbf{Set} the learning rate $\eta$ to a small positive value (e.g., $0.01$).
    \item For each \textbf{training example} $\left(x, t\right)$:
    \begin{itemize}
        \item \textbf{Compute the Perceptron's output} $y$ using the current weights:
        \begin{equation*}
            y = f\left(w^{T} x\right)
        \end{equation*}
        \item \textbf{Compare with the target} $t$.
        \begin{itemize}
            \item[\textcolor{Green3}{\faIcon{check-circle}}] If \hl{$y = t$}, the output $y$ matches the target $t$, the neuron is already correct, so \textbf{\underline{no weight update} is needed} since the association is already learned.
            \item[\textcolor{Red2}{\faIcon{times-circle}}] If \hl{$y \neq t$}, the output $y$ does not match the target $t$, the neuron is incorrect, and we need to \textbf{update the weights} to strengthen the association. This is done using the Hebbian learning rule:
            \begin{equation*}
                w_{i}^{(k+1)} = w_{i}^{(k)} + \eta \cdot x_{i} \cdot t
            \end{equation*}
            This can be explained in informal steps:
            \begin{itemize}
                \item For each weight $w_i$, compute the change $\Delta w_i = \eta \cdot x_i \cdot t$
                \item Update the weight: $w_i \leftarrow w_i + \Delta w_i$
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item Repeat until all examples are correctly classified or a stopping criterion is met (e.g., a maximum number of iterations).
\end{enumerate}

\newpage

\begin{examplebox}[: Hebbian Learning Rule]
    Let's say we're learning a simple OR function with two inputs $x_1$ and $x_2$. The target outputs $t$ for the four possible input combinations are:
    \begin{multicols}{2}
        \begin{itemize}
            \item $x = [0, 0] \rightarrow t = 0$
            \item $x = [0, 1] \rightarrow t = 1$
            \item $x = [1, 0] \rightarrow t = 1$
            \item $x = [1, 1] \rightarrow t = 1$
        \end{itemize}
    \end{multicols}
    We do not include a bias term in this example for simplicity. We'll use a step activation function:
    \begin{equation*}
        f(z) = \begin{cases}
            1 & \text{if } z \geq 0 \\
            0 & \text{if } z < 0
        \end{cases}
    \end{equation*}
    The algorithm proceeds as follows:
    \begin{enumerate}
        \item \important{Initialize weights} $w_1 = 0.0$, $w_2 = 0.0$ and learning rate $\eta = 0.1$.
        \item \important{First training example} $x = [0, 0]$, $t = 0$:
        \begin{itemize}
            \item[\faIcon{cogs}] \textbf{Compute output}: $y = f(0.0 \cdot 0 + 0.0 \cdot 0) = f(0) = 0$
            \item[\textcolor{Green3}{\faIcon{check-circle}}] Output matches target, so \textbf{no weight update needed}.
        \end{itemize}
        \item \important{Second training example} $x = [0, 1]$, $t = 1$:
        \begin{itemize}
            \item[\faIcon{cogs}] \textbf{Compute output}: $y = f(0.0 \cdot 0 + 0.0 \cdot 1) = f(0) = 0$
            \item[\textcolor{Red2}{\faIcon{times-circle}}] Output does not match target, so we \textbf{update weights}:
            \begin{align*}
                \Delta w_1 & = 0.1 \cdot 0 \cdot 1 = 0.0 \\
                \Delta w_2 & = 0.1 \cdot 1 \cdot 1 = 0.1 \\
                w_1 & \leftarrow 0.0 + 0.0 = 0.0 \\
                w_2 & \leftarrow 0.0 + 0.1 = 0.1
            \end{align*}
        \end{itemize}
        \item \important{Third training example} $x = [1, 0]$, $t = 1$:
        \begin{itemize}
            \item[\faIcon{cogs}] \textbf{Compute output}: $y = f(0.0 \cdot 1 + 0.1 \cdot 0) = f(0) = 0$
            \item[\textcolor{Red2}{\faIcon{times-circle}}] Output does not match target, so we \textbf{update weights}:
            \begin{align*}
                \Delta w_1 & = 0.1 \cdot 1 \cdot 1 = 0.1 \\
                \Delta w_2 & = 0.1 \cdot 0 \cdot 1 = 0.0 \\
                w_1 & \leftarrow 0.0 + 0.1 = 0.1 \\
                w_2 & \leftarrow 0.1 + 0.0 = 0.1
            \end{align*}
        \end{itemize}
        \item \important{Fourth training example} $x = [1, 1]$, $t = 1$:
        \begin{itemize}
            \item[\faIcon{cogs}] \textbf{Compute output}: $y = f(0.1 \cdot 1 + 0.1 \cdot 1) = f(0.2) = 1$
            \item[\textcolor{Green3}{\faIcon{check-circle}}] Output matches target, so \textbf{no weight update needed}.
        \end{itemize}
    \end{enumerate}
    After one pass through the training data, the weights are $w_1 = 0.1$ and $w_2 = 0.1$. Repeating this process over multiple epochs will further refine the weights until the Perceptron correctly models the OR function.
\end{examplebox}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Should the bias be updated if the output doesn't match the target?}}
\end{flushleft}
In the Hebbian learning rule, the \textbf{bias term can also be updated similarly to the weights}. The bias can be treated as a weight connected to an input that is always 1. Therefore, if the output does not match the target, the bias should also be updated to help correct the output. The update rule for the bias $b$ would be:
\begin{equation}
    \Delta b = \eta \cdot x_{0} \cdot t = \eta \cdot 1 \cdot t = \eta \cdot t \qquad x_{0} = 1
\end{equation}
So, if the \textbf{output} is \textbf{incorrect}, the bias would be adjusted by adding $\Delta b$ to the current bias value:
\begin{equation}
    b^{(k+1)} = b^{(k)} + \Delta b = b^{(k)} + \eta \cdot t
\end{equation}
This adjustment helps shift the activation threshold of the Perceptron, making it more likely to produce the correct output in future iterations.

\highspace
\hl{In other words}, we can think of the \textbf{bias} as a \emph{special weight} $w_{0}$ that connects to a \emph{constant input} $x_{0} = 1$. This trick lets us treat the bias \textbf{exactly the same} as all the other weights in the update rule. Therefore, the neuron computes:
\begin{equation*}
    y = f\left(w_0 \cdot 1 + w_1 x_1 + w_2 x_2 + \dots\right)
\end{equation*}
And the update rule applies uniformly to \textbf{every} $w_i$, including $w_0$ (the bias):
\begin{equation*}
    w_{i}^{(k+1)} = w_{i}^{(k)} + \eta \cdot x_{i} \cdot t \quad \text{for }i = 0, 1, 2, \ldots
\end{equation*}

\highspace
Thus, at every iteration:
\begin{itemize}
    \item The \textbf{normal weights} $\left(w_{1}, w_{2}, \ldots\right)$ adapt based on the input features and target output.
    \item The \textbf{bias} $b$ (also considered a weight, like $w_{0}$) is updated to help the Perceptron better fit the data. It adapts based on the target output $t$ alone, since its associated input is always 1 (i.e., $x_{0} = 1$).
\end{itemize}
The bias learns to \textbf{adjust the overall tendency} of the neuron to fire. If the network often needs to output 1 (positive target), the bias weight increases, making it easier for the neuron to activate. Conversely, if the network often needs to output 0 (negative target), the bias weight decreases, making it harder for the neuron to activate. This dynamic adjustment of the bias is crucial for the Perceptron to learn effectively from data.