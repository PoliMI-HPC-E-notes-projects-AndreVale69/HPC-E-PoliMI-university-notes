\subsubsection{Mathematical Model \& Logical Operations}

The \definition{Perceptron} is the \textbf{simplest neural network}, a \hl{single neuron that transforms multiple input signals into one output through a weighted sum and a thresholding function}.

\highspace
Formally, given inputs:
\begin{equation*}
    x = \left[x_{1}, x_{2}, \ldots, x_{I}\right]
\end{equation*}
And weights:
\begin{equation*}
    w = \left[w_{1}, w_{2}, \ldots, w_{I}\right]
\end{equation*}
The perceptron computes the quantity:
\begin{equation}
    a = \sum_{i=1}^{I} w_{i} x_{i} + b = w^{T} x + b
\end{equation}
Where:
\begin{itemize}
    \item $x_{i}$ are the input features,
    \item $w_{i}$ are the learnable connection weights,
    \item $b$ is the \textbf{bias} (representing the firing threshold).
\end{itemize}
Then, this \textbf{activation} $a$ passes through a \textbf{step function} (also called \textbf{threshold} or \textbf{activation function}) to produce the final output $y$:
\begin{equation}
    y =
    \begin{cases}
        1 & \text{if } a > 0 \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
In some conventions, the output can also be $-1$ or $+1$ instead of $0$ and $1$, depending on how the data is encoded.

\highspace
Sometimes, we include the bias directly as a weight $w_{0}$ associated with a fixed input $x_{0} = 1$, rewriting the equations as:
\begin{equation}
    y = f\left(w_0 x_0 + w_1 x_1 + \ldots + w_I x_I\right) = f\left(w^T x\right)
\end{equation}
This makes formulas simpler and more uniform for training algorithms (compact vector notation).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Interpretation of the Perceptron math}}
\end{flushleft}
The perceptron divides the input space into two regions separated by a \textbf{decision boundary} (a hyperplane\footnote{%
    A \textbf{hyperplane} is a \textbf{generalization of a line or a plane} to any number of dimensions. It's the mathematical way to describe a \emph{flat surface} that separates space into two parts. In 1D, a hyperplane is just a point that splits the line into two halves; in 2D, it's a line that divides the plane into two regions, one where the perceptron outputs 1 and the other where it outputs 0; in 3D, it's a plane that separates space into two halves. In higher dimensions, it remains a flat subspace that partitions the input space.
}). If the weighted sum of inputs exceeds the threshold, the neuron ``fires'' (outputs 1); otherwise, it stays silent (outputs 0). Thus, the perceptron acts as a \textbf{linear classifier}: it determines which side of the hyperplane the input vector lies on.

\highspace
We can express the \textbf{exact set of points where the neuron is undecided} (the \definition{Decision Boundary Equation}) by setting the activation $a$ to zero:
\begin{equation}
    w^T x + b = 0 \quad \text{(decision boundary equation)}
\end{equation}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{brain} \textbf{What it can actually do: Logical Operations}}
\end{flushleft}
Now, if the perceptron is a computational unit, \emph{what kind of computations can it perform?} To answer that, we need simple, well-defined \textbf{functions} to test it on. The most basic functions are the \textbf{logical operations} used in Boolean algebra. Logical operations (like AND, OR, NOT) are perfect because:
\begin{itemize}
    \item They have \textbf{binary inputs} (0 or 1), exactly like neuron activations.
    \item They produce \textbf{binary outputs} (true or false), like the perceptron's step function.
    \item They let us see immediately whether the neuron can separate input cases correctly.
\end{itemize}
Logical operations are the \textbf{first experiments} that show the perceptron's power as a \emph{linear classifier}.

\highspace
When the perceptron can reproduce logic operations like AND or OR, it proves that:
\begin{enumerate}
    \item A single neuron can implement \textbf{decision-making}.
    \item The model is capable of \textbf{classification} (separating inputs into categories).
    \item We can assign \textbf{geometric meaning} (a hyperplane dividing true/false examples).
\end{enumerate}

\highspace
\begin{examplebox}[: Logical OR ($\lor$)]
    \begin{center}
        \begin{tabular}{@{} c c c @{}}
            \toprule
            $x_{1}$ & $x_{2}$ & $y = x_{1} \, \lor \, x_{2}$ \\
            \midrule
            0 & 0 & 0 \\
            0 & 1 & 1 \\
            1 & 0 & 1 \\
            1 & 1 & 1 \\
            \bottomrule
        \end{tabular}
    \end{center}

    We want the perceptron to output \textbf{1} if \emph{any} input is 1. A possible set of parameters is:
    \begin{equation*}
        w_{1} = 1, \quad w_{2} = 1, \quad b = -0.5
    \end{equation*}
    This gives us the activation function:
    \begin{equation*}
        a = w_{1} x_{1} + w_{2} x_{2} + b = x_{1} + x_{2} - 0.5
    \end{equation*}
    Or equivalently:
    \begin{equation*}
        y =
        \begin{cases}
            1 & \text{if } x_{1} + x_{2} - 0.5 > 0 \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    So each neuron computes:
    \begin{equation*}
        y = f\left(
            w_{1} x_{1} + w_{2} x_{2} + b
        \right) = f\left(
            1 \cdot x_{1} + 1 \cdot x_{2} - 0.5
        \right)
    \end{equation*}
    Checking all input combinations:
    \begin{itemize}
        \item For $(0, 0)$: $a = 0 + 0 - 0.5 = -0.5$ $\Rightarrow$ $y = 0$
        \item For $(0, 1)$: $a = 0 + 1 - 0.5 = 0.5$ $\Rightarrow$ $y = 1$
        \item For $(1, 0)$: $a = 1 + 0 - 0.5 = 0.5$ $\Rightarrow$ $y = 1$
        \item For $(1, 1)$: $a = 1 + 1 - 0.5 = 1.5$ $\Rightarrow$ $y = 1$
    \end{itemize}
    Thus, the perceptron correctly implements the OR function.
\end{examplebox}

\begin{examplebox}[: Logical AND ($\land$)]
    \begin{center}
        \begin{tabular}{@{} c c c @{}}
            \toprule
            $x_{1}$ & $x_{2}$ & $y = x_{1} \, \land \, x_{2}$ \\
            \midrule
            0 & 0 & 0 \\
            0 & 1 & 0 \\
            1 & 0 & 0 \\
            1 & 1 & 1 \\
            \bottomrule
        \end{tabular}
    \end{center}

    We want the perceptron to output \textbf{1} only if \emph{both} inputs are 1. A possible set of parameters is:
    \begin{equation*}
        w_{1} = 1, \quad w_{2} = 1, \quad b = -1.5
    \end{equation*}
    This gives us the activation function:
    \begin{equation*}
        a = w_{1} x_{1} + w_{2} x_{2} + b = x_{1} + x_{2} - 1.5
    \end{equation*}
    Or equivalently:
    \begin{equation*}
        y =
        \begin{cases}
            1 & \text{if } x_{1} + x_{2} - 1.5 > 0 \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    So each neuron computes:
    \begin{equation*}
        y = f\left(
            w_{1} x_{1} + w_{2} x_{2} + b
        \right) = f\left(
            1 \cdot x_{1} + 1 \cdot x_{2} - 1.5
        \right)
    \end{equation*}
    Checking all input combinations:
    \begin{itemize}
        \item For $(0, 0)$: $a = 0 + 0 - 1.5 = -1.5$ $\Rightarrow$ $y = 0$
        \item For $(0, 1)$: $a = 0 + 1 - 1.5 = -0.5$ $\Rightarrow$ $y = 0$
        \item For $(1, 0)$: $a = 1 + 0 - 1.5 = -0.5$ $\Rightarrow$ $y = 0$
        \item For $(1, 1)$: $a = 1 + 1 - 1.5 = 0.5$ $\Rightarrow$ $y = 1$
    \end{itemize}
    Thus, the perceptron correctly implements the AND function. However, we can see that other weight/bias combinations could achieve the same result. For example:
    \begin{equation*}
        w_{1} = 1.5, \quad w_{2} = 1.5, \quad b = -2.0
    \end{equation*}
\end{examplebox}

\highspace
In both examples, the perceptron defines a \textbf{line (in 2D)} that separates input combinations giving output 1 from those giving output 0. For OR, the line lies closer to the origin, since only $\left(0,0\right)$ should give 0; for AND, the line lies further away, since only $\left(1,1\right)$ should give 1. So, by adjusting weights and bias, the perceptron can learn to classify inputs according to these logical rules. However, it's clear that \textbf{manually setting weights and biases for complex tasks is impractical}. This brings us to the next important topic: \textbf{\emph{how can it learn those weights automatically instead of us setting them by hand?}}