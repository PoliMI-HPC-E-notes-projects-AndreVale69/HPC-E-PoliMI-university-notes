\subsubsection{Perceptron as Linear Classifier}

A \textbf{classifier} is a model that assigns input data points to one of several classes. In the case of the perceptron, it classifies input vectors into two classes based on a linear decision boundary.

\highspace
A \textbf{linear classifier} is a type of classifier that makes its decisions based on a linear combination of the input features. In poor words, it makes a decision by checking on which side of a \emph{line} (in 2D), \emph{plane} (in 3D), or \emph{hyperplane} (in higher dimensions) the input data point lies.

\highspace
The perceptron computes:
\begin{equation*}
    a = w^{T} x + b
\end{equation*}
where \( w \) is the weight vector, \( x \) is the input vector, and \( b \) is the bias term, and decides:
\begin{equation}\label{eq:perceptron-decision}
    y = \begin{cases}
    1 & \text{if } a > 0 \\
    0 & \text{if } a \le 0
    \end{cases}
\end{equation}
So the \textbf{decision} happens depending on the \emph{sign} of $a$: positive values lead to class 1, while zero or negative values lead to class 0.

\highspace
The \definition{Decision Boundary} is the exact set of points where the model is \textbf{undecided}, where it switches from one class to the other. That happens precisely when the condition changes sign from negative to positive. The ``border'' between those two cases is when the activation $a$ equals \textbf{zero}. Formally, this occurs when:
\begin{equation*}
    w^{T} x + b = 0
\end{equation*}
That's where the perceptron's decision flips, and therefore it's the \textbf{boundary line (or hyperplane)}. This boundary divides the input space into two halves:
\begin{itemize}
    \item Points where \( w^{T} x + b > 0 \) are classified as class 1.
    \item Points where \( w^{T} x + b < 0 \) are classified as class 0.
    \item Points where \( w^{T} x + b = 0 \) lie exactly on the decision boundary.
\end{itemize}
\textcolor{Green3}{\faIcon{question-circle} \textbf{Wait, why is zero special? In the above equation (\ref{eq:perceptron-decision}), the perceptron outputs 0 when $a = 0$. Why is it called the decision boundary?}} In theory, the \textbf{boundary}:
\begin{equation*}
    w^{T} x + b = 0
\end{equation*}
Is \textbf{not assigned to any class}, it's the \textbf{limit} between them. Exactly on the boundary ($a=0$), the model \emph{is indifferent}, because \textbf{geometrically} that point is the \textbf{separator}, not really part of any region (see Figure \ref{fig:perceptron-linear-classifier}, page \pageref{fig:perceptron-linear-classifier}, to visualize this concept). However, in practice, the $\le$ sign in the perceptron decision rule is just a \textbf{tie-breaking rule}, otherwise we wouldn't know what to output when $a=0$. But for geometry and theory, we're interested in \textbf{where the switch happens}, so we call the exact set of points the \textbf{decision boundary}.

% \newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.9\textwidth]{img/perceptron/perceptron-linear-classifier.pdf}
    \caption{A 2D example of a perceptron as a linear classifier. The line represents the decision boundary where $w^{T} x + b = 0$. Points on one side of the line are classified as class 1 (green area, orange triangles, everything that satisfies $w^{T} x + b > 0$), while points on the other side are classified as class 0 (blue, $w^{T} x + b < 0$). The arrow indicates the \textbf{normal vector} $\vec{w}$, which is perpendicular to the decision boundary and points towards the class-1 side. The normal vector $\vec{w}$ points in the direction where the perceptron output increases.}
    \label{fig:perceptron-linear-classifier}
\end{figure}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.9\textwidth]{img/perceptron/perceptron-linear-classifier-2.pdf}
    \captionsetup{singlelinecheck=off}
    \caption[]{Geometric interpretation of the bias in a perceptron. The solid black line shows the decision boundary $w^{T} x + b = 0$ for $b = -1.2$, while the dashed gray line represents the case $b=0$. The red dotted segment highlights the vertical shift of the intercept caused by the bias. The normal vector $\vec{w}$ is perpendicular to the boundary and points toward the region where the neuron output is $1$ $(w^{T} x + b > 0)$.
    
    \textcolor{Green3}{\faIcon{question-circle} \textbf{If the bias is negative, why does the boundary shift upwards?}} Imagine $w = \left[1,1\right]$. Then $w^{T} x + b = x_1 + x_2 + b$. Without bias ($b=0$), the boundary is:
    \begin{equation*}
        x_1 + x_2 = 0
    \end{equation*}
    Is the \textbf{line through the origin} at a 45-degree angle. Now, if we \textbf{add} $b = -1.2$, the boundary becomes:
    \begin{equation*}
        x_1 + x_2 - 1.2 = 0 \quad \Rightarrow \quad x_1 + x_2 = 1.2
    \end{equation*}
    This line is \textbf{shifted upwards} because for any given $x_1$, $x_2$ must be larger to satisfy the equation. Thus, a \textbf{negative bias} shifts the decision boundary \textbf{upwards}, while a \textbf{positive bias} would shift it \textbf{downwards}. In this case, for $x_2$ direction, the bias effectively \textbf{increases} the threshold that $x_2$ must reach to cross the boundary:
    \begin{equation*}
        x_2 = -x_1 + 1.2
    \end{equation*}}
    \label{fig:perceptron-linear-classifier-2}
\end{figure}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l p{23em} @{}}
        \toprule
        \textbf{Concept} & \textbf{Meaning} \\
        \midrule
        $w$ & Defines the \emph{direction} of the separating hyperplane. \\[.3em]
        $b$ & Shifts the hyperplane from the origin. \\[.3em]
        $w^{T} x + b = 0$ & Equation of the decision boundary (hyperplane). \\[.3em]
        $w^{T} x + b > 0$ & Region classified as class 1. \\[.3em]
        $w^{T} x + b < 0$ & Region classified as class 0. \\[.3em]
        $\vec{w}$ & Normal vector to the decision boundary, indicating the direction of increasing output. \\[.3em]
        \textbf{Limitation} & Can only classify linearly separable data. \\
        \bottomrule
    \end{tabular}
\end{table}