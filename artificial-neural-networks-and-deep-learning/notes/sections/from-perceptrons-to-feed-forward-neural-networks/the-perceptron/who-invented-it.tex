\subsection{The Perceptron}

\subsubsection{Who Invented It?}

Once researchers realized that the brain could be viewed as a network of simple processing units, the next natural step was to formalize this idea into an actual \textbf{computational model}, what we now call a \textbf{neural network}.

\highspace
\begin{definitionbox}[: Neural Network]
    A \definition{Neural Network} is simply a \textbf{collection of artificial neurons} (page \pageref{def:artificial-neuron}) connected by weighted links. Each neuron:
    \begin{itemize}
        \item Receives inputs,
        \item Computes a weighted sum,
        \item Applies an activation function,
        \item And produces an \hl{output that becomes the input for the next neuron}.
    \end{itemize}
    Through these connections, the network forms a structure capable of \textbf{transforming input data into meaningful outputs}, a function approximator that \emph{learns} by adjusting its weights.
\end{definitionbox}

\noindent
The very first implementations appeared in the 1940s-1960s, with three major milestones:
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{puzzle-piece}}] \textcolor{Green3}{\textbf{McCulloch \& Pitts (1943)}}. They proposed the \definition{Threshold Logic Unit (TLU)}, the first mathematical model of a neuron. Each unit:
    \begin{itemize}
        \item Received multiple binary inputs,
        \item Multiplied them by fixed weights,
        \item Summed them up,
        \item Compared the sum to a threshold,
        \item Output 1 if the threshold was exceeded, 0 otherwise.
    \end{itemize}
    They proved that a network of such units could represent \textbf{any logical function}, meaning it could, in theory, ``compute'' anything if properly wired.
    \item[\textcolor{Green3}{\faIcon{\speedIcon}}] \textcolor{Green3}{\textbf{Frank Rosenblatt (1957)}}. He built the first \textbf{trainable model}, the \definition{Perceptron}. Rosenblatt's perceptron could automatically \textbf{learn} the correct weights from examples using an update rule based on errors. His prototype was implemented in hardware:
    \begin{itemize}
        \item The weights were stored as adjustable electrical components (potentiometers),
        \item Electric motors updated them during learning. This was the first step from theoretical neuroscience to \textbf{machine learning}.
    \end{itemize}
    \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{Bernard Widrow (1960)}}. He developed the \definition{ADALINE (Adaptive Linear Neuron)} and later the \definition{MADALINE (Multiple ADALINE network)}. Widrow's key idea was to express the threshold as a \textbf{bias term}, simplifying the equations and making it easier to train models using gradient-based optimization, a cornerstone of modern networks.
\end{itemize}
Together, these models represent the \textbf{first generation of neural networks}: simple, linear systems inspired by the brain but operating with mathematics and electricity. They laid the groundwork for the more complex architectures that would follow, leading to the deep learning revolution we see today.