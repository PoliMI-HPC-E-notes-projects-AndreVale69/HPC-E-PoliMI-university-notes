\paragraph{Reinforcement Learning}

\definition{Reinforcement Learning (RL)} is like \emph{learning by trial and error}. An \textbf{agent} interacts with an \textbf{environment} by taking \textbf{actions} and receiving \textbf{rewards} or \textbf{punishments}. The goal of the agent is to learn a policy that maximizes the cumulative reward over time.

\highspace
At each step, the agent:
\begin{enumerate}
    \item \textbf{Observes a state} $s_t$ from the environment.
    \item \textbf{Selects an action} $a_t$ based on its current policy $\pi\left(a_t \, | \, s_t\right)$.
    \item \textbf{Receives a reward} $r_t$ and a \textbf{new state} $s_{t+1}$.
\end{enumerate}
The agent's goal is to learn a \textbf{policy} $\pi\left(a \, | \, s\right)$ that maximizes the expected cumulative reward. Unlike supervised learning, no teacher gives the right answer; the agent learns from the \textbf{consequences} of its actions.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is an Agent?}}
\end{flushleft}
An \important{agent} is an \emph{entity} that \textbf{makes decisions and takes actions in an environment to achieve a specific goal}. In reinforcement learning, the agent learns to optimize its behavior based on feedback from the environment.

\highspace
With \textbf{\emph{entity}}, we mean anything that can perceive its environment through sensors and act upon that environment through actuators.

\begin{examplebox}[: Robot Navigation]
    For example, consider a robot navigating a maze. The robot (agent) perceives its surroundings (state), decides to move left or right (action), and receives feedback (reward) based on whether it gets closer to the exit or hits a wall. The robot's goal is to learn a strategy (policy) that maximizes its chances of reaching the exit while avoiding obstacles.

    \highspace
    In simple terms, the robot through cameras and sensors perceives the maze (environment), decides its next move (action), and learns from the outcomes (rewards) to improve its navigation strategy (policy).

    \highspace
    In summary:
    \begin{itemize}
        \item \textbf{Agent}: The robot.
        \item \textbf{Environment}: The maze.
        \item \textbf{State}: The robot's current position in the maze.
        \item \textbf{Action}: Moving left, right, forward, or backward.
        \item \textbf{Reward}: Positive reward for reaching the exit, negative reward for hitting a wall.
        \item \textbf{Policy}: The strategy the robot uses to decide its next move based on its current state.
    \end{itemize}
\end{examplebox}

\noindent
The agent's \textbf{primary objective} is to \textbf{learn a policy that maximizes the cumulative reward} it receives over time by interacting with the environment.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Formalization of Reinforcement Learning}}
\end{flushleft}
Reinforcement learning problems are often modeled using \textbf{Markov Decision Processes (MDPs)}. An MDP is defined by:
\begin{itemize}
    \item \textbf{Task (T)}: learn a policy $\pi\left(a \, | \, s\right)$ mapping states to actions. In other words, the task is to find the best action to take in each state to maximize cumulative reward.
    \item \textbf{Experience (E)}: consists of sequences of states, actions, and rewards obtained by interacting with the environment.
    \item \textbf{Performance Measure (P)}: expected return (sum of discounted rewards):
    \begin{equation*}
        P = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t\right]
    \end{equation*}
    Where $\gamma \in \left[0, 1\right]$ is the discount factor that determines the importance of future rewards.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{cogs} \textbf{Key Concepts in Reinforcement Learning}}
\end{flushleft}
The goal of this section is to introduce the Reinforcement Learning paradigm and its key concepts. These concepts will be covered in more detail in later sections. However, here are some of those concepts:
\begin{itemize}
    \item \textbf{Exploration vs. Exploitation}: The dilemma of choosing between exploring new actions to discover their effects (\important{\emph{exploration}}) and exploiting known actions that yield high rewards (\important{\emph{exploitation}}).
    
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why a dilemma?}} Because if the agent only exploits known actions, it may miss out on potentially better actions. Conversely, if it only explores, it may not accumulate enough reward.


    \item \textbf{Reward Signal}: The feedback received from the environment after taking an action, used to evaluate the action's effectiveness. It could be sparse or dense:
    \begin{itemize}
        \item \textbf{Sparse Reward}: Rewards are infrequent, making it challenging for the agent to learn. For example, in a game, the agent might only receive a reward upon winning or losing.
        \item \textbf{Dense Reward}: Rewards are given frequently, providing more immediate feedback. For example, in a driving simulation, the agent might receive small rewards for staying on the road and penalties for going off-road.
    \end{itemize}
    
    
    \item \textbf{Delayed reward}: The reward for an action may not be immediate, making it challenging to associate actions with their long-term consequences. For example, in a chess game, a move may not yield an immediate reward but could lead to a win several moves later. The agent must learn to evaluate actions based on their long-term impact rather than immediate outcomes. This requires the agent to consider future rewards when making decisions.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{RL vs. Supervised Learning}}
\end{flushleft}
Reinforcement learning differs from supervised learning in several key ways:

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} p{1.5cm} p{4.5cm} p{4.5cm} @{}}
        \toprule
        Aspect & Supervised Learning & Reinforcement Learning \\
        \midrule
        Data & Fixed labeled dataset (in-out pairs) & No labels; agent generates data by acting \\[.3em]
        Feedback & Correct answer for each example & Rewards (possibly delayed, sparse) \\[.3em]
        Goal & Minimize error (classification/regression) & Maximize cumulative reward \\[.3em]
        Typical methods & Regression, SVM, Neural Nets & Q-learning, Policy Gradients, Actor-Critic \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Challenges of Reinforcement Learning}}
\end{flushleft}
Reinforcement learning presents several challenges:
\begin{itemize}
    \item \textbf{Exploration}: need to try enough actions to discover good strategies.
    \item \textbf{Delayed Feedback}: rewards may not be immediate, complicating reward assignment.
    \item \textbf{Sample inefficiency}: often requires millions of trials to learn effective policies.
    \item \textbf{Stability}: training can be unstable with neural nets.
\end{itemize}
Despite these challenges, RL has achieved remarkable success in various domains, including game playing, robotics, and autonomous systems.

\highspace
In summary, reinforcement learning is a powerful paradigm for training agents to \textbf{make decisions in complex environments} by \textbf{learning from the consequences} of their actions. RL is distinct from supervised learning in its approach to data, feedback, and goals, making it suitable for a wide range of applications where direct supervision is not feasible.