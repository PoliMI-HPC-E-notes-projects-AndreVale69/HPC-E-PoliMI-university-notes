\paragraph{Supervised Learning}

\definition{Supervised Learning} is like learning \emph{with a teacher}:
\begin{itemize}
    \item The algorithm is given \textbf{examples of inputs and their correct outputs (labels)}.
    \item The goal is to learn a \textbf{mapping function} that predicts the correct output for new, unseen inputs.
\end{itemize}
Formally:
\begin{itemize}
    \item Training dataset:
    \begin{equation*}
        D = \left\{\left(x_{1}, t_{1}\right), \left(x_{2}, t_{2}\right), \dots, \left(x_{N}, t_{N}\right)\right\}
    \end{equation*}
    Where $x_{i}$ are inputs and $t_{i}$ are targets.
    \item Model: $f_{\theta}(x) \approx t$.
    \item Learning: choose parameters $\theta$ that minimize a loss function measuring error.
\end{itemize}
In other words, \textbf{Supervised Learning} is a type of machine learning where the algorithm is trained on a labeled dataset, meaning each training example includes both the input data and the correct output. And the goal is to learn a function that maps inputs to outputs, in order to make predictions on new, unseen data.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Types of Supervised Learning}}
\end{flushleft}
In supervised learning we always have:
\begin{itemize}
    \item \textbf{Inputs} $x$ (features).
    \item \textbf{Outputs} $t$ (labels/targets).
    \item A \textbf{model} $f_{\theta}(x)$ that learns a mapping from inputs to outputs.
\end{itemize}
The distinction between \textbf{classification} and \textbf{regression} depends on the \textbf{nature of the output}.
\begin{itemize}
    \item \definition{Classification}: Predict a \textbf{discrete class label}. The output space is a finite set of categories. For example:
    \begin{itemize}
        \item Binary: $\left\{0, 1\right\}$, e.g. spam vs not spam.
        \item Multi-class: $\left\{1, \dots, K\right\}$, e.g. digits 0-9.
    \end{itemize}
    From a mathematical point of view:
    \begin{equation*}
        f_\theta(x) : \mathcal{X} \to \{1, 2, \dots, K\}
    \end{equation*}
    \begin{examplebox}[: Cars vs Motorcycles]
        Use the classic triplet:
        \begin{itemize}
            \item \textbf{Task (T)}: distinguish between two categories (binary classification).
            \item \textbf{Experience (E)}: dataset of images labeled ``car'' or ``motorcycle''.
            \item \textbf{Performance (P)}: accuracy (percentage of correct predictions).
        \end{itemize}
        Pipeline (how supervised learning was traditionally done before deep learning):
        \begin{itemize}
            \item \textbf{Feature Extraction (Hand-Crafted Features)}. Raw data (like an image, sound, or text) is often too complex to give directly to a simple model. Traditionally, humans designed \emph{rules} or \emph{functions} to extract \textbf{features} from raw data.
            \begin{itemize}
                \item Example (images): count edges, corners, textures, or wheel shapes.
                \item Example (text): word frequencies, presence of certain keywords.
                \item Example (audio): pitch, energy, Mel-frequency coefficients (MFCCs).
            \end{itemize}
            These features are \textbf{manually engineered} to capture the most important aspects of the problem. The output is a vector of numbers (feature vector) that represents each example. This step is about ``\emph{what information to feed into the model}''.

            In this example, hand-crafted features are:
            \begin{itemize}
                \item Extract ``number of circular shapes'' (wheels);
                \item Extract ``dominant color'';
                \item Extract ``edge orientation histograms''.
            \end{itemize}
            The photo is now a vector like $\left[2, 0.6, 0.8\right]$


            \item \textbf{Learning a Model (Classifier)}. Once we have feature vectors, we train a \textbf{machine learning model} that learns to map those features to outputs (labels or numbers). The model \textbf{learns decision boundaries} (for classification) or \textbf{functions} (for regression) that separate categories or fit numeric values. This is the \textbf{actual learning step}: the algorithm adjusts its parameters from the data.
            
            In this example, the classifier could be a Support Vector Machine (SVM) model, which learns as follows: if ``number of wheels $\approx$ 2'' then is a motorcycle; if ``number of wheels $\approx$ 4'' then is a car.
        \end{itemize}
    \end{examplebox}


    \item \definition{Regression}: Predict a \textbf{continuous value}. The output space is the set of real numbers ($\mathbb{R}$). From a mathematical point of view:
    \begin{equation*}
        f_\theta(x) : \mathcal{X} \to \mathbb{R}
    \end{equation*}
    \begin{examplebox}[: Price Prediction]
        Use the classic triplet:
        \begin{itemize}
            \item \textbf{Task (T)}: predict a \textbf{continuous value} instead of a discrete label.
            \item \textbf{Experience (E)}: dataset of houses (features: size, location, rooms) with their selling prices.
            \item \textbf{Performance (P)}: Mean Squared Error (MSE), Mean Absolute Error (MAE), or $R^{2}$ score.
        \end{itemize}
        Pipeline:
        \begin{itemize}
            \item \textbf{Hand-crafted features}: e.g., number of rooms, square meters, distance to city center.
            \item \textbf{Learned regressor}: a model that predicts a continuous output.
        \end{itemize}
    \end{examplebox}
\end{itemize}
In simple terms, if our labels are:
\begin{itemize}
    \item Categories, it's \textbf{classification}.
    \item Numbers, it's \textbf{regression}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why Deep Learning Changed This}}
\end{flushleft}
In \textbf{deep learning}, feature extraction and learning are \textbf{not separated anymore}. Neural networks \textbf{learn features automatically from raw data} (pixels, sound waves, text). So the pipeline becomes \textbf{one end-to-end step}: input raw data $\to$ neural network $\to$ prediction.

\highspace
\appliedStatisticsRef{Supervised Learning}