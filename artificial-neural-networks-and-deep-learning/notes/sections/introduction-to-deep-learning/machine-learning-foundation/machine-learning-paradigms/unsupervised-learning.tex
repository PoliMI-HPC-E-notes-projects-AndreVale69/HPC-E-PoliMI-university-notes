\paragraph{Unsupervised Learning}

\definition{Unsupervised Learning} is like learning \emph{without a teacher}:
\begin{itemize}
    \item We only provide the algorithm with \textbf{inputs} $x_{1}, x_{2}, \dots, x_{N}$.
    \item There are \textbf{no labels/targets} telling the algorithm the ``correct answer''.
    \item The goal is to \textbf{discover hidden structures} or \textbf{representations} in the data.
\end{itemize}
Formally:
\begin{itemize}
    \item Dataset:
    \begin{equation*}
        D = \left\{x_{1}, x_{2}, \dots, x_{N}\right\}, \quad x_{i} \in \mathbb{R}^{d}
    \end{equation*}
    \item Task: find structure in $D$, e.g., groups, manifolds, lower-dimension embeddings.
    \item Performance measure: less obvious (since no labels). It can be internal measures (compact clusters, variance explained) or extrinsic measures (utility in downstream tasks).
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{The most intuitive unsupervised task: Clustering}}
\end{flushleft}
In supervised learning, we had ``car vs motorcycle'', categories are known. In unsupervised, no labels are given. The simplest question becomes: ``\emph{can we group the data into natural categories, even if we don't know their names?}''. That's exactly what clustering does. \definition{Clustering} is the process of grouping data points into \textbf{clusters} such that:
\begin{itemize}
    \item Points in the same cluster are \textbf{similar} to each other.
    \item Points in different clusters are \textbf{dissimilar}.
\end{itemize}
Clustering uses a \textbf{similarity measure}, such as Euclidean distance. The algorithm groups data into clusters that minimize within-cluster distance and maximize between-cluster distance. Some common algorithms include:
\begin{itemize}
    \item \textbf{Hierarchical Clustering}. Build a tree of clusters by progressively merging or splitting. Exists two approach: Agglomerative Clustering (Bottom-Up) or Divisive Clustering (Top-Down).
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.71\textwidth]{img/hierarchical-clustering/hierarchical-clustering.pdf}
        \includegraphics[width=.71\textwidth]{img/hierarchical-clustering/hierarchical-clustering-2.pdf}
        \includegraphics[width=.71\textwidth]{img/hierarchical-clustering/hierarchical-clustering-3.pdf}
        \caption{Agglomerative Clustering (top plot), Dendogram (mid plot) and Dendogram with cut (bottom plot).}
        \label{fig: hierarchical clustering}
    \end{figure}

    \newpage

    \noindent
    About Figure \ref{fig: hierarchical clustering}, page \pageref{fig: hierarchical clustering}. In the Agglomerative Clustering result, each dot is a \textbf{data point} (here we generated 50 synthetic points). The algorithm grouped them into \textbf{3 clusters}. We can see points within each cluster are \textbf{close together} in space. Also, the clusters are \textbf{well separated}, this is why hierarchical clustering works well here. The Dendogram shows the \textbf{hierarchical merging process}:
    \begin{itemize}
        \item At the \textbf{bottom}, each point starts as its own cluster.
        \item Going \textbf{upwards}, clusters that are close together are merged.
        \item The \textbf{height of each merge} (y-axis $=$ distance) indicates how far apart the clusters were when merged.
        \item At the \textbf{top}, all points are eventually merged into a single cluster.
    \end{itemize}
    In the last figure, we ``cut'' the dendogram horizontally at a certain height (distance threshold), and we obtain a chosen number of clusters (here, 3). Everything \textbf{below the line} remains as separate clusters. Everything \textbf{above the line} (higher merges) is ignored. In the Dendogram, cutting at $\approx 15$ gives \textbf{3 vertical ``branches'' crossing the red line}. Each branch corresponds to one cluster. These branches include \textbf{all 3 groups of points}.
    

    \item \textbf{K-Means}. Choose $k$ clusters; assign points to the nearest cluster centroid; and update centroids until convergence.
    \begin{examplebox}[: K-Means, taken from the Applied Statistics course]
        Below is a simple run of the K-means algorithm on a random dataset.
        \begin{itemize}
            \item Iteration 0 - \textbf{Initialization}
            \begin{center}
                \includegraphics[width=.8\textwidth]{img/k-means/iter_0.pdf}
            \end{center}
            This is the starting point of the K-Means algorithm. \textbf{Three centroids are randomly placed in the feature space}. At this point, no data points are assigned to clusters yet, or all are assumed to be uncolored/unclustered. The positions of the centroids will strongly influence how the algorithm proceeds.

            The goal here is to start with some guesses. The next step will use these centroids to form the initial clusters.


            \item Iteration 1 - \textbf{First Assignment and Update}
            \begin{center}
                \includegraphics[width=.7\textwidth]{img/k-means/iter_1.pdf}
            \end{center}
            Each data point is assigned to the closest centroid, forming the first version of the clusters. New centroids are computed by taking the average of the points in each cluster. We can already see structure forming in the data, as points begin grouping around centroids.

            This step is the first real clustering, and centroids begin to move toward dense regions of data.


            \item Iteration 2 - \textbf{Re-Assignment and Refinement}
            \begin{center}
                \includegraphics[width=.7\textwidth]{img/k-means/iter_2.pdf}
            \end{center}
            Clusters are recomputed based on updated centroids. Many points remain in the same clusters, but some may shift to a new cluster if a centroid has moved. Centroids continue moving closer to the center of their respective groups.

            The algorithm is now refining the clusters and reducing the total distance from points to centroids.


            \item Iteration 3 - \textbf{Further Convergence}
            \begin{center}
                \includegraphics[width=.7\textwidth]{img/k-means/iter_3.pdf}
            \end{center}
            At iteration 3, the K-Means algorithm reached convergence. The centroids no longer moved, and no points changed cluster.This means:
            \begin{itemize}
                \item The algorithm has found a locally optimal solution.
                \item Further iterations would not improve or change the clustering.
                \item The final configuration is considered the result of the algorithm.
            \end{itemize}
            In practice, this is how K-Means stops: it checks whether the centroids remain unchanged, and if so, it terminates automatically.
        \end{itemize}
    \end{examplebox}
\end{itemize}

\highspace
\appliedStatisticsRef{Unsupervised Learning and Clustering}