\subsection{Modern Pattern Recognition (Pre-DL)}

Before the rise of deep learning, modern pattern recognition techniques were primarily based on traditional machine learning algorithms and statistical methods. These techniques focused on feature extraction, dimensionality reduction, and classification using various algorithms.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Speech Recognition (early 1990s-2011)}}
\end{flushleft}
Speech recognition systems used a \textbf{multi-stage pipeline} approach, which included:
\begin{itemize}
    \item \textbf{Low-level features}: extracted from the raw audio waveform, such as MFCCs (Mel-Frequency Cepstral Coefficients), a compact representation of the spectral properties of the audio signal.

    \item \textbf{Mid-level features}: built by grouping/encoding low-level features over short time windows, capturing temporal dynamics. For example Mixture of Gaussians (MoG) used to model acoustic units (phonemes).
    
    \item \textbf{Classifier (high-level features)}: used to map mid-level features to words or phrases. Common classifiers included Hidden Markov Models (HMMs) combined with Gaussian Mixture Models (GMMs) to decode sequences of acoustic units into words.
\end{itemize}
This pipeline worked decently but was very \textbf{hand-crafted} and success depended heavily on the quality of feature engineering.


\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Object Recognition (2006-2012)}}
\end{flushleft}
Computer vision systems followed a similar multi-stage pipeline approach:
\begin{itemize}
    \item \textbf{Low-level features}: detect edges, corners, gradients using methods like SIFT (Scale-Invariant Feature Transform) or HOG (Histogram of Oriented Gradients).
    \item \textbf{Mid-level features}: combine low-level descriptors into higher-level ``visual words''. For example, clustering with k-means to create a codebook of visual words, and Sparse Coding to represent images as sparse combinations of these words.
    \item \textbf{Classifier (high-level features)}: train SVMs (Support Vector Machines) or Random Forests to classify images based on mid-level features.
\end{itemize}
Again, this approach was heavily reliant on hand-crafted features and required significant domain expertise to design effective features. However, before 2012, these methods were the state-of-the-art in many computer vision tasks.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{General Pipeline (Pre-DL Pattern Recognition)}}
\end{flushleft}
The general pattern recognition pipeline before deep learning can be summarized as follows:
\begin{enumerate}
    \item \textbf{Low-level features}: raw signal transformation (e.g., edges, frequencies).
    \item \textbf{Mid-level features}: encode or cluster low-level descriptors (e.g., visual words, acoustic units).
    \item \textbf{Classifier (high-level features)}: learns categories from hand-designed representations.
\end{enumerate}

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Limitations}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Domain expertise required}: Designing MFCCs, SIFT, HOG, etc. required significant knowledge of the specific domain (speech, vision).
    \item \textbf{task specific}: features built for one task often did not generalize well to others (e.g., MFCCs don't work well for images).
    \item \textbf{Brittleness}: sensitive to noise, illumination, scaling, speaker accents, etc. 
    \item \textbf{Limited expressiveness}: as dataset grew, hand-crafted pipelines saturated in accuracy.
\end{itemize}
Before deep learning, pattern recognition was a \textbf{multi-stage pipeline} heavily \textbf{reliant on hand-crafted features and domain expertise}. While effective for its time, it had significant limitations in scalability, generalization, and robustness that deep learning would later address.