\subsection{Summary}

Everything we've seen, supervised, unsupervised, or reinforcement learning, ultimately depends on \textbf{how we represent data}. In traditional ML, features are \emph{hand-crafted}. In Deep Learning, features are \emph{learned automatically} through hierarchical representations. The revolution of Deep Learning wasn't new math, it was learning \textbf{what matters} in the data instead of coding it by hand.
\begin{equation*}
    \text{Success of ML} \Rightarrow \text{Success of its feature representation}
\end{equation*}
Deep networks just made the \textbf{representation learning} automatic and scalable.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Deep Learning $=$ Learning Data Representation from Data}}
\end{flushleft}
Deep Learning is not a specific architecture (like CNN, RNN, or Transformers) or algorithm. It's the \textbf{paradigm} where:
\begin{enumerate}
    \item Input $\to$ raw data (e.g., pixels, text, audio)
    \item Model $\to$ multiple non-linear layers learning internal representations.
    \item Output $\to$ desired prediction/task.
    \item Learning $\to$ end-to-end optimization of all layers together.
\end{enumerate}
So instead of:
\begin{equation*}
    \text{Human designs features} \to \text{Model learns mapping}
\end{equation*}
We now have:
\begin{equation*}
    \text{Model learns both features and mapping} \to \text{directly from data}
\end{equation*}
This is the essence of Deep Learning: \textbf{learning hierarchical representations directly from raw data}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{brain} \textbf{``Which data?'' - The key question of the course}}
\end{flushleft}
This is the \textbf{transition line} to the rest of the course (notes). Now that we know \emph{what} Deep Learning is, the next question is \emph{what data we use and how}. Different data types define the upcoming sections:

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l p{18em} @{}}
        \toprule
        Data Type & Upcoming Section \\
        \midrule
        Tabular / numerical             & Perceptrons \& Feed-Forward NNs \\[.3em]
        Images                          & Convolutional Neural Networks (CNNs) \\[.3em]
        Sequential (text, time series)  & Recurrent Neural Networks (RNNs) \& Transformers \\
        Unlabeled data                  & Autoencoders \& Word Embeddings \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
So this question of ``which data?'' becomes the \textbf{roadmap} for the rest of the course (notes).