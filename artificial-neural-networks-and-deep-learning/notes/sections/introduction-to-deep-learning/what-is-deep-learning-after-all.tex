\subsection{What is Deep Learning after all?}

After showing the historical context, what Machine Learning is, the three\break paradigms and how pre-DL pattern recognition worked, we can finally answer the question:
\begin{center}
    \textbf{Now that we know what ML does, what makes Deep Learning \emph{different} from classic ML?}
\end{center}

\noindent
We will take our time answering this question. First, we need to understand the meanings of ``features'' and ``classifiers''.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What are ``features''?}}
\end{flushleft}
Features are \textbf{numerical representations of the raw data} that capture something meaningful for the task.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} c p{11.5em} p{10.4em} @{}}
        \toprule
        Type of Data & Raw data example & Example of features \\
        \midrule
        Images  & Pixels (RGB values)               & Edges, corners, textures         \\[.3em]
        Audio   & Waveform (amplitude over time)    & Pitch, frequency spectrum, MFCCs \\[.3em]
        Text    & Words or sentences                & Word counts, syntactic structure \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
In \textbf{classical ML}, these features were \textbf{manually designed} by humans; engineers decided \emph{what} was important and \emph{how to compute it}. For example:
\begin{equation*}
    \text{Input image} \rightarrow \text{extract edges manually} \rightarrow \text{feed into SVM classifier}
\end{equation*}
So we had:
\begin{equation*}
    \text{Handcrafted Features} \rightarrow \text{Learned Classifier}
\end{equation*}
Where ``handcrafted'' means ``coded by humans''. So, before Deep Learning, the \textbf{feature extraction} and the \textbf{classifier} were two separate stages in the pipeline, and humans designed the first stage. This approach worked, but only if the human correctly guessed \emph{what features matter} for the task.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What does ``Learned Features'' mean?}}
\end{flushleft}
Deep Learning says: ``\emph{\underline{Stop} handcrafting features; let the machine learn them automatically, layer by layer, together with the final classifier}''.

\highspace
In \textbf{Deep Learning}, the model itself learns how to transform raw data into useful internal representations. Each layer of a neural network acts as a \textbf{feature extractor} that learns automatically \emph{what patterns matter}:
\begin{itemize}
    \item First layers: detect edges, colors, or simple shapes.
    \item Intermediate layers: detect object parts (e.g., eyes, wheels, leaves).
    \item Deep layers: detect abstract categories (e.g., ``face'', ``car'', ``flower'').
\end{itemize}
So instead of telling the machine \emph{what to look for}, we let it \textbf{discover patterns directly from data}. This is the ``\textbf{learned features}'' part.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What does ``Learned Classifier'' mean?}}
\end{flushleft}
After features are extracted (automatically or manually), the model still needs to \textbf{make a decision}: classify, predict, or generate.
\begin{itemize}
    \item In traditional ML, this is the final \textbf{classifier} stage (e.g., SVM, logistic regression, random forest).
    \item In Deep Learning, the \textbf{last few layers} of the network act as that classifier, they map high-level learned features to output labels.
\end{itemize}
So, both parts, the \emph{feature extractor} and the \emph{decision function}, are \textbf{learned jointly} through backpropagation.
\begin{equation*}
    \text{Raw Data} \xrightarrow[\text{learned weights}]{\text{Feature Extractor}} \text{Representations} \xrightarrow[\text{learned weights}]{\text{Classifier}} \text{Predictions}
\end{equation*}
So DL uses a single model to learn both \textbf{features} and \textbf{classifier} together: Learned Features $+$ Learned Classifier. The model not only learns \emph{how to decide} but also \emph{how to see the world}, both are learned from data.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{So, ``What is Deep Learning after all?''}}
\end{flushleft}
Deep Learning is \textbf{not just a new algorithm}, it's a new way of \emph{approaching representation learning}. If we had to answer in one line: \hl{``Deep Learning is the automatic learning of hierarchical data representations and decision functions directly from raw data''}. That's why it's so powerful: it \emph{adapts} to the data and the task, without relying on human intuition about features.

\highspace
\begin{deepeningbox}[: Why Not Everything Is Deep Learning]
    Deep Learning is \emph{powerful}, but it's not a silver bullet, it's not \emph{free}. It's the best tool \textbf{when} we have: large amounts of diverse data, high compute, a task based on perception or pattern recognition. Otherwise, \textbf{traditional ML or statistical models} can be simpler, faster, and just as effective.
    \begin{itemize}
        \item \important{Deep Learning needs a \emph{lot of data}.} Deep models have \textbf{millions (sometimes billions)} of parameters. They only generalize well when trained on \textbf{massive labeled datasets} (e.g., ImageNet: 14M images). If we have small data, like 300 samples from an industrial machine, a deep model will likely \textbf{overfit} and perform worse than simpler methods. In other words, Deep Learning shines when there is \textbf{data abundance}, but struggles in \textbf{data scarcity}.

        \item \important{Deep Learning needs a \emph{lot of computation}.} Training is computationally heavy, requiring specialized hardware: GPUs, TPUs, clusters, or cloud computing. Classic ML (SVMs, Decision Trees, Random Forests) can run on a laptop. Deep nets require weeks of GPU training, hyperparameter tuning, and energy cost. So, if the task doesn't justify the cost, simpler ML is more efficient.

        \item \important{Deep models are \emph{black boxes}.} We can rarely explain \emph{why} a deep network made a decision. For critical systems (healthcare, law, finance, safety) we need \textbf{interpretability} and \textbf{traceability}. Simpler models like linear regression or decision trees are \textbf{transparent}, easy to justify in front of regulators or domain experts. For example, a hospital won't risk a deep net saying ``tumor'' without being able to explain which features caused that prediction.

        \item \important{Deep models are \emph{hard to train and tune}.} Choosing architecture (layers, neurons, learning rate, dropout, etc.) is an art. Training can \textbf{diverge} or \textbf{get stuck} (vanishing gradients, overfitting, exploding losses). We often need extensive experimentation and deep knowledge of optimization tricks. So, not every team or project can afford the expertise and trial cycles DL requires.

        \item \important{Deep Learning doesn't always fit the problem.} Some tasks simply:
        \begin{enumerate}
            \item Have \textbf{structured or tabular data} (e.g., bank records, tabular logs). Here, traditional ML (XGBoost, Random Forests) often outperforms DL.
            \item Require \textbf{symbolic reasoning} or \textbf{logic}, not pattern recognition. Here, DL struggles to capture rules and relationships that classical AI or rule-based systems handle better.
            \item Need \textbf{causal inference}, not just correlations. DL finds patterns but doesn't understand cause-effect relationships, which are crucial in many scientific and policy domains. Let's think about a real-world example: predicting disease spread based on interventions (lockdowns, vaccinations) requires understanding causality, not just correlations in data (not just ``if X happens, Y follows'', but ``if we do X, Y will change'').
        \end{enumerate}

        \item \important{Deep Learning needs \emph{good data}.} DL is extremely sensitive to: label noise (wrong annotations ruin learning); biases in the dataset (can reproduce or amplify them); distribution shifts (fails badly if test data differ from training). Traditional methods often handle noise and small variations more robustly. So, ``Garbage in $\to$ garbage out'' is even more true with DL.
        
        \item \important{Deep Learning doesn't mean \emph{understanding}.} DL recognizes \textbf{patterns}, not meaning. It can detect a cat, but it doesn't \emph{know} what a cat is. It can predict outcomes, but not always \emph{why} they happen. That's why current research explores \textbf{hybrid systems} combining DL with: symbolic reasoning (neuro-symbolic AI), knowledge graphs, logic and interpretability layers.
    \end{itemize}
\end{deepeningbox}

\newpage

\begin{deepeningbox}[: ChatGPT, LLaMA \& Modern AI Models - What Are They?]
    ChatGPT, LLaMA, Gemini, Claude, etc. are all based on a specific kind of \textbf{Deep Neural Network} called a \textbf{Transformer}, introduced in 2017 by Vaswani et al. (``Attention is All You Need''). So, fundamentally:
    \begin{equation*}
        \text{ChatGPT, LLaMA, Gemini, etc.} \in \text{Deep Learning}
    \end{equation*}
    They are not ``beyond'' DL, they are its \textbf{current frontier}.

    \highspace
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What kind of Deep Learning model?}} They belong to the family of \textbf{Large Language Models (LLMs)}.
    \begin{itemize}
        \item \textbf{Architecture}: Transformer (a type of deep neural network specialized for sequences and attention).
        \item \textbf{Learning paradigm}: mainly \emph{self-supervised learning}, a subform of unsupervised learning.
        \item \textbf{Objective}: predict the next word (token) given the previous ones.
    \end{itemize}
    Mathematically:
    \begin{equation*}
        P\left(w_t \, | \, w_1, w_2, \dots, w_{t-1}\right)
    \end{equation*}
    ``Given this context, what's the next most probable word?''. That's the only thing it learns. Everything else (reasoning, style, facts) \emph{emerges} from learning this next-token distribution on vast text corpora.

    \highspace
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why are they still called ``Deep Learning''?}} They perfectly fit the definition we discussed earlier: \hl{``Deep Learning is the learning data representation and decision functions directly from data''}.
    \begin{itemize}
        \item They learn \textbf{representations} of words, sentences, and even concepts automatically.
        \item They have \textbf{layers upon layers} (up to $100+$ in GPT-4).
        \item They \textbf{don't rely on hand-crafted linguistic features} (no human tells them grammar rules).
        \item They learn everything \textbf{directly from raw text data} (syntax, semantics, even reasoning patterns).
    \end{itemize}
    So they exemplify:
    \begin{gather*}
        \text{Learned Features (embeddings)} \, + \\
        \text{Learned Classifier (next word predictor)}
    \end{gather*}
    But at \textbf{massive scale}, with \textbf{billions of parameters} and trained on \textbf{terabytes of text}. This scale is what enables their surprising capabilities.

    \highspace
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What makes them \emph{different} from earlier Deep Learning.}} Traditional DL (e.g., CNNs, RNNs) had strong \textbf{task specialization}: CNNs for vision, RNNs for sequences, LSTMs for time series. Instead, Transformers with LLMs changed the game because they are \textbf{general-purpose learners}:
    \begin{itemize}
        \item They can handle language, code, images, audio, even multimodal data.
        \item Their \textbf{attention mechanism} learns relationships between all parts of the input simultaneously.
    \end{itemize}
    They are sometimes called: ``Foundation Models'', because they can be \emph{fine-tuned} for many downstream tasks (translation, summarization, question answering, etc.).

    \highspace
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why do they feel intelligent?}} When we train on \emph{massive data} (trillions of words) and \emph{huge models} (hundreds of billions of parameters), the model starts showing \textbf{emergent behaviors}:
    \begin{itemize}
        \item Understanding context, humor, and nuance.
        \item Performing reasoning and arithmetic.
        \item Generating coherent, creative text.
        \item Translating languages fluently.
        \item Writing code snippets.
    \end{itemize}
    But still, it's pattern prediction. There is \textbf{no explicit symbolic reasoning} or understanding; it's just learned statistical structure at enormous scale. So we say: ``They are \textbf{Deep Learning models}, trained on \textbf{massive dataset}, showing \textbf{emergent intelligence}''.
\end{deepeningbox}