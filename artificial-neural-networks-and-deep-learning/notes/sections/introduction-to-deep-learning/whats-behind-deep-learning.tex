\subsection{What's Behind Deep Learning?}\label{sec:whats-behind-deep-learning}

If the concept of neural networks exists since the 1950s, \textbf{\emph{why did Deep Learning explode only after 2012?}} This is a natural question that comes \emph{after} we've seen what Deep Learning is. To answer this question, we show two perspectives: the \textbf{MIT view} and \textbf{The Economist view}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{lightbulb} \textbf{The MIT view: Computational Power}}\label{mit-view}
\end{flushleft}
According to MIT and many early researchers, Deep Learning became possible only when \textbf{computational resources} caught up with the theory. It means that the mathematics and algorithms (backpropagation, perceptrons, convolutional nets) existed for decades, but \textbf{training deep networks} requires enormous computation:
\begin{itemize}
    \item Millions of matrix multiplications.
    \item Thousands of gradient updates per sample.
    \item Gigantic datasets.
\end{itemize}
Before 2010, this was impractical. Around 2011-2012, \textbf{GPUs} (Graphics Processing Units) changed everything:
\begin{itemize}
    \item They made large-scale matrix computations thousands of times faster.
    \item Deep learning frameworks (Theano, TensorFlow, PyTorch) made GPU computing accessible.
    \item Hardware parallelism allowed training networks with \textbf{hundreds of layers} instead of 3-4.
\end{itemize}
So from the MIT perspective: Deep Learning rose because \textbf{we finally had the computational power to train deep models}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{lightbulb} \textbf{The Economist view: Big Data}}\label{economist-view}
\end{flushleft}
In 2012, \emph{The Economist} (yes, the famous magazine) proposed a different, and equally valid, explanation: ``Deep Learning exploded because the world finally generated \textbf{enough data} to feed it''. It means that the Internet, social media, smartphones, sensors, and cloud storage created \textbf{massive labeled datasets}:
\begin{itemize}
    \item ImageNet (over 14 million labeled images).
    \item YouTube (millions of labeled videos).
    \item Text from web, Wikipedia, books, perfect for LLM pretraining.
\end{itemize}
Deep neural networks thrive on data volume: they don't generalize well with few examples. The more data, the better they learn \textbf{hierarchical representations}. So from the Economist perspective: ``Deep Learning rose because \textbf{we finally had Big Data}, the fuel it needs to work''.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{lightbulb} \textbf{The Real View: Both Matter}}\label{real-view}
\end{flushleft}
In reality, both perspectives are correct and complementary. Deep Learning's success is due to the \textbf{synergy of computational power and big data}:
\begin{itemize}
    \item Before 2010, algorithms existed but computing was too slow and data too scarce. Then neural networks were limited to shallow architectures and small datasets.
    \item Around 2012, hardware (GPUs, TPUs, distributed training) made computation feasible. Simultaneously, the explosion of digital data provided the massive labeled datasets needed.
\end{itemize}
This combination triggered the \textbf{Deep Learning revolution}. The turning point was \textbf{ImageNet 2012}, where Krizhevsky, Sutskever, and Hinton demonstrated that a deep convolutional network (AlexNet) could drastically outperform traditional methods on image classification. This success was possible only because:
\begin{itemize}
    \item They used two NVIDIA GPUs to train a deep network with millions of parameters.
    \item They trained on the large ImageNet dataset with 1.2M labeled images.
\end{itemize}
The result was an error rate of 15\%, compared to 26\% for the best traditional method. This landmark event showcased the \textbf{power of deep learning when both computational resources and big data are available}.