\subsubsection{K-Fold Cross-Validation}

\definition{K-Fold Cross-Validation} divides the available dataset into $K$ equally (or nearly equally) sized subsets, called \emph{folds}. The model is trained and validated $K$ times, each time using a different fold as the \textbf{validation set}, and the remaining $K-1$ folds as the \textbf{training set}. After completing all $K$ rounds, the $K$ validation errors are \textbf{averaged} to estimate the model's overall generalization performance.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
\begin{enumerate}
    \item Given a dataset with $N$ samples:
    \begin{equation*}
        \mathcal{D} = \left\{
            \left(x_1, t_1\right),
            \left(x_2, t_2\right),
            \ldots,
            \left(x_N, t_N\right)
        \right\}
    \end{equation*}
    \item Split the dataset $\mathcal{D}$ into $K$ \emph{folds} (disjoint subsets):
    \begin{equation*}
        \mathcal{D} = \mathcal{D}_1 \cup \mathcal{D}_2 \cup \ldots \cup \mathcal{D}_K
    \end{equation*}
    Where each fold $\mathcal{D}_k$ contains approximately $\dfrac{N}{K}$ samples:
    \begin{equation*}
        \left| \mathcal{D}_k \right| \approx \dfrac{N}{K}, \quad \text{for } k = 1, 2, \ldots, K
    \end{equation*}
    \item For each fold $k = 1, 2, \ldots, K$:
    \begin{itemize}
        \item Train the model on the other $K-1$ folds (i.e., all folds except the $k$-th one, mathematically $\mathcal{D} \setminus \mathcal{D}_k$):
        \begin{equation*}
            \mathcal{D}_{\text{train}}^{(k)} = \bigcup_{\substack{j=1 \\ j \neq k}}^{K} \mathcal{D}_j
        \end{equation*}
        \item Validate (test) the model on the $k$-th fold:
        \begin{equation*}
            \mathcal{D}_{\text{val}}^{(k)} = \mathcal{D}_k
        \end{equation*}
        \item Compute the validation error $\hat{e}_{k}$ on the validation set $\mathcal{D}_{\text{val}}^{(k)}$.
    \end{itemize}
    \item Collect all validation errors $E_{k}$ from each iteration, and compute the \textbf{average error}:
    \begin{equation*}
        \hat{E}_{\text{K-Fold}} = \dfrac{1}{K} \cdot \sum_{k=1}^{K} \hat{e}_{k}
    \end{equation*}
    This average error $\hat{E}_{\text{K-Fold}}$ provides an estimate of the model's generalization error.
\end{enumerate}
Differently from LOOCV, K-Fold Cross-Validation allows us to choose a smaller $K$:
\begin{itemize}
    \item \important{5-Fold Cross-Validation} is the most commonly used value, balancing bias and variance in the error estimate.
    \item \important{10-Fold Cross-Validation} is also popular, especially in scenarios where more data is available, providing a slightly lower bias at the cost of increased computational time.
    \item \important{Stratified K($K = N$)-Fold Cross-Validation} equivalent to LOOCV, where each fold contains exactly one sample. Unbiased but computationally expensive.
    \item \important{2 or 3-Fold Cross-Validation} can be used for very large datasets where computational efficiency is a concern, but may lead to higher variance in the error estimate.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Limitations}} \textbf{and} \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\textbf{Compared to Hold-Out}, it uses the entire dataset more efficiently because each sample is used for validation once and for training $K-1$ times (lower bias). Also, \textbf{compared to LOOCV}, it is much cheaper computationally (only $K$ trainings instead of $N$).
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Efficient data usage}}: all samples contribute to both training and validation.
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Reduced bias}}: the averaged approximates the expected generalization error better than a single split.
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Stability}}: more reliable than hold-out because the specific data division matter less.
    \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Computational cost}}: the model is trained $K$ times, still heavier than a single hold-out validation. For deep neural networks, this can be impractical.
    \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Data leakage risk}}: all processing (normalization, scaling, etc.) must be \textbf{recomputed inside each fold}, otherwise information from the validation folds can leak into the training folds.
    \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Variance in small datasets}}: if $K$ is too small, each fold may not represent the full data distribution well. 
\end{itemize}