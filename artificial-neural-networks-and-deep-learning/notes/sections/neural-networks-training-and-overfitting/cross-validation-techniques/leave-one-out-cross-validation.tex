\subsubsection{Leave-One-Out Cross-Validation (LOOCV)}

\definition{Leave-One-Out Cross-Validation (LOOCV)} is a special case of \emph{K-Fold Cross-Validation} (next section) where the number of folds $K$ is equal to the number of samples $N$ in the dataset. It means that each data point acts \textbf{once} as validation data, and $N-1$ times as part of the training set.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
\begin{enumerate}
    \item Given a dataset with $N$ samples:
    \begin{equation*}
        \mathcal{D} = \left\{
            \left(x_1, t_1\right),
            \left(x_2, t_2\right),
            \ldots,
            \left(x_N, t_N\right)
        \right\}
    \end{equation*}
    \item For each sample $i = 1, 2, \ldots, N$:
    \begin{itemize}
        \item Train the model on all samples except the $i$-th one:
        \begin{equation*}
            \mathcal{D}_{\text{train}}^{(i)} = \mathcal{D} \setminus \left\{ \left(x_i, t_i\right) \right\}
        \end{equation*}
        \item Validate (test) the model on the $i$-th sample:
        \begin{equation*}
            \mathcal{D}_{\text{val}}^{(i)} = \left\{ \left(x_i, t_i\right) \right\}
        \end{equation*}
    \end{itemize}
    \item Collect all validation errors $E_{i}$ from each iteration, and compute the \textbf{average error}:
    \begin{equation*}
        \hat{E}_{\text{LOOCV}} = \dfrac{1}{N} \cdot \sum_{i=1}^{N} E_{i}
    \end{equation*}
    This average error $\hat{E}_{\text{LOOCV}}$ gives an \textbf{almost unbiased estimate} of the model's generalization error.
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Risks and Limitations}}
\end{flushleft}
While LOOCV maximizes data usage for training and provides a nearly unbiased error estimate, it has some drawbacks:
\begin{enumerate}
    \item \important{Computational Cost}. We must train the model $N$ times (once for each sample), which can be \textbf{prohibitively expensive} for large datasets or deep neural networks.
    \item \important{High variance in models}. Each training set differs by only one sample, so the models can be \textbf{very similar}. This can lead to high variance in the validation errors, making the average error estimate less stable.
    \item \important{Not practical for deep learning}. Neural networks often require thousands of training iterations to converge, making LOOCV impractical for large-scale problems.
\end{enumerate}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When to use LOOCV?}}
\end{flushleft}
LOOCV has an \textbf{unbiased estimate} (each data point serves as validation once, so every sample influences the estimate equally) and an \textbf{efficient use of data} (almost all samples are used for training in each iteration, ideal when data are scarce). Thanks to these properties, LOOCV is particularly useful when:
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check}}] When $N$ is \textbf{small} (e.g., a few hundred samples or less).
    \item[\textcolor{Green3}{\faIcon{check}}] When we want an \textbf{almost unbiased} generalization error estimate.
    \item[\textcolor{Green3}{\faIcon{check}}] When computation time is \textbf{not a concern} (e.g., simple models).
\end{itemize}
LOOCV approximates the \emph{true expected error} $E_{\text{test}}$ and it's nearly unbiased because each sample plays both roles, training and validation, but it tends to have \textbf{high variance} because each training set is almost identical, leading to similar models. However, for modern deep learning, it's mostly of \textbf{theoretical interest}, a conceptual benchmark rather than a practical tool.