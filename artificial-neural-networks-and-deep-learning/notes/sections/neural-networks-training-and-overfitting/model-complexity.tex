\subsection{Model Complexity}

Machine learning, and neural networks in particular, are based on an \textbf{inductive assumption}: \hl{``if a model performs well on a \textbf{large and representative set of training examples}, then it will also perform well on \textbf{unseen examples} drawn from the same distribution''}. This is called \definition{Inductive Hypothesis} (or \definition{Inductive Bias}) because it assumes that the patterns learned from the training data will generalize to new data.

\highspace
In other words, we \textbf{trust that patterns learned from the training data generalize} to future data, as long as the data are \textbf{independent and identically distributed (i.i.d.)}, and the model captures the \textbf{true underlying structure} rather than random noise.

\highspace
Formally, if:
\begin{equation*}
    E_{\text{train}} = \mathbb{E}_{\left(x, t\right) \sim D_{\text{train}}} \left[\left(f(x; w) - t\right)^2\right]
\end{equation*}
is the expected training error, and
\begin{equation*}
    E_{\text{test}} = \mathbb{E}_{\left(x, t\right) \sim D_{\text{test}}} \left[\left(f(x; w) - t\right)^2\right]
\end{equation*}
is the expected test error, where \(D_{\text{train}}\) and \(D_{\text{test}}\) are drawn from the same distribution, \textbf{then the inductive hypothesis assumes} $E_{\text{train}} \approx E_{\text{test}}$ when:
\begin{itemize}
    \item $D_{\text{train}} \approx D_{\text{test}}$ (i.i.d. data)
    \item The model has learned \textbf{general rules} rather than memorizing specific examples (i.e., it has not overfitted)
\end{itemize}
If that \textbf{assumption breaks} (e.g., due to \emph{too high complexity}, \emph{data shift}, or \emph{small dataset}), \textbf{generalization fails}, leading to poor performance on unseen data.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How can we quantify model complexity?}}
\end{flushleft}
The \textbf{complexity} of a neural network is determined by several factors, including:
\begin{itemize}
    \item The \textbf{number of parameters} (weights and biases) in the network: More parameters generally mean higher complexity.
    \item The \textbf{depth} of the network (number of layers): Deeper networks can capture more complex patterns.
    \item The \textbf{nonlinearities} introduced by activation functions: More complex activation functions can increase the model's capacity to learn intricate patterns.
    \item The \textbf{regularization} applied. Regularization is a technique used to reduce model complexity and prevent overfitting.
\end{itemize}
As complexity increases, the model's \textbf{capacity} to fit the data grows.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Not too complex, not too simple: how to find the right balance?}}
\end{flushleft}
Finding the right model complexity is crucial for good generalization. This involves balancing:
\begin{itemize}
    \item \textbf{Underfitting}: When the model is too simple to capture the underlying patterns in the data, leading to high bias and poor performance on both training and test data.
    \item \textbf{Overfitting}: When the model is too complex and captures noise in the training data, leading to low training error but high test error (high variance).
\end{itemize}
This balance is often referred to as the \definition{Bias-Variance Trade-off}, a mathematical intuition that helps explain the relationship between model complexity, bias, and variance:
\begin{equation}
    E_{\text{test}} = E\left[\left(y - \hat{f}(x)\right)^{2}\right] =
    \underbrace{\left(\text{Bias}^{2}\right)}_{\text{Systematic error}} +
    \underbrace{\text{Variance}}_{\text{Sensitivity error}} +
    \underbrace{\text{Noise}}_{\text{Irreducible error}}
\end{equation}
Where:
\begin{itemize}
    \item $E_{\text{test}}$ is the expected test error.
    \item $y$ is the true output.
    \item $\hat{f}(x)$ is the model's prediction.
    \item \textbf{Bias} is the \hl{systematic error introduced by approximating a real-world problem with a simplified model}. High bias can cause the model to miss relevant relations between features and target outputs (underfitting). For example, a linear model trying to fit a highly nonlinear relationship will have high bias.
    \item \textbf{Variance} is the \hl{sensitivity error due to fluctuations in the training data}. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting). For example, a very deep neural network with many parameters may fit the training data perfectly but perform poorly on unseen data.
    \item \textbf{Noise} is the \hl{irreducible error inherent in the data itself}, which \underline{cannot be} \underline{eliminated by any model}. For example, measurement errors or inherent randomness in the data generation process contribute to noise.
\end{itemize}
The \hl{goal is to find a model complexity that minimizes the total error}, which is the sum of bias and variance, paying attention to underfitting (high bias, low variance) and overfitting (low bias, high variance). In practice, the \textbf{inductive hypothesis is what makes machine learning possible}, \textbf{model complexity decides whether this hypothesis \emph{holds} or \emph{breaks}}, and the \textbf{bias-variance trade-off provides a framework to understand and manage this balance}.

\highspace
\begin{definitionbox}[: Inductive Hypothesis]
    The \definition{Inductive Hypothesis} (or \textbf{inductive bias}) is the fundamental assumption that \textbf{a model that performs well on  the training data will also perform well on unseen data}, proved that both come from the \textbf{same underlying distribution}.

    \highspace
    In mathematical terms:
    \begin{equation}
        E_{\text{test}} \approx E_{\text{train}} \quad \text{if} \quad
        D_{\text{train}} \sim D_{\text{test}}
    \end{equation}
    Where $E_{\text{train}}$ and $E_{\text{test}}$ are the expected training and test errors, respectively, and $D_{\text{train}}$ and $D_{\text{test}}$ are the training and test data distributions; the $\sim$ symbol indicates that both datasets are drawn from the same distribution.

    \highspace
    This assumption underlies \emph{all} machine learning: \hl{without it, no matter how low the training error is, we would have no reason to believe the model will generalize to new data.}
\end{definitionbox}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/neural-networks-and-overfitting/error-vs-model-complexity.pdf}
    \caption{This plot illustrates how \textbf{training} and \textbf{test} errors evolve as \textbf{model complexity} increases, giving the classic U-shaped curve for test error. Initially, as model complexity increases, both training and test errors decrease, indicating better fit to the data. However, beyond a certain point (the optimal complexity), the test error starts to increase due to overfitting, while the training error continues to decrease. The optimal model complexity is where the test error is minimized, balancing bias and variance effectively.}
\end{figure}