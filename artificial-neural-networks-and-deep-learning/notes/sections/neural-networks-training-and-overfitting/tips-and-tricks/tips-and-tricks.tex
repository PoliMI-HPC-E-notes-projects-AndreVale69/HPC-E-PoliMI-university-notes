\subsection{Tips \& Tricks}\label{sec:tips-and-tricks}

Once we know the loss function, the optimization rule (gradient descent/backpropagation), and the regularization techniques (weight decay, dropout, etc.), we still face a \textbf{practical problem}: ``\emph{even if our model and data are correct, why does training sometimes fail, diverge, or get stuck?}''. This last section focuses on the \textbf{engineering side} of deep learning, the small design decisions that make or break our model's ability to learn effectively.

\highspace
In this section, we will cover six practical ``tricks'' to improve neural network training:
\begin{enumerate}
    \item \textbf{Activation Function Saturation} (\autopageref{sec:activation-function-saturation}): Sigmoid and Tanh saturate for large inputs, then gradients vanish (go to zero). We'll see the \textbf{zero-gradient problem} and why modern networks use non-saturating functions.

    \item \textbf{ReLu and Variants} (\autopageref{sec:relu-and-variants}): the most widely used activation today. Simple, fast, avoids saturation, but comes with its own issues (dead neurons). Variants like \textbf{Leaky ReLu}, \textbf{ELU}, and \textbf{GELU} help mitigate these problems.

    \item \textbf{Weight Initialization} (\autopageref{sec:weight-initialization}): setting initial weights correctly is \emph{critical} for stable gradient propagation. The \textbf{Xavier (Glorot)} and \textbf{He} initializations balance signal variance between layers to avoid vanishing/exploding gradients.
    
    \item \textbf{Batch Normalization} (\autopageref{sec:batch-normalization}): normalizes intermediate activations to keep each layer's input distribution stable across training. This speeds up convergence and allows for higher learning rates.
    
    \item \textbf{Mini-Batch Training} (\autopageref{sec:mini-batch-training}): instead of computing gradients on the full dataset or single sample, we use small batches for better convergence and generalization. It's the practical compromise between \textbf{SGD} (Stochastic Gradient Descent) and \textbf{full-batch gradient descent}.
    
    \item \textbf{Learning Rate Scheduling} (\autopageref{sec:learning-rate-scheduling}): the learning rate controls the ``step size'' in optimization. Adaptive or decaying schedules (e.g., \textbf{Momentum}, \textbf{RMSProp}, \textbf{Adam}) ensure stable and efficient learning.
\end{enumerate}