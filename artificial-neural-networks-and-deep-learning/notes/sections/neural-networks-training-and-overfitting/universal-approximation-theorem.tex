\section{Neural Networks and Overfitting}

\subsection{Universal Approximation Theorem}

During the 1980s, researchers were trying to \textbf{understand the theoretical power} of neural networks. Before this period, many scientists were skeptical about the capabilities of neural networks: ``\emph{could neural networks really learn any kind of relationship between inputs and outputs, or were they limited to simple functions?}''. In 1989-1991, a series of papers by:
\begin{itemize}
    \item \textbf{George Cybenko (1989)}: \emph{``Approximation by superpositions of a sigmoidal function''} \cite{cybenko1989approximation};
    \item \textbf{Kurt Hornik (1991)}: \emph{``Approximation Capabilities of Multilayer Feedforward Networks''} \cite{hornik1991approximation}.
\end{itemize}
Proved rigorously that: \hl{even a \textbf{single hidden layer} feed-forward neural network with enough neurons and a non-linear activation (like sigmoid or tanh) can approximate \textbf{any continuous function} on a compact domain of $\mathbb{R}^n$ to any desired degree of accuracy}. This result gave \textbf{mathematical legitimacy} to neural networks, showing that they are not just \emph{pattern machines}, but theoretically \emph{universal function approximators}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Formal Statement}}
\end{flushleft}
Let's formalize the theorem.

\begin{theorem}[\definition{Universal Approximation Theorem}]%
    Let $f : \mathbb{R}^{n} \rightarrow \mathbb{R}$ be a \textbf{continuous} function defined on a \textbf{compact set} $K \subset \mathbb{R}^{n}$.

    \highspace
    Then, for any smaller number $\varepsilon > 0$, there exists a neural network function $F(x)$ of the form:
    \begin{equation}
        F(x) = \displaystyle\sum_{j=1}^{m} \alpha_{j} \cdot g\left(w_{j}^{T} x + b_{j}\right)
    \end{equation}
    Such that:
    \begin{equation}
        \left| F(x) - f(x) \right| < \varepsilon \quad \forall x \in K
    \end{equation}
    Where:
    \begin{itemize}
        \item $g\left(\cdot\right)$ is a \textbf{nonlinear}, \textbf{continuous}, \textbf{bounded activation function} (e.g., sigmoid, tanh);
        \item $\alpha_{j}, w_{j}, b_{j}$ are the network parameters (weights and biases);
        \item $m$ is the number of neurons in the hidden layer.
        \item $\varepsilon$ is the desired approximation accuracy.
    \end{itemize}
    In other words, with \textbf{enough hidden neurons}, a simple feedforward neural network can represent \textbf{any function}, no matter how complex.
\end{theorem}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{lightbulb} \textbf{Intuition}}
\end{flushleft}
The Universal Approximation Theorem tells us that \textbf{neural networks are incredibly powerful function approximators}. Even with just a \textbf{single hidden layer}, they can learn to represent \textbf{any continuous function} to an arbitrary degree of accuracy, as long as we provide enough neurons. This is because the non-linear activation functions allow the network to \textbf{combine simple building blocks} (the outputs of individual neurons) into \textbf{complex structures} that can capture intricate patterns in the data. However, it's important to note that while the theorem guarantees the existence of such a network, it does not provide a practical way to find the right architecture or parameters, nor does it address issues like \textbf{overfitting} or \textbf{generalization} to unseen data.

\highspace
\textcolor{Green3}{\faIcon{square-root-alt} \textbf{Geometric Intuition.}} Geometrically, each neuron in the hidden layer defines a \textbf{non-linear ``bump''} or ``ridge'' in the input space. By combining enough of these bumps (weighted by $\alpha_{j}$), the network can \textbf{shape its output} to follow any desired surface. Visually, if we had a 2D function $f(x_1, x_2)$, each hidden neuron adds a small deformation to the surface. Stacking many of them yields a highly flexible model:
\begin{equation*}
    f(x_1, x_2) \approx \sum_{j=1}^{m} \alpha_{j} \cdot g\left(w_{j1} \cdot x_1 + w_{j2} \cdot x_2 + b_{j}\right)
\end{equation*}
So, neural networks are \textbf{universal sculptors} of mathematical functions, capable of molding their outputs to fit any continuous shape we desire.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Practical Implications}}
\end{flushleft}
The theorem tells us \textbf{existence}, not \textbf{constructability} (it says a perfect network \emph{exists}, but not how to find its weights efficiently). In practice, we rely on \textbf{training algorithms (like backpropagation)}, which may get stuck in local minima or underfit/overfit the data. Also, even though a single layer is theoretically enough, \textbf{deep networks} (many layers) cam approximate the same function with \emph{fewer neurons} and \emph{more efficient representations}. This is why \emph{deep learning} became the dominant paradigm.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/neural-networks-and-overfitting/universal-approximation-theorem.pdf}
    \caption{This grid of four plots illustrates how a simple 1-hidden layer network (with 20 neurons) can approximate a nonlinear function, such as a sine wave. This simple 1-hidden layer neural network learns to approximate $\sin(x)$ even though we never told it what ``sine'' is. Also, this example shows how increasing the number of iterations allows the network to better fit the target function.}
\end{figure}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{Ockham's Razor and Model Simplicity}}
\end{flushleft}
The idea dates back to \textbf{William of Ockham (c. 1285-1349)}, a Franciscan friar and philosopher who formulated a logical and methodological principle still fundamental to science and machine learning ``Entia non sunt multiplicanda praeter necessitatem'' (entities must not be multiplied beyond necessity). In essence, \hl{prefer the \textbf{simplest explanation} that fits the data}. This became known as \definition{Ockham's Razor}, where ``razor'' metaphorically represents the act of \textbf{cutting away unnecessary complexity}.

\highspace
In the context of Neural Networks and Deep Learning, Ockham's Razor suggests that when building models, we should \textbf{favor simpler architectures} that adequately capture the underlying patterns in the data without introducing unnecessary complexity. In other words, among all models that can explain the training data, choose the one with the \textbf{lowest complexity} that still generalizes well to unseen data. Because neural networks are universal approximators, they can model \textbf{almost anything}, including true underlying patterns and random noise. But this flexibility is dangerous: a too-powerful model may \textbf{memorize} the training data rather than \textbf{learn patterns}, leading to \important{overfitting}. However, a model that is too simple may \important{underfit}, failing to capture important structures in the data. Thus, Ockham's Razor guides us to find a \textbf{balance} between simplicity and complexity, aiming for models that are just complex enough to capture the true patterns without overfitting.

\begin{remarkbox}[: What is Overfitting?]
    \definition{Overfitting} occurs when a \textbf{model learns} not just the \textbf{true patterns} in the data, \textbf{but also the random noise}. It's like memorizing answer for an exam instead of understanding the subject; we do well on the questions we saw before (training data), but fail on new ones (test data).

    Imagine fitting a curve to data points:
    \begin{center}
        \begin{tabular}{@{} l | p{20em} @{}}
            \toprule
            Fit Type & Description \\
            \midrule
            \textbf{Underfitting}   & The model is too simple (e.g., a straight line when the pattern is curved), it misses important structure. \\ [.5em]
            \textbf{Good Fit}       & The model captures the true pattern without being too complex. \\ [.5em]
            \textbf{Overfitting}    & The model bends and twists to go exactly through every training point, even if those points contain random noise. \\
            \bottomrule
        \end{tabular}
    \end{center}

    \textcolor{Red2}{\faIcon{exclamation-triangle}} Our model is overfitting when we observe the following:
    \begin{multicols}{2}
        \begin{itemize}
            \item \textbf{Training error}: very \emph{low}
            \item \textbf{Test error}: \emph{high}
        \end{itemize}
    \end{multicols}
    The model performs \textbf{too well} on the training set because it's \textbf{memorized it}, not generalized it. Usually, we can spot overfitting by plotting training and test errors over time:
    \begin{center}
        \includegraphics[width=.8\textwidth]{img/neural-networks-and-overfitting/overfitting.pdf}
    \end{center}
\end{remarkbox}

\begin{remarkbox}[: What is Underfitting?]
    \definition{Underfitting} occurs when a model is \textbf{too simple} to capture the underlying structure or patterns of the data. It performs poorly not only on unseen (test) data but \textbf{also on the training data}. Formally, if $f(x)$ is the true function we want to learn, and $\hat{f}$ is our model's prediction, underfitting means that:
    \begin{equation*}
        \hat{f}(x) \text{ cannot approximate } f(x) \text{ even on the training set}
    \end{equation*}
    So, the \textbf{training error remains high}, and naturally the \textbf{test error} will be high as well.

    \highspace
    We can think of underfitting as using a \textbf{too rigid model} for a complex relationship. For example, trying to fit a \textbf{straight line} to data that follows a \textbf{sinusoidal} curve, or training a neural network with \textbf{too few neurons/layers}, so it can't represent the non-linearities of the data. The model simply \textbf{doesn't have enough capacity} (parameters, complexity, expressiveness) to learn the data's structure.

    \begin{center}
        \includegraphics[width=\textwidth]{img/neural-networks-and-overfitting/underfitting-vs-overfitting.pdf}
    \end{center}

    \highspace
    \textcolor{Red2}{\faIcon{exclamation-triangle}} In a neural network context, underfitting can occur when:
    \begin{itemize}
        \item It has \textbf{too few neurons} or \textbf{layers} to represent the data completely.
        \item The \textbf{training time} is too short $\to$ not enough gradient updates to learn the patterns.
        \item The \textbf{learning rate} is too high $\to$ never converges properly.
        \item Strong \textbf{regularization} (e.g., large weight decay, dropout) limits learning capacity.
        \item Features are \textbf{not informative} (bad processing, missing normalization).
    \end{itemize}
\end{remarkbox}