\subsubsection{Bidirectional LSTM (BiLSTM) Networks}\label{sec:bidirectional-lstm-networks}

A standard (unidirectional) LSTM processes a sequence \textbf{from left to right}, meaning that at each time step \(t\), the LSTM has access only to the information from the previous time steps \(1, 2, \ldots, t-1\):
\begin{equation*}
    x_1 \rightarrow x_2 \rightarrow x_3 \rightarrow \ldots \rightarrow x_{T-1} \rightarrow x_T
\end{equation*}
This means the hidden state at time $t$ is based only on: the past inputs ($x_1, \dots, x_{t-1}$) and the current input ($x_t$). However, \textbf{in many tasks we \emph{also} need to consider future context}, i.e., information from the subsequent time steps \(t+1, t+2, \ldots, T\). This is where Bidirectional LSTM (BiLSTM) networks come into play.

\highspace
\begin{definitionbox}[: Bidirectional LSTM (BiLSTM)]
    A \definition{Bidirectional LSTM (BiLSTM)} is an \textbf{extension} of the traditional LSTM that can \textbf{capture information from both past and future contexts by processing the input sequence in both directions}.

    It consists of two LSTM layers:
    \begin{itemize}
        \item A \emph{forward} LSTM that processes the input sequence from left to right (from \(x_1\) to \(x_T\)).
        \item A \emph{backward} LSTM that processes the input sequence from right to left (from \(x_T\) to \(x_1\)).
    \end{itemize}
    At each time step \(t\), the outputs from both LSTMs are concatenated (or combined in some other way) to form the final output for that time step. This allows the network to have access to both past and future context when making predictions.
\end{definitionbox}

\highspace
\begin{examplebox}[: BiLSTM Application]
    For example, in natural language processing tasks: ``\emph{I arrived at the bank to \textbf{deposit} money}''. The word ``\emph{bank}'' is ambiguous, because we need the \emph{future} (``\emph{to deposit money}'') to understand that it refers to a financial institution rather than the side of a river. A BiLSTM can effectively capture this context by considering both preceding and succeeding words in the sentence.
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon[regular]{lightbulb} \textbf{Core Idea}}
\end{flushleft}
The core idea behind BiLSTMs is to \textbf{leverage two separate LSTM networks to process the input sequence in both directions}, allowing the model to capture a more comprehensive understanding of the data by considering both past and future contexts simultaneously. It consists of two LSTM layers:
\begin{itemize}
    \item \textbf{Forward LSTM}: Processes the input sequence from left to right, capturing information from past time steps.
    \begin{equation*}
        \overrightarrow{h}_t = \text{LSTM}_{\text{forward}}(x_t, \overrightarrow{h}_{t-1})
    \end{equation*}
    Here, \(\overrightarrow{h}_t\) represents the hidden state at time step \(t\) from the forward LSTM, and \(\overrightarrow{h}_{t-1}\) is the hidden state from the previous time step. This step learns representations from \textbf{past context}.
    
    \item \textbf{Backward LSTM}: Processes the input sequence from right to left, capturing information from future time steps.
    \begin{equation*}
        \overleftarrow{h}_t = \text{LSTM}_{\text{backward}}(x_t, \overleftarrow{h}_{t+1})
    \end{equation*}
    Here, \(\overleftarrow{h}_t\) represents the hidden state at time step \(t\) from the backward LSTM, and \(\overleftarrow{h}_{t+1}\) is the hidden state from the next time step. This step learns representations from \textbf{future context}.
    
    \item \textbf{Combining Outputs}: At each time step \(t\), the outputs from both the forward and backward LSTMs are combined (e.g., concatenated) to form the final output representation.
    \begin{equation*}
        h_t = [\overrightarrow{h}_t; \overleftarrow{h}_t]
    \end{equation*}
    Here, \(h_t\) is the combined hidden state at time step \(t\), which incorporates information from both past and future contexts.
\end{itemize}

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{When BiLSTM cannot be used}}
\end{flushleft}
Bidirectional models \textbf{cannot be used when future inputs are unknown at the time of prediction} (inference time):
\begin{itemize}
    \item \textbf{Real-time applications}: In scenarios where predictions must be made in real-time (e.g., live speech recognition, online translation), future data points are not available, making it impossible to utilize BiLSTMs effectively.
    \item \textbf{Online filtering}: In applications like online filtering or streaming data analysis, where data arrives sequentially and predictions must be made on-the-fly, BiLSTMs cannot be employed since they require access to future inputs.
    \item \textbf{Streaming sensor systems}: In systems that process data from sensors in real-time (e.g., IoT devices, autonomous vehicles), future sensor readings are not accessible at the moment of prediction, rendering BiLSTMs unsuitable.
    \item \textbf{Autoregressive generation}: In tasks such as text generation or time series forecasting, where each output depends on previously generated outputs, future inputs are inherently unknown during the generation process, preventing the use of BiLSTMs.
\end{itemize}
In these cases, \textbf{unidirectional LSTMs} or \textbf{GRUs} are typically employed, as they can make predictions based solely on past and current inputs without requiring future context.