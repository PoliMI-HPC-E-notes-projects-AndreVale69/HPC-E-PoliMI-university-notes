\subsection{Memoryless Models}

\subsubsection{Autoregressive (AR) Models}

The \definition{Autoregressive (AR) Model} is one of the oldest approach to handle temporal sequences, especially used in \textbf{signal processing}, \textbf{finance}, and \textbf{time-series forecasting}. Its principle is simple: the current value of a sequence depends on a finite number of its \textbf{past values}. In other words, the model \textbf{uses the past to predict the future}, but \emph{without} any hidden memory structure.

\highspace
\begin{definitionbox}[: Autoregressive (AR) Model]
    For a scalar time-series $x_t$, an \definition{Autoregressive (AR) Model}, or simply $AR(p)$ model of order $p$, is defined as:
    \begin{equation}
        \hat{x}_t = a_1 x_{t-1} + a_2 x_{t-2} + \ldots + a_p x_{t-p} + \epsilon_t
    \end{equation}
    Where:
    \begin{itemize}
        \item $a_1, a_2, \ldots, a_p$ are the model parameters (weights) that determine the influence of past values.
        \item $p$ is the number of \textbf{delay taps} (how back in time we look).
        \item $\epsilon_t$ is a white noise error term, accounting for randomness or unexplained variations.
    \end{itemize}
    If we arrange data as vectors, we can express the AR model in vector form:
    \begin{equation*}
        \mathbf{x}_t = \left[x_t, x_{t-1}, x_{t-2}, \ldots, x_{t-p+1}\right]^{T}
    \end{equation*}
    Thus, the AR model can be compactly written as:
    \begin{equation}
        \hat{x}_t = \mathbf{a}^{T} \mathbf{x}_{t-1} + \epsilon_t
    \end{equation}
    Where $\mathbf{a} = [a_1, a_2, \ldots, a_p]^{T}$. Or equivalently, for multivariate data:
    \begin{equation*}
        \hat{\mathbf{x}}_t = \mathbf{A}_1 \mathbf{x}_{t-1} + \mathbf{A}_2 \mathbf{x}_{t-2} + \ldots + \mathbf{A}_p \mathbf{x}_{t-p} + \boldsymbol{\epsilon}_t
    \end{equation*}
\end{definitionbox}

\highspace
\begin{examplebox}[: Autoregressive (AR) Model]
    Suppose we want to predict the next day's temperature:
    \begin{equation*}
        T_{\text{today}} = 0.7 \cdot T_{\text{yesterday}} + 0.2 \cdot T_{\text{2 days ago}} + 0.1 \cdot T_{\text{3 days ago}}
    \end{equation*}
    This means that the most recent day (yesterday) has the highest influence on today's temperature, while the influence decreases for days further back in time. The model linearly mixes these past temperatures to make a prediction.
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Properties and Limitations}}
\end{flushleft}
\begin{itemize}
    \item \important{Linear limitation}: The AR model assumes a linear relationship between past and current values, which \textbf{may not capture complex patterns in data} (e.g., if signal rising, slow down).
    \item \important{Finite memory (fixed window)}: It only considers a fixed number of past values (determined by $p$), \textbf{ignoring longer-term dependencies}.
    \item \important{Stationary assumption}: AR models typically assume that the underlying time series is stationary, meaning its statistical properties do not change over time. This can be a limitation when dealing with real-world data that may exhibit trends or seasonal patterns.
    \item \important{Deterministic, no hidden representation}: AR models do not maintain a hidden state or memory of past inputs beyond the fixed window, which can limit their ability to model complex temporal dependencies.
\end{itemize}
That's why we move toward \textbf{Feed-Forward extensions} and eventually \textbf{Recurrent Neural Networks}, which can introduce non-linearities and maintain hidden states for better temporal modeling.