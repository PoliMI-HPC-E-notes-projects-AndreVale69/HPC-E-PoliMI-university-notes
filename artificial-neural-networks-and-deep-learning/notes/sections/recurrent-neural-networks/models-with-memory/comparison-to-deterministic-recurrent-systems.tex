\subsubsection{Comparison to Deterministic Recurrent Systems}

In this section, we will attempt to answer the following question: \emph{How do classical dynamical models (e.g., Kalman filters and Hidden Markov Models) compare to deterministic recurrent systems (e.g., Recurrent Neural Networks)?} The answer will prepare us for the introduction of \textbf{neural-based recurrent architectures} in the next section.

\highspace
First of all, we highlight the difference between \textbf{probabilistic} and \textbf{deterministic} models.
\begin{itemize}
    \item \textbf{Probabilistic Dynamical Systems} (e.g., HMMs, Kalman filters) model the evolution of a system over time by \textbf{incorporating uncertainty and randomness}. They \textbf{use probability distributions} to represent the state of the system and the observations, allowing them to handle noise and uncertainty in the data effectively. In HMMs, for instance, the system transitions between hidden states based on probabilistic rules, and observations are generated from these states according to specific probability distributions.


    \item \textbf{Deterministic Recurrent Systems} (e.g., RNNs) model the evolution of a system over time by \textbf{using fixed rules or functions} to determine the next state based on the current state and input. They do not inherently account for uncertainty or randomness in their predictions. Instead, they rely on learned weights and activation functions to capture temporal dependencies in sequential data.
\end{itemize}
Let's make a more detailed comparison between these two types of models:
\begin{itemize}
    \item \definition{Probabilistic Dynamical Systems} are well-suited for tasks where uncertainty and noise are prevalent, such as speech recognition, natural language processing, and time series forecasting. They can effectively model the underlying stochastic processes that generate the observed data.
    
    \textcolor{Green3}{\faIcon{check} \textbf{Key Properties}}
    \begin{itemize}
        \item State transitions are \textbf{probabilistic}, allowing the model to capture uncertainty in the system's evolution.
        \item Observations are \textbf{probabilistic}, enabling the model to handle noisy data effectively.
        \item They explicitly represent \textbf{uncertainty} in both state transitions and observations.
        \item Inference is based on \textbf{probability distributions}, allowing for robust predictions in the presence of noise.
        \item Decisions are based on \textbf{likelihoods} and \textbf{argmax} (e.g., Viterbi algorithm for HMMs) to find the most probable sequence of states.
    \end{itemize}
    In practice, they are often \hl{used in scenarios where the data is noisy or incomplete, and where modeling uncertainty is crucial for accurate predictions}.
    
    \item \definition{Deterministic Recurrent Systems} excel in tasks that require learning complex temporal patterns and dependencies from large datasets, such as language modeling, machine translation, and video analysis. They can capture intricate relationships in sequential data through their learned representations. They are the ancestor of modern RNN architectures like LSTMs and GRUs. A deterministic recurrent system can be described by the following equations:
    \begin{equation*}
        h_t = f\left(h_{t-1}, \, x_t\right)
    \end{equation*}
    Where $f$ is a \textbf{deterministic function} (no randomness).

    \textcolor{Green3}{\faIcon{check} \textbf{Key Properties}}
    \begin{itemize}
        \item No noise terms.
        \item No probability distributions.
        \item Hidden state evolves deterministically.
        \item Same input sequence, then same hidden state trajectory.
        \item Learning (if any) is achieved via optimization of a loss function (e.g., MSE, cross-entropy) using gradient-based methods, not via probabilistic inference.
    \end{itemize}
    This makes them computationally efficient and easier to implement, especially for large-scale problems, but less expressive for handling uncertainty; less suited for tasks where observations are noisy; harder to reason about probabilistically. In practice, they are often \hl{used in scenarios where large amounts of sequential data are available, and the focus is on learning complex temporal patterns rather than modeling uncertainty}.
\end{itemize}
The main reason to transition from probabilistic dynamical systems to deterministic recurrent systems is that the latter can \textbf{learn complex temporal patterns and dependencies from large datasets}. This makes them more suitable for tasks such as language modeling, machine translation, and video analysis. RNNs can capture intricate relationships in sequential data through their learned representations, which is often more challenging for probabilistic dynamical systems that rely on predefined probabilistic rules and distributions. Additionally, RNNs can be trained end-to-end using gradient-based optimization techniques, allowing them to adapt to the specific characteristics of their training data. This flexibility and adaptability make RNNs a powerful choice for many modern deep learning applications.