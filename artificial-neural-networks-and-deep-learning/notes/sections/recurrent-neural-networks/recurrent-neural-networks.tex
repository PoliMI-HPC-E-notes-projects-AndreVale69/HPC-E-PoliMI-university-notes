\section{Recurrent Neural Networks (RNNs)}

In the previous sections, we have explored Feed-Forward Neural Networks\break (\autopageref{sec:feed-forward-neural-networks}), which are well-suited for tasks where inputs and outputs are fixed in size and independent of each other. So far, every model we have discussed processes data in a \textbf{static} manner: they took a fixed-size input vector $\mathbf{x}$  and produced a fixed-size output vector $\mathbf{y}$, \textbf{without considering any temporal or sequential dependencies} (no history, no sequence, no memory).

However, many real-world applications involve sequential data, where the order of the data points matters, such as time series analysis, natural language processing, and speech recognition. To handle such sequential data, we need a different type of neural network architecture known as \textbf{Recurrent Neural Networks (RNNs)}. They are models designed to handle \textbf{sequences} of data by maintaining a \textbf{hidden state} that captures information about previous time steps, allowing them to exhibit temporal dynamic behavior.
\begin{itemize}
    \item \textbf{Sequence Modeling}: RNNs are specifically designed to model sequences of data, making them suitable for tasks where the order of inputs matters.
    \item \textbf{Memoryless vs Memory-based models}: Unlike FNNs, which are memoryless and process each input independently, RNNs have a form of memory that allows them to retain information from previous inputs in the sequence. We will explore some models that exhibit this property, such as Autoregressive Models, Dynamical Systems, Hidden Markov Models (HMMs), and RNNs themselves.
    \item \textbf{Recurrent Neural Networks (RNNs)}: RNNs are a class of neural networks that have connections that form directed cycles, allowing information to persist over time. This enables them to capture temporal dependencies in sequential data. We will delve into the architecture of RNNs, recurrent connections, and hidden states.
    \item \textbf{Backpropagation Through Time (BPTT)}: Training RNNs involves a specialized version of backpropagation called Backpropagation Through Time (BPTT). We will discuss how BPTT works and how it allows RNNs to learn from sequential data.
    \item \textbf{Vanishing and Exploding Gradients}: RNNs can suffer from vanishing and exploding gradient problems during training, which can hinder their ability to learn long-term dependencies. We will explore these issues and discuss techniques to mitigate them.
    \item \textbf{Long Short-Term Memory (LSTM)}: To address the limitations of standard RNNs, we will introduce Long Short-Term Memory (LSTM) networks, which are a type of RNN designed to capture long-term dependencies more effectively.
\end{itemize}
In simple terms, if a feed-forward neural network maps a function from input to output, a recurrent neural network maps a sequence of inputs to a sequence of outputs, taking into account the temporal dependencies between the inputs. This makes RNNs particularly powerful for tasks involving sequential data, where the context provided by previous inputs is crucial for making accurate predictions.