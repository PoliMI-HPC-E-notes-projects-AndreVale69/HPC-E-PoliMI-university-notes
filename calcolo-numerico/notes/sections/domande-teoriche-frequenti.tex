\section{Domande Teoriche Frequenti}

In questa sezione sono raccolte le domande di teoria più ricorrenti, emerse durante le lezioni, le esercitazioni e le prove d'esame. L'obiettivo è fornire un ``compendio'' sintetico e mirato, che permetta di:
\begin{itemize}
    \item Avere un quadro immediato dei concetti fondamentali,
    \item Ripassare rapidamente i punti teorici più importanti,
    \item Orientarsi sulle domande che più spesso vengono utilizzate per verificare la comprensione.
\end{itemize}
Questa raccolta non sostituisce lo studio completo dei materiali, ma rappresenta una guida veloce per fissare e richiamare alla memoria i temi principali.

\begin{itemize}
    \item \textcolor{Red2}{\faIcon{question-circle} \textbf{\emph{%
        Elencare e discutere:
        \begin{enumerate}
            \item Le condizioni sufficienti per la convergenza del metodo di Gauss-Seidel.
            \item Le condizioni necessarie e sufficienti per la convergenza del metodo di Gauss-Seidel.
        \end{enumerate}
        Data $B_{GS}$ la matrice di iterazione di Gauss-Seidel, dimostrare che $\left\| B_{GS} \right\| < 1$ implica la convergenza.}}}

    \textcolor{Green3}{\faIcon{check-circle} \textbf{\emph{Soluzione.}}} Le condizioni sufficienti per la convergenza del metodo di Gauss-Seidel sono 3:
    \begin{itemize}
        \item La matrice $A$ deve essere a \textbf{dominanza diagonale stretta per righe}:
        \begin{equation*}
            \left|a_{ii}\right| > \displaystyle\sum_{j \ne i} \left|a_{ij}\right| \hspace{1em} \forall i
        \end{equation*}
        \item La matrice $A$ deve essere \textbf{simmetrica definita positiva (SPD)}:
        \begin{equation*}
            \mathbf{x}^{T} A \mathbf{x} > 0 \hspace{1em} \forall \mathbf{x} \ne 0
        \end{equation*}
        \item La matrice $A$ deve essere una \textbf{Matrice di classe M (M-Matrice) non singolare}. Ovvero una matrice con diagonale positiva e elementi extradiagonali (non sulla diagonale) non positivi.
    \end{itemize}
    L'unica condizione necessarie e sufficiente per la convergenza è quando il raggio spettrale della matrice di iterazione $B_{GS}$ è minore di uno:
    \begin{equation*}
        \text{convergenza} \iff \rho(B_{GS}) < 1
    \end{equation*}
    \begin{proof}[Dimostrazione ($\left\| B_{GS} \right\| < 1$ implica la convergenza)]
        Il metodo di Gauss-Seidel si scrive
        \begin{equation*}
            x^{(k+1)} = B_{GS}x^{(k)} + c
        \end{equation*}
        Definendo l'errore $e^{(k)} = x^{(k)} - x^{*}$, dove $x^{*}$ rappresenta la soluzione esatta del sistema lineare che Gauss-Seidel cerca di raggiungere, e $x^{(k)}$ l'approssimazione della soluzione al passo $k$, si ha
        \begin{equation*}
            e^{(k+1)} = B_{GS} e^{(k)} \quad \Rightarrow \quad e^{(k)} = B_{GS}^k e^{(0)}
        \end{equation*}
        Se $\|B_{GS}\| < 1$, allora:
        \begin{equation*}
            \|e^{(k)}\| \le \|B_{GS}\|^k \|e^{(0)}\| \to 0,
        \end{equation*}
        Per $k \to \infty$. Quindi $x^{(k)} \to x^\ast$: il metodo converge.
    \end{proof}

    \begin{deepeningbox}[: Spiegazione della Dimostrazione]
        L'idea principale della dimostrazione è: \textbf{l'errore ad ogni passo viene moltiplicato da una matrice}; se questa matrice ``schiaccia'' tutti i vettori di almeno un fattore $q < 1$, allora l'errore cala geometricamente $\to 0$.

        \begin{itemize}
            \item \important{Passo 0: Dal sistema $Ax=b$ all'iterazione}. Per Gauss-Seidel si scrive (con $A = D + L + U$):
            \begin{equation*}
                \begin{array}{rcl}
                    x^{(k+1)} &=& B_{GS}\,x^{(k)} + c \\ [.3em]
                    B_{GS} &=& -(D+L)^{-1}U \\ [.3em]
                    c &=& (D+L)^{-1}b
                \end{array}
            \end{equation*}
            Questa è una \textbf{iterazione a punto fisso} $x^{(k+1)}=T(x^{(k)})$ con $T(x)=B_{GS}x+c$. Si ricorda che l'iterazione a punto fisso è un metodo iterativo per risolvere $A\mathbf{x} = \mathbf{b}$ che si può sempre scrivere nella forma $x^{(x+1)} = T(x^{(k)})$, dove $T$ è una funzione; se esiste una $x^{*}$ tale che $T(x^{*}) = x^{*}$, allora $x^{*}$ si chiama punto fisso di $T$.

            \highspace
            Nel caso Gauss-Seidel, si ha:
            \begin{equation*}
                x^{(k+1)} = B_{GS} x^{(k)} + c
            \end{equation*}
            Quindi $T(x) = B_{GS} x^{(k)} + c$. Il punto fisso $x^{*}$ soddisfa:
            \begin{equation*}
                x^{*} = B_{GS} x^{*} + C
            \end{equation*}
            Cioè proprio il sistema originale $Ax = b$.


            \item \important{Passo 1: Errore che si propaga}. Sia $x^{*}$ la soluzione (il punto fisso): soddisfa $x^{*} = B_{GS}x^{*} + c$. Definiamo l'errore $e^{(k)} = x(k) - x^{*}$. Sottraendo le due relazioni:
            \begin{equation*}
                e^{(k+1)} = B_{GS} e^{(k)}
            \end{equation*}
            Quindi l'errore al passo successivo è semplicemente $B_{GS}$ per l'errore attuale.


            \item \important{Passo 2: Usiamo una norma ``coerente''}. Prendiamo una norma matriciale indotta (coerente con una norma vettoriale), cioè una per cui vale: $\left\| A v \right\| \le \left\| A \right\| \left\| v \right\|$. Applicandola:
            \begin{equation*}
                \left\| e^{(k+1)} \right\| = \left\| B_{GS}e^{(k)} \right\| \le \left\| B_{GS} \right\| \left\| e^{(k)} \right\|
            \end{equation*}
            Iterando:
            \begin{equation*}
                \left\| e^{(k)} \right\| \le \left\| B_{GS} \right\|^{k} \left\| e^{(0)} \right\|
            \end{equation*}

            
            \item \important{Passo 3: Conclusione (contrazione geometrica)}. Se $\left\| B_{GS} \right\| = q < 1$, allora $q^{k} \to 0$. Dunque $\left\| e^{(k)} \right\| \to 0$ e quindi $x^{(k)} \to x^{*}$. \textbf{Questo è tutto}. L'ipotesi $\left\|B_{GS}\right\| < 1$ garantisce convergenza \textbf{da qualunque} $x^{(0)}$.
        \end{itemize}
    \end{deepeningbox}


    \item \textcolor{Red2}{\textbf{\emph{Si introduca l'interpolante di Lagrange composito $\Pi_{k}^{H}$ con $k \ge 1$ definendo con precisione la notazione utilizzata. A partire dalla stima di convergenza dell'interpolatore Lagrangiano, si deduca la stima dell'errore di interpolazione composita in funzione di $H$ ed $k$.}}}
    
    \textcolor{Green3}{\faIcon{check-circle} \textbf{\emph{Soluzione.}}}  Sia $I = \left[a,b\right]$ un intervallo e sia data una \textbf{partizione} (o griglia):
    \begin{equation*}
        \mathcal T_H={a=x_0<x_1<\dots<x_M=b}, \qquad I_j=[x_j,x_{j+1}],\; j=0,\dots,M-1
    \end{equation*}
    Con \textbf{passo massimo}:
    \begin{equation*}
        H:=\max_{0\le j\le M-1} h_j, \qquad h_j:=x_{j+1}-x_j
    \end{equation*}
    Fissato un \textbf{grado} $k \ge 1$, su ciascun sottointervallo $I_{j}$, si scelgono $k+1$ \textbf{nodi locali} $\left\{x^{(i)}_j\right\}_{i=0}^k\subset I_j$ (ad es. equispaziati in $I_{j}$) e si definisce $\Pi_{k}^{H} f$ \textbf{a tratti} come il \important{polinomio di Lagrange} di grado $k$ che interpola $f$ nei nodi di $I_{j}$:
    \begin{equation*}
        \left(\Pi_k^H f\right)|_{I_j} \;\; := \;\; \Pi_k\left(f|_{I_j}\right)
        \quad\text{con}\quad
        \left(\Pi_k f\right)\left(x^{(i)}_j\right)=f\left(x^{(i)}_j\right),\; i=0,\dots,k
    \end{equation*}

    \textbf{Stima dell'errore}. Si assuma $f \in C^{k+1}\left(\left[a,b\right]\right)$. Sulla \textbf{singola} cella $I_{j}$ di ampiezza $h_{j}$ la classica stima dell'errore dell'interpolazione di Lagrange di grado $k$:
    \begin{equation*}
        \left\| f - \Pi_{k} f \right\|_{L^\infty \left(I_j\right)} \; \le \; C \: h_{j}^{k+1} \left\| f^{\left(k+1\right)} \right\|_{L^{\infty} \left(I_{j}\right)}
    \end{equation*}
    Con $C$ indipendente da $h_{j}$ (deriva dalla formula dell'errore con $\prod \left(x-x_{i}\right)$). Prendendo il massimo su tutte le celle e usando $h_{j} \le H$:
    \begin{equation*}
        \left\| f - \Pi_{k}^{H} f \right\|_{L^{\infty}\left(\left[a,b\right]\right)} \; \le \;
        C \, H^{k+1} \, \left\| f^{\left(k+1\right)} \right\|_{L^{\infty}\left(\left[a,b\right]\right)}
    \end{equation*}
    Ossia \textbf{ordine} $\left(k+1\right)$ in $H$.

    \textbf{Caso $k=1$, lineare composito}. In particolare, se $f \in C^{2}$ e si usando $k=1$ e nodi agli estremi di ogni $I_{j}$, vale:
    \begin{equation*}
        \left\| f - \Pi_{1}^{H} f \right\|_{\infty} \; \le \; \dfrac{H^{2}}{8} \, \left\| f'' \right\|_{\infty}
    \end{equation*}
    Che mostra esplicitamente l'ordine 2 in $H$.

    \begin{takeawaysbox}[: Interpolazione di Lagrange composita]\label{takeaways:interpolazione_composita}
        Per ricordare:
        \begin{itemize}
            \item \important{Schema per l'interpolante di Lagrange composito}:
            \begin{enumerate}
                \item \textbf{Partizione dell'intervallo}. Sia $\left[a,b\right]$ un intervallo suddiviso in sottointervalli $I_{j} = \left[x_{j}, x_{j+1}\right]$ con passo massimo $H_{\max_{j} h_{j}}$ con $h_{j} = x_{j+1} - x_{j}$.

                \begin{itemize}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Cosa si intende con ``partizione dell'intervallo''?}} Si considera un intervallo globale $\left[a,b\right]$. Per costruire un interpolante \textbf{composito}, non lo si tratta tutto in una volta, ma viene diviso in pezzi pi\uaccent piccoli:
                    \begin{equation*}
                        a = x_{0} < x_{1} < x_{2} < \dots < x_{M} = b
                    \end{equation*}
                    I piccoli intervalli $\left[ x_{j}, x_{j+1} \right]$ si chiamano \textbf{sottointervalli} o \textbf{celle}. Essi vengono indicati pi\uaccent formalmente come:
                    \begin{equation*}
                        I_{j} = \left[x_{j}, x_{j+1}\right], \quad j = 0, 1, \dots, M-1
                    \end{equation*}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Cosa rappresenta $h_{j}$?}} La \textbf{lunghezza} del sottointervallo $I_{j}$ (ovvero la lunghezza della cella $j$-esima):
                    \begin{equation*}
                        h_{j} = x_{j+1} - x_{j}
                    \end{equation*}
                    Ovviamente, se tutti gli $x_{j}$ fossero equispaziati, allora $h_{j}$ sarebbe costante. In generale pu\oaccent variare da un intervallo all'altro.
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Cos'\eaccent il ``passo massimo''?}} Si definisce:
                    \begin{equation*}
                    H = \max_{0 \le j \le M-1} h_{j}
                    \end{equation*}
                    Ovvero, tra tutte le celle nell'intervallo, si considera quella pi\uaccent lunga. Serve come \textbf{misura della finezza della partizione}: pi\uaccent $H$ \eaccent piccolo, pi\uaccent la griglia \eaccent fitta. L'errore dell'interpolazione composita dipender\aaccent proprio da $H$.
                \end{itemize}
                Quindi, in altre parole, si divide l'intervallo in ``pezzi'', dove $I_{j}$ rappresenta ogni piccolo intervallo; l'ampiezza di ogni intervallo \eaccent dato da $h_{j}$, e il sottointervallo pi\uaccent grande (il ``peggiore''), entra nella stime dell'errore.

                \item \textbf{Locale}. Fissato un grado $k \ge 1$, su ogni $I_{j}$, si sceglie $k+1$ nodi (es. equispaziati) e si definisce $\left(\Pi_{k}^{H} f\right)|_{I_{j}}$ come il \textbf{polinomio di Lagrange di grado $k$} che interpola $f$ in quei nodi.
                
                \begin{itemize}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{L'idea ``locale''.}} L'intervallo ``grande'' scelto all'inizio $\left[a,b\right]$ \eaccent stato diviso in sottointervalli $I_{j} = \left[x_{j}, x_{j+1}\right]$. A questo punto, invece di costruire un \textbf{unico polinomio globale}, si inizia intanto con i \textbf{pezzi}: su ogni sottointervallo $I_{j}$ si costruisce un piccolo polinomio interpolante.
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Grado $k$ e nodi locai.}} Si fissa un grado del polinomio, in questo caso $k \ge 1$ come suggerito dal testo. Su ciascun sottointervallo $I_{j}$ si scelgono $k+1$ \textbf{nodi} (solitamente equispaziati, ma non \eaccent obbligatorio). Per esempio:
                    \begin{itemize}
                        \item $k=1$, i due estremi: $\left\{x_{j}, x_{j+1}\right\}$
                        \item $k=2$, estremi e punto medio: $\left\{x_{j}, x_{j+\frac{1}{2}}, x_{j+1}\right\}$
                        \item $k=3$, estremi e due punti:
                        \begin{equation*}
                            \left\{x_{j}, x_{j+\frac{1}{3}}, x_{j+\frac{2}{3}}, x_{j+1}\right\}
                        \end{equation*}
                    \end{itemize}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Polinomio di Lagrange locale.}} Su questi $k+1$ nodi si costruisce il \textbf{polinomio di Lagrange di grado $k$} che interpola $f$ (la funzione). Si indica quindi il polinomio locale con:
                    \begin{equation*}
                        \left(\Pi_{k}^{H} f\right) |_{I_{j}} \in \mathbb{P}_{k}
                    \end{equation*}
                    Ovvero appartiene allo spazio dei polinomi di grado $\le k$ e soddisfa:
                    \begin{equation*}
                        \left(\Pi_{k}^{H} f\right)\left(x_{j}^{\left(i\right)}\right) = f\left(x_{j}^{(i)}\right) \quad i = 0, \dots, k
                    \end{equation*}
                \end{itemize}
                Quindi, con locale si intende che per ogni sottointervallo $I_{j}$ si costruisce un polinomio di Lagrange di grado $k$ che interpola la funzione in $k+1$ nodi scelti in quel sottointervallo. In altre parole, si dice locale, perch\eaccent si divide l'intervallo in tanti sottointervalli $I_{j}$, e su ciascuno si costruisce un polinomio di grado $k$ che interpola \textbf{solo i nodi interni a quel sottointervallo}.

                \item \textbf{Globale}. L'\textbf{interpolante di Lagrange composito}:
                \begin{equation*}
                    \Pi_{k}^{H} f \in X_{h}^{k} := \left\{v \in C^{0}\left(\left[a,b\right]\right) : v|_{I_{j}} \in \mathbb{P}_{k} \forall j\right\}
                \end{equation*}

                \begin{itemize}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{L'idea ``globale''.}} Finora \eaccent stato visto la costruzione \textbf{locale}: su ciascun sottointervallo $I_{j}$ un polinomio di Lagrange di grado $k$. Ora si mette insieme tutti questi polinomi locali per formare una funzione definita su tutto l'intervallo $\left[a,b\right]$. Questa funzione \eaccent l'\textbf{interpolante di Lagrange composito} $\Pi_{k}^{H} f$.
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Spazio funzionale $X_{h}^{k}$.}} La notazione:
                    \begin{equation*}
                        X_{h}^{k} := \left\{ v \in C^{0}\left(\left[a,b\right]\right) : v|_{I_{j}} \in \mathbb{P}_{k} \quad \forall j \right\}
                    \end{equation*}
                    Vuol dire:
                    \begin{itemize}
                        \item $C^{0}\left(\left[a,b\right]\right)$: le funzioni sono continue su tutto l'intervallo $\left[a,b\right]$ (non ci sono salti agli estremi delle celle).
                        \item $v|_{I_{j}} \in \mathbb{P}_{k}$: la restrizione di $v$ a ciascun sottointervallo $I_{j}$ \eaccent un polinomio di grado $\le k$. In parole semplici, su ogni cella, la funzione \eaccent un polinomio di grado $k$.
                        \item $\forall j$: questo vale per tutti i sottointervalli $I_{j}$.
                    \end{itemize}
                    Quindi $X_{h}^{k}$ \eaccent lo spazio di tutte le funzioni continue su $\left[a,b\right]$ che, su ogni sottointervallo $I_{j}$, sono polinomi di grado $\le k$. L'interpolante composito $\Pi_{k}^{H} f$ appartiene a questo spazio, perch\eaccent costruito proprio in questo modo.
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{L'interpolante composito.}} Per come \eaccent stata costruita, l'interpolante $\Pi_{k}^{H} f$ apparitene a $X_{h}^{k}$, dato che:
                    \begin{itemize}
                        \item Su ogni sottointervallo $I_{j}$, \eaccent un polinomio di grado $k$ (quindi la restrizione \eaccent in $\mathbb{P}_{k}$);
                        \item Ed \eaccent continuo sugli estremi (tutti i polinomi locali coincidono col valore di $f$ sui nodi condivisi).
                    \end{itemize}
                \end{itemize}
            \end{enumerate}

            \item \important{Stima dell'errore}
            \begin{enumerate}
                \item \textbf{Locale}. Su $I_{j}$, ampiezza $h_{j}$, per $f \in C^{k+1}\left(I_{j}\right)$:
                \begin{equation*}
                    \left\| f - \Pi_{k} f \right\|_{L^{\infty}\left(I_{j}\right)} \; \le \; C_{k} h_{j}^{k+1} \left\| f^{\left(k+1\right)} \right\|_{L^{\infty}\left(I_{j}\right)}
                \end{equation*}
                Dalla formula classica dell'errore di Lagrange.

                \begin{itemize}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Che cosa si vuole stimare?}} A questo punto, si ha la funzione originale $f$ e il suo polinomio interpolante composito $\Pi_{k} f$ su un sottointervallo $I_{j} = [x_{j}, x_{j+1}]$. Si vuole stimare \textbf{quanto si discostano} su quell'intervallo, ovvero l'errore di interpolazione:
                    \begin{equation*}
                        f(x)-\Pi_k f(x)
                    \end{equation*}
                    Questo passaggio \eaccent fondamentale per capire la qualit\aaccent dell'approssimazione.
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Formula dell'errore (caso generale).}} Se $f \in C^{k+1}(I_{j})$, ovvero $f$ \eaccent sufficientemente liscia (derivabile fino a ordine $k+1$), allora esiste un punto $\xi(x) \in I_{j}$ tale che:
                    \begin{equation*}
                        f(x) - \Pi_k f(x) = \dfrac{f^{(k+1)}(\xi_x)}{(k+1)!} \, \prod_{i=0}^{k} (x - x_{i})
                    \end{equation*}
                    Dove:
                    \begin{itemize}
                        \item $x_{0}, \dots, x_{k}$ sono i nodi usati nell'interpolazione su $I_{j}$;
                        \item $\xi_{x}$ \eaccent un punto (non noto) in $I_{j}$ che dipende da $x$.
                        \item $k+1$ \eaccent il numero di nodi, quindi il grado del polinomio pi\uaccent uno.
                        \item $(k+1)!$ \eaccent il fattoriale di $k+1$.
                        \item $f^{(k+1)}(\xi_{x})$ \eaccent la derivata di ordine $k+1$ di $f$ valutata in $\xi_{x}$.
                        \item $\prod_{i=0}^{k} (x - x_{i})$ \eaccent il prodotto di tutti i termini $(x - x_{i})$, che misura la distanza di $x$ dai nodi.
                    \end{itemize}
                    Quindi, l'errore dipende da due fattori:
                    \begin{enumerate}
                        \item La derivata di ordine $k+1$ di $f$, che misura la ``curvatura'' della funzione.
                        \item Il prodotto $\prod\left(x - x_{i}\right)$, che misura quanto $x$ \eaccent lontano dai nodi, ovvero quanto l'interpolante pu\oaccent discostarsi dalla funzione.
                    \end{enumerate}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Quanto vale quel prodotto?}} Se i nodi stanno in $I_{j} = \left[x_{j}, x_{j+1}\right]$, allora ogni fattore $\left(x-x_{i}\right)$ \eaccent al massimo grande quanto la lunghezza dell'intervallo:
                    \begin{equation*}
                        \left| x - x_{i} \right| \le h_{j} = x_{j+1} - x_{j}
                    \end{equation*}
                    Quindi, il prodotto di tutti i fattori \eaccent al massimo:
                    \begin{equation*}
                        \left| \prod_{i=0}^{k} (x - x_{i}) \right| \le h_{j}^{k+1}
                    \end{equation*}
                    Dove $C_{k}$ \eaccent una costante che dipende solo da $k$, ovvero da come sono scelti i nodi (es. se sono equispaziati, se sono di Chebyshev, etc.).
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Risultato finale.}} Mettendo tutto insieme:
                    \begin{equation*}
                        \left| f(x) - \Pi_{k} f(x) \right| \le C_{k} h_{j}^{k+1} \max_{y \in I_{j}} \left\| f^{(k+1)}(y) \right\|
                    \end{equation*}
                    Se si prende il massimo su $x \in I_{j}$, si ottiene la stima dell'errore in norma infinito su $I_{j}$:
                    \begin{equation*}
                        \left\| f - \Pi_{k} f \right\|_{L^{\infty}\left(I_{j}\right)} \le C_{k} h_{j}^{k+1} \left\| f^{(k+1)} \right\|_{L^{\infty}\left(I_{j}\right)}
                    \end{equation*}
                \end{itemize}
                In altre parole, l'errore \textbf{decresce come una potenza} $h_{j}^{k+1}$, quando il sottointervallo si restringe. L'ordine di convergenza \eaccent $k+1$: pi\uaccent alto \eaccent il grado del polinomio ($k$), pi\uaccent velocemente l'errore decresce al restringersi dell'intervallo ($h_{j} \to 0$). Infine, la costante $C_{k}$ dipende solo da come sono scelti i nodi (es. equispaziati, Chebyshev, etc.), ovvero dal grado e dalla posizione dei nodi, non da $h_{j}$.

                \item \textbf{Globale}. Poich\eaccent $h_{j} \le H$ per tutti $j$, si ottiene:
                \begin{equation*}
                    \left\| f - \Pi_{k}^{H} f \right\|_{L^{\infty}\left(\left[a,b\right]\right)} \; \le \; C_{k} H^{k+1} \left\| f^{\left(k+1\right)} \right\|_{L^{\infty}\left(\left[a,b\right]\right)}
                \end{equation*}
                Quindi ordine $k+1$ in H.

                \begin{itemize}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Da locale a globale.}} Finora \eaccent stata vista la stima dell'errore su un singolo sottointervallo $I_{j}$:
                    \begin{equation*}
                        \left\| f - \Pi_{k} f \right\|_{L^{\infty}\left(I_{j}\right)} \le C_{k} h_{j}^{k+1} \left\| f^{(k+1)} \right\|_{L^{\infty}\left(I_{j}\right)}
                    \end{equation*}
                    Cio\eaccent su ogni cella si sa quanto \eaccent grande l'errore. Tuttavia l'obiettivo \eaccent stimare l'errore su tutto l'intervallo $\left[a,b\right]$. Esso si ottiene prendendo il massimo su tutte le celle (sottointervalli):
                    \begin{equation*}
                        \left\| f - \Pi_{k}^{H} f \right\|_{L^{\infty}\left(\left[a,b\right]\right)} \le \max_{0 \le j \le M-1} \left\| f - \Pi_{k} f \right\|_{L^{\infty}\left(I_{j}\right)}
                    \end{equation*}
                    \item[\textcolor{Green3}{\faIcon{question-circle}}] \textcolor{Green3}{\textbf{Uso del passo massimo $H$.}} Dato che ogni $h_{j} \le H$, si pu\oaccent sostituire $h_{j}$ con $H$ nella stima dell'errore:
                    \begin{equation*}
                        \left\| f - \Pi_{k} f \right\|_{L^{\infty}\left(I_{j}\right)} \le C_{k} H^{k+1} \left\| f^{(k+1)} \right\|_{L^{\infty}\left(I_{j}\right)}
                    \end{equation*}
                    Prendendo il massimo su tutte le celle:
                    \begin{equation*}
                        \hspace{-1.9em}\left\| f - \Pi_{k}^{H} f \right\|_{L^{\infty}\left(\left[a,b\right]\right)} \le C_{k} H^{k+1} \max_{0 \le j \le M-1} \left\| f^{(k+1)} \right\|_{L^{\infty}\left(I_{j}\right)}
                    \end{equation*}
                \end{itemize}
                La dipendenza dall'ampiezza massimo $H$ mostra che \textbf{raffinando la partizione} (riducendo $H$), l'errore decresce. L'\textbf{ordine di convergenza} \eaccent $k+1$: se si usa un polinomio lineare ($k=1$), l'errore decresce come $H^{2}$ ($\mathcal{O}(H^{2})$); se si usa un polinomio quadratico ($k=2$), l'errore decresce come $H^{3}$ ($\mathcal{O}(H^{3})$), e cos\iaccent via. Infine, la costante $C_{k}$ dipende solo da come sono scelti i nodi (es. equispaziati, Chebyshev, etc.), ovvero dal grado e dalla posizione dei nodi, non da $H$.
                
                In altre parole, dall'ultima equazione, ne consegue che l'interpolante di Lagrange composito converge alla funzione originale al crescere della finezza della partizione (ovvero al diminuire di $H$), con un ordine di convergenza che dipende dal grado del polinomio usato in ogni sottointervallo.
            \end{enumerate}
        \end{itemize}
    \end{takeawaysbox}


    \item \textcolor{Red2}{\textbf{\emph{Si consideri il seguente problema di Cauchy:
    \begin{equation*}
        \begin{cases}
            y'(t) = \left[\dfrac{\pi \cos\left(\pi t\right)}{2 + \sin\left(\pi t\right)} - \dfrac{1}{2}\right] y(t) & t \in \left(0, 10\right) \\[1em]
            y(0) = 2
        \end{cases}
    \end{equation*}
    La cui soluzione esatta è $y(t) = \left(2 + \sin\left(\pi t\right)\right) e^{- t/2}$. Si riporti l'algoritmo del metodo di Eulero in avanti applicato al problema di Cauchy definendo con precisione tutta la notazione utilizzata. Dopo aver posto $f(t, y) = -\lambda y$, con $\lambda > 0$, si ricavi la condizione di assoluta stabilità per il metodo di Eulero in avanti.}}}

    \hqlabel{problema-cauchy-con-eulero-avanti}{\textcolor{Green3}{\faIcon{check-circle} \textbf{\emph{Soluzione.}}}} Si ha un \textbf{problema di Cauchy} nella forma generale:
    \begin{equation*}
        y'(t) = f(t, y(t)) \quad y(t_0) = y_0
    \end{equation*}
    L'obbiettivo dei \textbf{metodi numerici per ODE} (Ordinary Differential Equations) \eaccent costruire una sequenza di valori ($u_n \approx y(t_n)$) che approssimi la soluzione in punti discreti ($t_n = T_0 + n h$). In altre parole, si vuole costruire un metodo per approssimare la soluzione della ODE numericamente, calcolando dei valori $y(t_0), y(t_1), y(t_2), \dots$, che si avvicinino alla curva vera.

    \textbf{Metodo di Eulero in avanti}. Si immagini di avere una curva $y(t)$ ma di conoscerne solo un punto iniziale $(t_0, y_0)$. Se si applica la \textbf{derivata} in quel punto, si ottiene la \textbf{pendenza} della curva in quel punto:
    \begin{equation*}
        y'(t_0) = f(t_0, y_0)
    \end{equation*}
    A questo punto, si può \textbf{disegnare la tangente} alla curva in quel punto $t_{0}$. Se ci si sposta di un piccolo passo $h$ lungo l'asse $t$, la variazione in $y$ è di circa:
    \begin{equation*}
        \Delta y = h \cdot y'(t_0) = h \cdot f(t_0, y_0)
    \end{equation*}
    Quindi, il nuovo punto sarà:
    \begin{equation*}
        y(t_1) \approx y_0 + h \cdot f(t_0, y_0)
    \end{equation*}
    E questo è il cuore del \textbf{metodo di Eulero in avanti}.

    Più formalmente:
    \begin{enumerate}
        \item Si sceglie un passo $h$ (piccolo, per essere precisi), come per esempio $h = 0.1$.
        \item Si creano i punti temporali:
        \begin{equation*}
            t_n = t_0 + n h, \quad n = 0, 1, 2, \dots, N
        \end{equation*}
        Che rappresentano i punti in cui si vuole approssimare la soluzione.
        \item Si definisce il punto iniziale:
        \begin{equation*}
            u_0 = y_0 \quad \text{(valore iniziale noto)}
        \end{equation*}
        \item Dopodiché, per ogni passo che si vuole fare, si applica la formula di Eulero in avanti:
        \begin{equation*}
            u_{n+1} = u_n + h \cdot f(t_n, u_n)
        \end{equation*}
        Dove $u_n$ è l'approssimazione di $y(t_n)$.
    \end{enumerate}
    Viene chiamato ``\textbf{in avanti}'' perché si usa l'informazione del punto attuale $t_n$ per calcolare il punto successivo $t_{n+1}$.

    \highspace
    \textbf{Risposta.} Per il problema specifico:
    \begin{enumerate}
        \item Si ha $t \in \left[0, 10\right]$, la condizione iniziale $y(0) = 2$, e la funzione:
        \begin{equation*}
            f(t, y) = \left[\dfrac{\pi \cos\left(\pi t\right)}{2 + \sin\left(\pi t\right)} - \dfrac{1}{2}\right] y
        \end{equation*}
        Quindi:
        \begin{itemize}
            \item Intervallo: $t \in \left[0, 10\right]$
            \item Numero di passi $N$
            \item Passo, bisogna sceglierlo molto piccolo per evitare errori, quindi:
            \begin{equation*}
                h = \dfrac{10 - 0}{N} = \dfrac{10}{N}
            \end{equation*}
            \item Punti temporali su cui si approssima la soluzione:
            \begin{equation*}
                t_n = t_0 + n h \quad n = 0, 1, \dots, N
            \end{equation*}
            \item Passo iniziale identico alla condizione iniziale:
            \begin{equation*}
                u_0 = y(0) = 2
            \end{equation*}
        \end{itemize}

        \item In generale, il metodo di Eulero in avanti si scrive come:
        \begin{equation*}
            u_{n+1} = u_{n} + h \cdot f(t_{n}, u_{n})
        \end{equation*}
        Dove $u_{n}$ rappresenta l'approssimazione di $y(t_{n})$ e $f(t_{n}, u_{n})$ \eaccent la pendenza della curva in quel punto, ovvero la derivata.

        Nel nostro caso, si ha:
        \begin{equation*}
            u_{n+1} = u_{n} + h \cdot \left(
                \dfrac{\pi \cos\left(\pi t_{n}\right)}{2 + \sin\left(\pi t_{n}\right)} - \dfrac{1}{2}
            \right) \cdot u_{n}
        \end{equation*}

        \item A questo punto si studia la stabilità del metodo ponendo:
        \begin{equation*}
            f(t, y) = -\lambda y, \quad \lambda > 0
        \end{equation*}
        Quindi, il metodo di Eulero muta in:
        \begin{equation*}
            u_{n+1} = u_{n} + h \cdot \left(-\lambda u_{n}\right) = u_{n} - h\lambda u_{n} = u_{n} \cdot \left(1 - h\lambda\right)
        \end{equation*}

        \item La stabilità del metodo dipende dal fattore moltiplicativo:
        \begin{equation*}
            g = 1 - h\lambda
        \end{equation*}
        Dato che viene moltiplicato ad ogni passo. Per garantire che la soluzione numerica sia stabile (ovvero non cresca indefinitamente), si richiede che il suo valore assoluto sia minore di 1:
        \begin{equation*}
            |g| < 1 \quad \Rightarrow \quad |1 - h\lambda| < 1
        \end{equation*}
        Per definizione di valore assoluto, questo implica:
        \begin{equation*}
            -1 < 1 - h\lambda < 1
        \end{equation*}
        L'obbiettivo è isolare $h \lambda$. Per cui si deve come primo passo rimuovere il $1 - \dots$. Per fare questo, si sottrae $1$ in tutte e due le disuguaglianze:
        \begin{equation*}
            \left(-1\right) + \left(-1\right) < \left(1 - h\lambda\right) - 1 < 1 + \left(- 1\right) \quad \Rightarrow \quad -2 < - h\lambda < 0
        \end{equation*}
        A questo punto, si moltiplica per $-1$ (cambiando il verso delle disuguaglianze):
        \begin{equation*}
            2 > h\lambda > 0 \quad \Rightarrow \quad 0 < h\lambda < 2
        \end{equation*}
        Infine, dividendo per $\lambda$ (positivo, quindi non cambia il verso):
        \begin{equation*}
            \dfrac{1}{\lambda} \cdot 0 < \dfrac{1}{\lambda} \cdot h\lambda < \dfrac{1}{\lambda} \cdot 2 \quad \Rightarrow \quad 0 < h < \dfrac{2}{\lambda}
        \end{equation*}
        Quindi, la condizione di stabilità per il metodo di Eulero in avanti è:
        \begin{equation*}
            0 < h < \dfrac{2}{\lambda}
        \end{equation*}
        Dove $h$ è il passo scelto per l'iterazione e $\lambda$ è il parametro della funzione $f(t, y) = -\lambda y$.
    \end{enumerate}
\end{itemize}