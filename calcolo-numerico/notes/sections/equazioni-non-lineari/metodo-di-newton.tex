\subsection{Il metodo di Newton}

Il \definition{metodo di Newton} sfrutta la funzione $f$ maggiormente rispetto al metodo di bisezione, usando i suoi valori e la sua derivata.

\highspace
Si ricorda che la retta tangente alla curva $\left(x, f\left(x\right)\right)$ nel punto $x^{(k)}$ è:
\begin{equation*}
    y\left(x\right) = f\left(x^{(k)}\right) + f'\left(x^{(k)}\right)\left(x-x^{(k)}\right)
\end{equation*}
\textbf{Cercando} un $x^{\left(k+1\right)}$ tale che la \textbf{retta tangente in quel punto sia uguale a zero} $y\left(x^{\left(k+1\right)}\right) = 0$, allora si trova:
\begin{equation}\label{eq: metodo di Newton}
    x^{\left(k+1\right)} = x^{\left(k\right)} - \dfrac{f\left(x^{\left(k\right)}\right)}{f'\left(x^{\left(k\right)}\right)} \hspace{2em} k \ge 0
\end{equation}
Purché la derivata prima nel punto $x^{(k)}$ sia diversa da zero, cioè $f'\left(x^{k}\right) \ne 0$.

\highspace
Questa equazione consente di calcolare una successione di valori $x^{(k)}$ a partire da un dato iniziale $x^{(0)}$. In altre parole, il \textbf{metodo di Newton calcola lo zero di $f$ sostituendo localmente a $f$ la sua retta tangente}.

\highspace
A differenza del metodo di bisezione, tale \textbf{metodo converge allo zero in un solo passo quando la funzione $f$ è lineare}, ovvero nella forma $f\left(x\right) = a_{1}x + a_{0}$.

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Limitazione}}
\end{flushleft}
La \textbf{convergenza} del metodo di Newton \underline{non} è garantita \textbf{per ogni scelta} di $x^{(0)}$, ma \textbf{soltanto} per valori di $x^{(0)}$ \textbf{sufficientemente vicini} ad $\alpha$, ovvero \textbf{appartenenti ad un intorno} $I\left(\alpha\right)$ sufficientemente piccolo di $\alpha$.

\highspace
Alcune osservazioni a seguito anche di questa limitazione:
\begin{itemize}
    \item A seguito di questa limitazione, risulta evidente che se $x^{(0)}$ è stato scelto opportunamente e se lo zero $\alpha$ è semplice ($f'\left(\alpha\right) \ne 0$), allora il metodo converge.
    
    \item Nel caso in cui $f$ è derivabile con continuità pari a due, allora si ottiene la seguente convergenza:
    \begin{equation}
        \displaystyle \lim_{k \rightarrow \infty} \dfrac{x^{\left(k+1\right) - \alpha}}{\left(x^{(k)} - \alpha\right)^{2}} = \dfrac{f''\left(\alpha\right)}{2f'\left(\alpha\right)}
    \end{equation}
    Il significato è: se $f'\left(\alpha\right) \ne 0$ il metodo di Newton converge almeno quadraticamente o con ordine 2.

    In parole povere, \textbf{per $k$ sufficientemente grande, l'errore al passo $\left(k+1\right)$-esimo si comporta come il quadrato dell'errore al passo $k$-esimo, moltiplicato per una costante indipendente da $k$}.

    \item Se lo zero $\alpha$ ha molteplicità $m$ maggiore di $1$, ovverosia:
    \begin{equation*}
        f'\left(\alpha\right) = 0, \dots, f^{\left(m-1\right)}\left(\alpha\right) = 0
    \end{equation*}
    Allora il metodo di Newton è ancora convergente, purché $x^{(0)}$ sia scelto opportunamente e $f'\left(x\right) \ne 0$ $\forall x \in I\left(\alpha\right) \setminus \left\{\alpha\right\}$. Tuttavia in questo caso l'ordine di convergenza è pari a 1. In tal caso, l'ordine 2 può essere ancora recuperato usando la seguente relazione al posto dell'equazione~\ref{eq: metodo di Newton} ufficiale:
    \begin{equation}
        x^{\left(k+1\right)} = x^{(k)} - m \cdot \dfrac{f\left(x^{(k)}\right)}{f'\left(x^{(k)}\right)} \hspace{2em} k \ge 0
    \end{equation}
    Purché $f'\left(x^{(k)}\right) \ne 0$. Naturalmente, questo \textbf{metodo di Newton modificato} richiede una conoscenza a priori di $m$.
\end{itemize}

\longline

\subsubsection{Come arrestare il metodo di Newton}

Data una tolleranza fissa $\varepsilon$, esistono due tecniche applicabili per capire quando è necessario fermarsi ed evitare di continuare ad iterare:
\begin{itemize}
    \item La \definition{differenza fra due iterate consecutive}, il quale si arresta in corrispondenza del più piccolo intero $k_{\min}$ per il quale:
    \begin{equation}
        \left| x^{\left(k_{\min}\right)} - x^{\left(k_{\min} - 1\right)} \right| < \varepsilon
    \end{equation}
    (test sull'incremento).

    \item Un'altra tecnica applicata anche per altri metodi iterativi è il \definition{residuo} al passo $k$, il quale è definito come:
    \begin{equation*}
        r^{(k)} = f\left(x^{(k)}\right)
    \end{equation*}
    Che è nullo quando $x^{(k)}$ è uno zero di $f$. In questo modo, il metodo viene arrestato alla prima iterata $k_{\min}$:
    \begin{equation}
        \left| r^{\left(k_{\min}\right)} \right| = \left| f\left(x^{\left(k_{\min}\right)}\right) \right| < \varepsilon
    \end{equation}
    Da notare che tale tecnica fornisce una \textbf{stima accurata dell'errore} \underline{solo quando} $\left| f'\left(x\right) \right|$ è circa pari a $1$ in un intorno di $I_{\alpha}$ dello zero $\alpha$ cercato.

    \underline{\textbf{Attenzione!}} Se la derivata non è circa pari a $1$ in un intorno dello zero cercato, la tecnica porterà:
    \begin{itemize}
        \item Ad una \textbf{sovrastima} dell'errore se $\left| f'\left(x\right) \right| \gg 1$ per $x \in I_{\alpha}$
        \item Ad una \textbf{sottostima} dell'errore se $\left| f'\left(x\right) \right| \ll 1$ per $x \in I_{\alpha}$
    \end{itemize}
\end{itemize}