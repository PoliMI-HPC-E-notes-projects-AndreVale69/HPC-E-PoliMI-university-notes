\subsection{2023}

\subsubsection{July 7}

\subsubsection*{True (\trueIcon) false (\falseIcon) questions}

\begin{enumerate}
    \item \falseIcon \: Hot/cold aisle containment reduces the efficiency of the cooling method for datacenters.
    \item \trueIcon \: Edge computing is not a suitable alternative to cloud computing for applications that require large-scale data processing.
    \begin{deepeningbox}[: \emph{Why is this True?}]
        \textbf{Edge computing} pushes computation \emph{closer} to the data source, like IoT devices, local gateways, smart sensors. The main goals are \textbf{low latency}, \textbf{reduced bandwidth usage}, and \textbf{local immediate processing}.

        \highspace
        \underline{BUT} edge devices are resource-constrained: limited CPU, memory, storage. \textbf{Large-scale data processing} (like training huge ML models, massive search indexing, or large analytics) \textbf{needs the aggregated compute power} of datacenter-scale clusters, which edge nodes alone cannot deliver efficiently.

        \highspace
        So, if our workload is \textbf{heavy data crunching}, we still need \textbf{cloud computing / warehouse-scale computing}, with big clusters, specialized accelerators (GPUs, TPUs), high-throughput storage, large memory pools.
    \end{deepeningbox}
    \item \trueIcon \: RAID 0 provides the best read and write performance of any RAID level composed of the same amount of disks.
    \item \falseIcon \: Datacenters within the same Geographic Areas share the same power and network infrastructure.
    \begin{deepeningbox}[: \emph{Why is this False?}]
        Main rule:
        \begin{equation*}
            \text{Physically Nearby} \ne \text{Operationally Linked}
        \end{equation*}
        Just because two datacenters are in the same city or region doesn't mean they literally share the same power grid \textbf{distribution} or the same \textbf{network backbone} \emph{inside} the facility.

        \highspace
        The isolation is intentional. Large operators (Google, AWS, Azure) deliberately design datacenters with \textbf{redundancy} and \textbf{fault domains}:
        \begin{itemize}
            \item Different substations.
            \item Independent fiber routes.
            \item Diverse network peers.
            \item Separate cooling \& power feeds.
        \end{itemize}
        If one power supply or fiber route fails, the other datacenter is unaffected, this \textbf{improves availability} and disaster tolerance.

        \highspace
        Another important main rule:
        \begin{equation*}
            \text{Geo-Redundancy} \ne \text{Shared Infrastructure}
        \end{equation*}
        Multiple datacenters in a region might be \emph{replicas} for lower latency or local failover, but they are separate buildings with separate utility connections to avoid \textbf{single points of failure}.
    \end{deepeningbox}
    \item \falseIcon \: TOR switches are typically used in small-scale datacenter deployments and are not suitable for larger networks.
    \item \trueIcon \: Virtualization can help reduce energy consumption by consolidating multiple physical machines into fewer physical ones.
    \item \falseIcon \: POD, Virtual-Chassis, DCell and BCube models are all evolutions of the leaf-spine network architectures.
    \begin{deepeningbox}[: \emph{Why is this False?}]
        Because DCell and BCube are \textbf{not} evolutions of leaf-spine, they are fundamentally \textbf{different architectural families}:
        \begin{itemize}
            \item (base) \textbf{Leaf-Spine} is a two-level hierarchical system.
            \begin{itemize}
                \item \textbf{Leaf (access)} switches connect servers.
                \item \textbf{Spine (aggregation/core)} switches connect all leafs.
            \end{itemize}
            Any leaf can reach any other leaf with equal cost. It is simple, flat, and have an high bisection bandwidth. Typical for modern cloud datacenters and big clusters.

            \item (correct \textcolor{Green3}{\faIcon{check}}) \textbf{POD}: for example, in traditional Cisco architectures, a ``POD'' is a \emph{modular block} that \emph{can} be built with Leaf-Spine. So PODs can align with Leaf-Spine.

            \item (correct \textcolor{Green3}{\faIcon{check}}) \textbf{Virtual-Chassis}: Vendor feature, where multiple switches act like one logical switch, it fits within Leaf-Spine designs for scaling port counts and management.

            \item (incorrect \textcolor{Red2}{\faIcon{times}}) \textbf{DCell} and \textbf{BCube}. Here's why the answer is incorrect. These come from Microsoft Research (and Huawei) for \emph{modular}, \emph{server-centric} topologies.
            \begin{itemize}
                \item \textbf{DCell}: Recursive server-based hierarchy. Servers act as mini-switches $\rightarrow$ multi-hop paths $\rightarrow$ scalable, fault-tolerant.
                \item \textbf{BCube}: Server-centric, designed for modular containers. Each server has multiple NICs and connects to multiple mini-switches $\rightarrow$ many alternative paths.
            \end{itemize}
            So \textbf{both break the typical Leaf-Spine model}, instead of a simple two-level switch hierarchy, they \emph{distribute} switching responsibilities to servers and small switches. They're \emph{peer-to-peer inspired}, not classic core/aggregation.
        \end{itemize}
    \end{deepeningbox}
    \item \falseIcon \: Type 2 hypervisors run on top of a guest operating system that provides indirect access to hardware resources.
    \begin{deepeningbox}[: \emph{Why is this False?}]
        The confusion is about host OS vs guest OS. A \textbf{Type 2 hypervisor} is called hosted because it runs \emph{on top of an existing Host Operating System} (e.g., Oracle VirtualBox). The \textbf{host OS} is the \textbf{real operating system installed on the physical machine} (Windows, Linux, macOS). The \textbf{Type 2 hypervisor} is just an application running on that host OS, it uses the host OS's drivers to access CPU, memory, disk, network. Then the hypervisor runs \textbf{guest OSes} inside virtual machines, then the VMs see \emph{virtual} hardware.
        \begin{equation*}
            \text{Physical hardware} \rightarrow \text{Host OS} \rightarrow \text{Type 2 hypervisor} \rightarrow \text{Guest OS}
        \end{equation*}
        So the hypervisor runs \textbf{on the host OS}, not on a guest OS. The \textbf{guest OS} runs inside the hypervisor.
    \end{deepeningbox}
    \item \trueIcon \: In-row cooling systems allow for higher server densities and increased rack power densities in datacenters.
    \item \falseIcon \: DAS (Direct Attached Storage) is a storage system directly attached to the datacenter network.
    \begin{deepeningbox}[: \emph{Why is this False?}]
        Because DAS is not connected to the datacenter network, it's \textbf{directly attached to a specific server} (see page \pageref{paragraph: DAS, NAS and SAN}). DAS (Direct Attached Storage) means the storage (HDDs, SSDs) is connected \textbf{directly to a single server}, through SATA, SAS, PICe, or other local buses. There's \textbf{no network in between}, no shared storage fabric (like SAN or NAS). Other servers cannot directly access that storage unless the host server exports it via network protocols.

        \highspace
        Instead, NAS (Network Attached Storage) is a storage connected to the datacenter network via Ethernet, clients access files via NFS/SMB. Finally, SAN (Storage Area Network) is a dedicated network for block storage (e.g., Fibre Channel).
    \end{deepeningbox}
\end{enumerate}

\subsubsection*{Exercises}

\begin{enumerate}
    \setcounter{enumi}{10}

    \item Let us consider a set of requests in the disk queue referring to the following cylinders of the disk: 57, 32, 46, 83, 12. Consider the initial position of the disk head at cylinder 54 and moving from inside (lower cylinder number) to outside (higher cylinder number). Writes the order of the served requests (from the Ô¨Årst to the last) if the disk head scheduling algorithm adopted is C-SCAN (Circular SCAN)? Use the cylinder number to refer to the request.

    \textcolor{Green3}{\textbf{\emph{Answer:}}} C-SCAN (Circular SCAN) is a disk scheduling algorithm (see page \pageref{def: C-SCAN - Elevator Algorithm}). It works like the SCAN, but when it reaches the end, it jumps back to the beginning, like a circular elevator.

    The exercise text gives us a hint about the original movement: ``\texttt{[...]} from inside (lower cylinder number) to outside (higher cylinder number) \texttt{[...]}''. Therefore, we know that the next value will be 57:
    \begin{equation*}
        \left(54 \to\right) \, \mathbf{57} \to \mathbf{83} \xrightarrow{\text{end reached, jump back}} \left(\to 0\right) \to \mathbf{12} \to \mathbf{32} \to \mathbf{46}
    \end{equation*}

    \item We have to design a RAID 5 storage architecture composed of an array of 8 disks. Knowing that we cannot guarantee an average MTTR lower than 6 hours and that we would like to have a MTTF for the storage infrastructure ($\text{MTTF}_{\text{RAID5}}$) higher than 15 years, what is the minimum MTTF that we should consider for each disk? Consider all the disks with the same characteristics.

    \textcolor{Green3}{\textbf{\emph{Answer:}}} We can calculate the desired MTTF for each disk using the MTTF of the RAID 5 (see page \pageref{eq: RAID 5 - MTTF}):
    \begin{equation*}
        \begin{array}{rcl}
            \text{MTTF}_{\text{RAID5}} &=& \dfrac{\text{MTTF}_{\text{disk}}^{2}}{G\left(G-1\right)\cdot \text{MTTR}} \\ [1.5em]
            \left(15 \times 365 \times 24 \right) \, \text{hours} &=& \dfrac{\text{MTTF}_{\text{disk}}^{2}}{8\left(8-1\right)\cdot 6 \, \text{hours}} \\ [1.5em]
            131'400 \, \text{hours} &=& \dfrac{\text{MTTF}_{\text{disk}}^{2}}{56 \cdot 6 \, \text{hours}} \\ [1.5em]
            336 \, \text{hours} \cdot 131'400 \, \text{hours} &=& \dfrac{\text{MTTF}_{\text{disk}}^{2}}{\cancel{336 \, \text{hours}}} \cdot \cancel{336 \, \text{hours}} \\ [1.5em]
            44'150'400 \, \text{hours} &=& \text{MTTF}_{\text{disk}}^{2} \\ [.5em]
            \text{MTTF}_{\text{disk}} &=& \sqrt{44'150'400 \, \text{hours}} \\ [.5em]
            &=& 6644.576735955 \, \text{hours} \\ [.5em]
            &=& \left(6644.576735955 \div 24\right) \, \text{days} \\ [.5em]
            &=& \mathbf{276.8}57363998 \, \text{days}
        \end{array}
    \end{equation*}

    \newpage

    \item A sensing system demonstrates an average availability equal to 0.98. The system is composed of a single device that needs to be substituted when fails. Knowing that its MTTR is equal to 3 hours, what will be the reliability of a new deployed device after 4 days?
    
    \textcolor{Green3}{\textbf{\emph{Answer:}}} Here is a recap of the data provided by the exercise:
    \begin{itemize}
        \item Availability $A = 0.98$
        \item Mean Time To Repair $\text{MTTR} = 3$ hours
        \item Time $t = 4$ days $= 96$ hours
    \end{itemize}
    The reliability formula is:
    \begin{equation*}
        R\left(t\right) = e^{-\lambda t} \quad \text{where } \lambda = \dfrac{1}{\text{MTTF}}
    \end{equation*}
    So, we need to find the Mean Time To Failure (MTTF). We can use the Availability formula as a little trick to find the unknown variable, MTTF:
    \begin{equation*}
        \begin{array}{rcl}
            A &=& \dfrac{\text{MTTF}}{\text{MTTF} + \text{MTTR}} \\ [1.5em]
            \left(\text{MTTF} + 3\right) \cdot 0.98 &=& \dfrac{\text{MTTF}}{\cancel{\text{MTTF} + 3}} \cdot \cancel{\left(\text{MTTF} + 3\right)} \\ [1.5em]
            0.98 \, \text{MTTF} + 2.94 &=& \text{MTTF} \\ [.5em]
            0.98 \, \text{MTTF} - \text{MTTF} &=& - 2.94 \\ [.5em]
            \text{MTTF} \cdot \left(0.98-1\right) &=& - 2.94 \\ [.5em]
            \dfrac{1}{\cancel{\left(0.98-1\right)}} \cdot \text{MTTF} \cdot \cancel{\left(0.98-1\right)} &=& - 2.94 \cdot \dfrac{1}{\left(0.98-1\right)} \\ [1.5em]
            \text{MTTF} &=& \dfrac{-2.94}{-0.02} \\ [1.3em]
            &=& \dfrac{2.94}{0.02} \\ [1.3em]
            &=& 147 \, \text{hours}
        \end{array}
    \end{equation*}
    Thanks to MTTF, we have all the information necessary to calculate reliability:
    \begin{equation*}
        \begin{array}{rcl}
            R\left(t\right) &=& e^{-\lambda t} \\ [1em]
            R\left(96\right) &=& e^{-\frac{1}{147} \cdot 96} \\ [1em]
            &=& e^{-0.653061224} \\ [1em]
            &=& \mathbf{0.52}0450121
        \end{array}
    \end{equation*}
    So the probability that a new deployed device \emph{does not fail} in 4 days is about $\approx 52\%$.

    \newpage

    \item Consider a queuing system with two stations. We have the following information about the system:
    \begin{itemize}
        \item station 1 response time: 5 s
        \item station 2 response time: 3 s
        \item station 1 throughput: 3 transactions/s
        \item station 2 throughput: 7 transactions/s
        \item system throughput: 4 transactions/s
    \end{itemize}
    Which is the average response time of the system?
    
    \textcolor{Green3}{\textbf{\emph{Answer:}}} Jobs flow through the system. Each job visits each station some average number of times. The total response time is the sum of all the time spent at all the stations per job. But, we don't have the \textbf{visit counts} directly, instead we have each station's throughput $X_{i}$ and the system throughput $X$. And that tells us how many times a job on average goes through each station.

    \highspace
    The visit ratio for station $i$ is (see page \pageref{eq: Visit Count}):
    \begin{equation*}
        V_{i} = \dfrac{X_{i}}{X}
    \end{equation*}
    This tells us, on average, each job causes how many operations at that station.

    \highspace
    The time \emph{per visit} is given (5s, 3s). So, the total time spent at that station per job is:
    \begin{equation*}
        \text{Total time at station i} = V_i \times R_i
    \end{equation*}
    So:
    \begin{equation*}
        \begin{array}{rcl}
            R &=& \displaystyle \sum_{i=1}^{n = 2} R_i \cdot \frac{X_i}{X} \\ [1.5em]
            &=& \underbrace{5 \cdot \dfrac{3}{4}}_{\text{station 1}} + \underbrace{3 \cdot \dfrac{7}{4}}_{\text{station 2}} \\ [2.5em]
            &=& 3.75 + 5.25 \\ [.5em]
            &=& 9 \, \text{seconds}
        \end{array}
    \end{equation*}

    \newpage

    \item Your data science team develops deep learning models by relying on a server including 8 CPUs, one GPU, and one SSD disk. Your team includes $N = 10$ users who submit training jobs with a Think Time $Z = 1$ hour. Every job uses all the available CPUs. Assuming that your server demands are the following:
    \begin{itemize}
        \item $D_{\text{CPU}^{i}} = 2$ min $\forall i \in \left\{1 \dots 8\right\}$. i.e. each CPU has a service demand equal to 2 min.
        \item $D_{\text{GPU}} = 12$ min
        \item $D_{\text{Disk}} = 3$ min
    \end{itemize}
    In the context of the asymptotic bounds, what is the system throughput upper bound and response time lower bound?

    \textcolor{Green3}{\textbf{\emph{Answer:}}} The data of the exercises are:
    \begin{itemize}
        \item $D_{\text{CPU}^{i}} = 2$ min
        \item $D_{\text{GPU}} = 12$ min
        \item $D_{\text{Disk}} = 3$ min
        \item Think time $Z = 1$ hour $= 60$ min
    \end{itemize}
    Before applying the throughput formula, it is necessary to understand whether the system has a heavy or light load in order to apply an optimistic or pessimistic view. We use $N^{*}$ to determine whether to use the light load bound ($N < N^{*}$) or the heavy load bound ($N > N^{*}$, see page \pageref{eq: N start - limit population size}):
    \begin{equation*}
        N^{*} = \dfrac{D+Z}{D_{\max}} = \dfrac{17 + 60}{12} = 6.416666667 \approx 6.42
    \end{equation*}
    Where $D$ is the total service demand per cycle:
    \begin{equation*}
        D = \displaystyle\sum_{k = 1}^{K} D_{k} = D_{\text{CPU}} + D_{\text{GPU}} + D_{\text{Disk}}
    \end{equation*}
    And the $D_{\max}$ is the $D_{\text{GPU}}$. We use the critical path (parallel) to calculate $N^{*}$, because $N^{*}$ identifies which single resource becomes saturated first. This occurs along the longest delay path in the network, which is the real elapsed time it takes one job to pass through all necessary stages while taking full parallelism into account.
    \begin{equation*}
        N > N^{*} \quad \Rightarrow \quad 10 > 6.42 \quad \Rightarrow \quad \textbf{use heavy load bound}
    \end{equation*}
    In a \textbf{heavy load situation}, we use the critical path as $D$ because we're analyzing which resource will saturate first under high load. So, the throughput bound is (see page \pageref{eq: throughput limits - heavy load}):
    \begin{equation*}
        \dfrac{N}{N \cdot D + Z} \le X(N) \le \min\left(\dfrac{1}{D_{\max}}, \dfrac{N}{D+Z}\right)
    \end{equation*}
    Since we are searching for the upper bound:
    \begin{equation*}
        \begin{array}{rcl}
            X_{\max} &=& \min\left(\dfrac{1}{D_{\max}},\, \dfrac{N}{D+Z}\right) \\ [1.5em]
            &=& \min\left(\dfrac{1}{12},\, \dfrac{10}{17 + 60}\right) \\ [1.5em]
            &=& \min\left(0.083333333,\, 0.12987013\right) \\ [.5em]
            &=& 0.083333333\, \text{min} \\ [.5em]
            &=& \left(0.083333333 \cdot 60\right) \, \text{jobs/hour} \\ [.5em]
            &=& \mathbf{5} \, \textbf{jobs/hour}
        \end{array}
    \end{equation*}
    Where:
    \begin{itemize}
        \item $D_{\max} = D_{\text{GPU}}$
        \item $D$ is the total service demand per cycle:
        \begin{equation*}
            D = D_{\text{CPU}} + D_{\text{GPU}} + D_{\text{Disk}} = 2 + 12 + 3 = 17 \, \text{min}
        \end{equation*}
    \end{itemize}
    Finally, when under a \textbf{heavy load situation}, the response time is (see page \pageref{eq: response time - heavy load}):
    \begin{equation*}
        \max\left(D, N \cdot D_{\max} - Z\right) \le R\left(N\right) \le N \cdot D
    \end{equation*}
    Since we are searching for the lower bound:
    \begin{equation*}
        \begin{array}{rcl}
            R_{\min} &=& \max\left(D, N \cdot D_{\max} - Z\right) \\ [.5em]
            &=& \max\left(17, \, 10 \cdot 12 - 60\right) \\ [.5em]
            &=& \max\left(17, \, 60\right) \\ [.5em]
            &=& \mathbf{60} \, \textbf{min}
        \end{array}
    \end{equation*}

    \item If in the previous system (Question 15) you add 3 more GPUs (having hence 4 GPUs overall), what is the system throughput upper bound and response time lower bound? Consider that the original workload is evenly split across the entire set of GPUs.
    
    \textcolor{Green3}{\textbf{\emph{Answer:}}} Adding three more GPUs would split the service demand, meaning the original workload would be evenly split across the entire set of GPUs. This would greatly change the problem!

    We move from a heavy load to a light load because $N < N^{*}$:
    \begin{equation*}
        \begin{array}{rcl}
            D &=& D_{\text{CPU}} + D_{\text{GPU}} + D_{\text{Disk}} \\ [.5em]
            &=& 2 + \dfrac{12}{4} + 3 \\ [.5em]
            &=& 2 + 3 + 3 \\ [.5em]
            &=& 8\, \text{min} \\ [1em]
            N^{*} &=& \dfrac{D+Z}{D_{\max}} \\ [1.5em]
            &=& \dfrac{8 + 60}{3} = 22.666666667
        \end{array}
    \end{equation*}
    With a \textbf{light load situation}:
    \begin{itemize}
        \item For each formula, we need to use the $D$ value as the \textbf{total resource consumption} because throughput is limited by the total amount of work done, not wall-clock. This means that each task cannot be counted as parallel but rather sequential. Therefore:
        \begin{equation*}
            \begin{array}{rcl}
                D &=& \displaystyle\sum_{i=1}^{n=8} D_{\text{CPU}_{i}} + \displaystyle\sum_{j=1}^{k=4} D_{\text{GPU}_{j}} + D_{\text{Disk}} \\ [1.5em]
                &=& 16 + 12 + 3 \\ [.5em]
                &=& 31
            \end{array}
        \end{equation*}
        Simply put, when the load is light, the system isn't saturated and jobs flow freely without waiting. Throughput is limited by total resource consumption rather than maximum latency. In our case, 8 CPUs (each working 2 minutes) results in 16 minutes of lost CPU time per job, even though the work is done in parallel. Therefore, we sum all resource times to reflect the true workload capacity needed.
        \item The \textbf{throughput} is (see page \pageref{eq: upper bound for light load}):
        \begin{equation*}
            \begin{array}{rcl}
                X_{\max} &=& \dfrac{N}{\left(D + Z\right)} \\ [1.3em]
                &=& \dfrac{10}{\left(31 + 60\right)} \\ [1.3em]
                &=& \dfrac{10}{\left(91\right)} \\ [1.3em]
                &=& 0.10989011 \, \text{jobs/min} \\ [.5em]
                &=& \left(0.10989011 \times 60\right) \, \text{jobs/hour} \\ [.5em]
                &=& 6.593406593 \approx \mathbf{6.6} \, \textbf{jobs/hour}
            \end{array}
        \end{equation*}
        \item The \textbf{response time} is:
        \begin{equation*}
            R_{\min} = D = \mathbf{31} \, \textbf{min}
        \end{equation*}
    \end{itemize}
\end{enumerate}

\newpage

\subsubsection*{Open Questions}

\begin{enumerate}
    \setcounter{enumi}{16}

    \item Discuss the main differences between para-virtualization and full-virtuali-\break zation.
    
    \textcolor{Green3}{\textbf{\emph{Answer:}}} Full virtualization is a technique that provides a complete simulation of the underlying hardware. In contrast, para-virtualization modifies the operating system (OS) to use hypercalls instead of certain instructions. This is the main difference. For this reason, paravirtualization performs better than full virtualization. However, full virtualization provides complete isolation, though it is not permitted on all architectures. Finally, para-virtualization requires modification of the guest OS.

    The trade-off is that para-virtualization needs guest OS source code access, which is possible for open-source OSes (like Linux), but not for proprietary ones (like Windows). Also, full virtualization typically provides stronger isolation and can run a wider range of unmodified operating systems.

    \item What are the adopted strategies for efficient cooling of data center infrastructures targeting highly computational demanding applications, such as HPC and deep-learning workloads?
    
    \textcolor{Green3}{\textbf{\emph{Answer:}}} During the CI course, we studied three main cooling strategies.
    \begin{enumerate}
        \item \definition{Hot/Cold Aisle Containment} is a physical containment of airflow to separate cold supply and hot exhaust air, maximizing cooling efficiency for dense racks.
        \item \textbf{In-Rack and In-Row Cooling} are local cooling solutions that remove heat close to the servers, reducing recirculation and hotspots, used especially for high-density or high-performance clusters.
        \item \textbf{Liquid Cooling} is a direct liquid cooling of servers/components (e.g., cold plates for CPUs/GPUs, or immersion cooling). It is essential when air cooling is no longer enough for extreme heat loads like dense GPUs for AI training.
    \end{enumerate}
\end{enumerate}