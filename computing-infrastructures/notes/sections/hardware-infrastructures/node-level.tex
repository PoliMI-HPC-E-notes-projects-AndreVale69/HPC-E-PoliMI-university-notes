\subsection{Node-level}

\subsubsection{Server (computation, HW accelerators)}\label{subsubsection: Server (computation, HW accelerators)}

A \definition{Server} is a computing system designed to manage, process, and deliver data or services to other computers (clients) over a network. In the context of datacenters and Warehouse-Scale Computers (WSCs), servers are the \textbf{atomic units of computation}, the fundamental building blocks of the entire system architecture.

\highspace
Though \textbf{conceptually similar to a desktop PC}, servers \textbf{differ} in critical ways:
\begin{itemize}
    \item They are \textbf{significantly more powerful}, scalable, and modular.
    \item They are designed for \textbf{continuous operation}, high availability, and\break \textbf{dense physical packaging} within a rack structure.
\end{itemize}
Servers in modern datacenters must balance \textbf{performance}, \textbf{density}, \textbf{power efficiency}, and \textbf{maintainability}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{Server Types}}
\end{flushleft}
\textbf{Server form} factors \textbf{define how} servers are \textbf{physically organized} and \textbf{deployed} in datacenter environments. There are three principal types:
\begin{enumerate}
    \item \definition{Tower Server}s (section \ref{paragraph: Tower Server}, page \pageref{paragraph: Tower Server}). Resemble traditional desktop PCs. They are ideal for small-scale deployments or low-density use cases.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}: Easy to upgrade, good cooling, low cost.
        \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}: Large footprint, not optimized for rack deployment.
    \end{itemize}

    \item \definition{Rack Server}s (section \ref{paragraph: Rack Servers}, page \pageref{paragraph: Rack Servers}). Designed to slide \textbf{into a rack} (standardized shelves) in units (U); e.g., 1U = 1.75 inches. It is the most common server format in datacenters.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}: High compute density, easy to cable and scale.
        \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}: Requires dedicated infrastructure (rack, cooling, power).
    \end{itemize}
    
    \item \definition{Blade Server}s (section \ref{paragraph: Blade Servers}, page \pageref{paragraph: Blade Servers}). Extremely compact and multiple blades share power, cooling, and networking through a \textbf{blade enclosure}. It is excellent for environments where space and energy are at a premium.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}: Highest density and modularity, centralized management.
        \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}: Higher initial cost, vendor lock-in, increased heat density.
    \end{itemize}
\end{enumerate}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Server Architecture}}
\end{flushleft}
Servers are typically \textbf{integrated into a tray or blade enclosure}, which contains:
\begin{itemize}
    \item \important{Motherboard}: The central PCB that \textbf{interconnects all components}.
    \item \important{Chipset}: \textbf{Manages data flow} between CPU, RAM, storage, and peripherals.
    \item \important{Expansion slots}: For GPUs, network cards, and other \textbf{accelerators}.
\end{itemize}
Servers in WSCs tend to \textbf{use homogeneous hardware/software platforms} to simplify large-scale orchestration and maintenance.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{microchip} \textbf{Server Architecture}}
\end{flushleft}
The \definition{Motherboard} acts as a \textbf{central nervous system for the server}, it hosts:
\begin{itemize}
    \item \textbf{CPU sockets} (e.g., up to 2 for dual Xeon systems)
    \item \textbf{DIMM slots} for RAM
    \item \textbf{Storage connectors} (e.g., SATA, NVMe)
    \item \textbf{NIC slots} (Network Interface Cards)
\end{itemize}
This level of configurability allows tailoring servers for compute-heavy, memory-bound, or I/O-intensive applications.

\newpage

\paragraph{Tower Server}\label{paragraph: Tower Server}

A \definition{Tower Server} is a type of server designed in a vertical, standalone chassis that closely resembles a \textbf{standard tower desktop computer}. Unlike blade or rack servers, which are designed for high-density environments, tower servers prioritize \textbf{simplicity and accessibility}, often at the cost of physical footprint.
\begin{itemize}
    \item \important{Structure}: Independent, \textbf{vertical} case (not meant for rack mounting).
    \item \important{Deployment}: Common in small businesses, branch offices, or settings where only a few servers are needed.
    \item \important{Internal layout}: Lots of \textbf{space for expansion components} like disks or PCIe cards.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Scalability \& Ease of Upgrade}}. \textbf{Easy} to open and \textbf{upgrade}, users can add storage, memory, or cards as needed.
    \item \textcolor{Green3}{\textbf{Cost-Effective}}. Usually the \textbf{cheapest server type}, suitable for budget-constrained environments.
    \item \textcolor{Green3}{\textbf{Easy Cooling}}. Due to \textbf{low component density}, natural airflow is often sufficient. Less need for specialized cooling systems.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{times-circle} \textbf{Limitations}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textcolor{Red2}{\textbf{Space Consumption}} Tower servers consume \textbf{significant physical\break space} and don't scale well in quantity.
    \item \textcolor{Red2}{\textbf{Basic Performance}} They usually \textbf{offer lower performance and redundancy} compared to enterprise-grade rack or blade servers.
    \item \textcolor{Red2}{\textbf{Cable Management}} Not ideal for structured environments, cables can become messy and hard to manage.
\end{itemize}

\newpage

\paragraph{Rack Servers}\label{paragraph: Rack Servers}

A \definition{Rack Server} is a server built specifically to be \textbf{mounted vertically in standardized racks}, which are metallic shelves designed to hold multiple servers and IT components. Rack servers are the \textbf{default choice} in medium to large-scale datacenters, balancing compute density, modularity, and serviceability.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Physical Standardization}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Servers are stored in racks} which follow a global standard:
    \begin{itemize}
        \item \textbf{1U (\definition{Rack Unit})} = 1.75 inches (44.45 mm) in height.
        \item Servers may come in 1U, 2U, 4U, up to 10U formats depending on power and component density.
    \end{itemize}

    \item Racks also house other components: networking switches, storage arrays, power distribution units (PDUs), and cooling units.
\end{itemize}
This \textbf{standardization allows for efficient vertical stacking} of servers, optimizing physical space and simplifying cabling.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Racks as More Than Just Shelves}}
\end{flushleft}
A rack is not just a mechanical holder, it is \textbf{part of the power, networking, and management infrastructure of the datacenter}:
\begin{itemize}
    \item \important{Power Infrastructure}:
    \begin{itemize}
        \item Shared power distribution units.
        \item Battery backup (UPS).
        \item Power conversion units.
    \end{itemize}
    
    \item \important{Networking}:
    \begin{itemize}
        \item Top of Rack (ToR) switches connect all servers in the rack to the datacenter network fabric.
        \item Simplifies cabling and reduces latency.
    \end{itemize}
  
    \item \important{Cooling}: designed for front-to-back airflow, aligned with datacenter cooling strategy (e.g., cold aisle containment).
  
    \item \important{Dimensions} can vary, but the classic rack is 19 inches wide and up to 48 inches deep.
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Modularity}}: Individual servers can be \textbf{hot-swapped}, upgraded, or replaced \textbf{without disrupting others}.
    \item \textcolor{Green3}{\textbf{Failure Containment}}: Easy to \textbf{isolate and service a failed node} without bringing down the system.
    \item \textcolor{Green3}{\textbf{Cable Management}}: Organized by rear/backplanes or Top-of-Rack\break (ToR) switches.
    \item \textcolor{Green3}{\textbf{Cost-Efficient Scaling}}: \textbf{Scales vertically} at relatively lower incremental cost compared to other formats.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{times-circle} \textbf{Challenges}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textcolor{Red2}{\textbf{High Power Demand}}: \textbf{Higher component density} requires more energy and advanced cooling systems.
    \item \textcolor{Red2}{\textbf{Thermal Hotspots}}: Tight stacking can cause \textbf{hot zones}, especially with accelerator-heavy nodes.
    \item \textcolor{Red2}{\textbf{Maintenance Overhead}}: Large racks with tens of servers can become \textbf{complex to manage} physically as systems scale.
\end{itemize}

\newpage

\paragraph{Blade Servers}\label{paragraph: Blade Servers}

\definition{Blade Server}s represent the \textbf{most advanced evolution} in server form factors. They are designed to \textbf{maximize space efficiency} and \textbf{centralized manageability}, making them ideal for \textbf{large-scale enterprise datacenters} and \textbf{high-performance computing environments}.

\highspace
A blade server is essentially a \textbf{stripped-down}, \textbf{ultra-thin server board} (the ``blade'') that fits into a blade enclosure, a shared chassis providing:
\begin{multicols}{2}
    \begin{itemize}
        \item Power
        \item Cooling
        \item Networking
        \item Centralized management
    \end{itemize}
\end{multicols}
\noindent
The enclosure conforms to the same \textbf{rack unit standard} (U), allowing it to integrate seamlessly with existing rack infrastructure. We can think of a blade system as a server equivalent of a modular bookshelf, where each ``book'' is a full server, and the ``bookshelf'' provides shared power, ventilation, and data connectivity.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Compactness \& Density}}: The \textbf{smallest physical form factor} among all servers, allowing high-density deployments within a minimal footprint.
    \item \textcolor{Green3}{\textbf{Minimal Cabling}}: The \textbf{shared backplane} removes the need for complex cabling; power and network connections are centralized.
    \item \textcolor{Green3}{\textbf{Centralized Management}}: Blade systems typically include \textbf{unified management interfaces} (e.g., iLO, iDRAC) to monitor and configure blades collectively.
    \item \textcolor{Green3}{\textbf{Scalability \& Reliability}}: New \textbf{blades can be added} with minimal disruption; enclosures support \textbf{load balancing} and \textbf{failover mechanisms}.
    \item \textcolor{Green3}{\textbf{Uniform Infrastructure}}: Simplifies deployment with \textbf{shared cooling}, \textbf{network fabrics}, and \textbf{power redundancy}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{times-circle} \textbf{Disadvantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textcolor{Red2}{\textbf{High Initial Cost}}: Blade enclosures and vendor-specific blades often demand \textbf{significant upfront investment}.
    \item \textcolor{Red2}{\textbf{Vendor Lock-In}}: Typically, only blades from the \textbf{same manufacturer} (e.g., HPE, Dell, Cisco) \textbf{are compatible} with a given enclosure.
    \item \textcolor{Red2}{\textbf{Thermal Density}}: The compact form causes \textbf{higher heat output per rack unit}, requiring advanced HVAC design and monitoring.
    \item \textcolor{Red2}{\textbf{Limited Flexibility}}: While modular, blade systems trade off flexibility for density, upgrades and replacements may be \textbf{constrained by the enclosure's architecture}.
\end{itemize}

\newpage

\paragraph{Machine Learning}

While Moore's Law historically predicted that transistor density would double every 18-24 months, the \textbf{growth in ML model complexity} has surpassed this pace. Since 2013, compute demand for AI training has doubled approximately \textbf{every 3.5 months}. This exponential curve far exceeds the capabilities of general-purpose CPUs, triggering a renaissance in specialized hardware.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is Machine Learning?}}
\end{flushleft}
At its core, \definition{Machine Learning (ML)} refers to \textbf{computational methods} that enable systems to \textbf{learn from data} without being explicitly programmed. Rather than defining rules manually, ML allows a system to build a model from \textbf{patterns observed in examples}.

\highspace
In supervised learning:
\begin{itemize}
    \item A system learns a \textbf{target function} $y = f\left(x\right)$ that \textbf{maps inputs} (features) \textbf{to outputs} (labels).
    \item This is \textbf{done using a training dataset} $\left(x_{1}, y_{1}\right), \dots, \left(x_{N}, y_{N}\right)$, and the model is later tested on unseen inputs.
\end{itemize}
Applications include: classification (e.g., cat vs. dog), regression (e.g., predicting flight delays), image recognition, speech synthesis, fraud detection, etc.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Artificial Neural Networks (ANNs)}}
\end{flushleft}
\definition{Artificial Neural Networks (ANNs)} are a \textbf{subset of ML models} inspired by the human brain. They consist of \textbf{layers of interconnected neurons}, including:
\begin{itemize}
    \item \textbf{Input layer}: receives the data
    \item \textbf{Hidden layers}: transform data using weighted functions and nonlinear activations
    \item \textbf{Output layer}: produces the prediction
\end{itemize}
The key learning mechanisms are:
\begin{itemize}
    \item \textbf{Backpropagation}: adjusts weights based on the error between prediction and actual target.
    \item \textbf{Gradient descent}: optimizes the model parameters iteratively.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Hardware Acceleration: Why ML Needs More Than CPUs}}
\end{flushleft}
Modern ML, particularly \textbf{deep learning}, is computationally expensive. Training models like GPT or ResNet involves processing \textbf{billions of parameters} across massive datasets. To meet these demands, \textbf{Warehouse-Scale Computers (WSCs) integrate specialized accelerators such as}:
\begin{itemize}
    \item \definition{Graphics Processing Units (GPUs)}. GPUs are highly parallel processors originally designed for graphics rendering but are now extensively used for ML because they:
    \begin{itemize}
        \item Execute the \textbf{same operation across many data elements in parallel} (SIMD).
        \item Accelerate matrix operations central to deep learning.
        \item Support ML frameworks via CUDA, OpenCL, OpenMP, SYCL, etc.
    \end{itemize}
    GPUs are often housed in \textbf{PCIe-attached trays}, interconnected via NVLink or NVSwitch for ultra-fast data exchange.
    
    Distributed training across multiple GPUs requires \textbf{low-latency}, \textbf{high-bandwidth interconnects}. Performance can also be bottlenecked by slowest learner or network synchronization delays. 


    \item \definition{Tensor Processing Units (TPUs)}. Developed by Google, TPUs are \textbf{domain-specific architectures} designed \textbf{specifically for ML workloads}. TPU generations:
    \begin{itemize}
        \item \textbf{TPUv1}: Inference-only, connected via PCIe.
        \item \textbf{TPUv2}: Supports both training and inference; includes MXUs (matrix units) and high-bandwidth memory (HBM).
        \item \textbf{TPUv3}: Liquid-cooled, supercomputing-class performance. Up to 100 PFLOPS per pod.
        \item \textbf{TPUv4/TPUv5}:
        \begin{itemize}
            \item v4 pod: 4096 devices
            \item v5e: cost-efficient variant
            \item v5p: high-performance variant scalable to 8000+ devices
            \item Used in global data centers since 2023
        \end{itemize}
    \end{itemize}
    A \textbf{TPU Pod} aggregates \textbf{hundreds of TPU cores with shared memory} and custom high-speed networks for massive parallelism.

    
    \item \definition{Field-Programmable Gate Arrays (FPGAs)}. FPGAs offer \textbf{customizable digital logic} that can be reprogrammed after manufacturing.
    \begin{itemize}
        \item Flexible hardware, can be reconfigured for different algorithms.
        \item Suitable for:
        \begin{itemize}
            \item Network acceleration
            \item Security tasks (e.g., encryption)
            \item Data analytics
            \item Specialized ML inference
        \end{itemize}
    \end{itemize}
    For example, Microsoft Azure integrates FPGAs for infrastructure efficiency, lowering carbon footprint and improving hardware reuse.
\end{itemize}

\newpage

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l | l | l | l @{}}
            \toprule
            \textbf{Feature} & \textbf{GPU} & \textbf{TPU} & \textbf{FPGA} \\
            \midrule
            Purpose         & General-purpose ML compute    & ML-specific acceleration (esp. DL) & Flexible, reconfigurable hardware  \\ [.5em]
            Programmability & CUDA, OpenCL, etc.            & TensorFlow, PyTorch (high-level)   & VHDL, Verilog (low-level, HDL)     \\ [.5em]
            Flexibility     & High                          & Medium (optimized for tensors)     & Very high (reprogrammable)         \\ [.5em]
            Efficiency      & Good                          & Excellent (for tensor ops)         & Excellent (for specific pipelines) \\ [.5em]
            Use Case        & Training \texttt{+} Inference & Training \texttt{+} Inference      & Offloading, network, analytics     \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Summary: GPU vs TPU vs FPGA.}
\end{table}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} c p{14em} p{14em} @{}}
        \toprule
        & \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}} & \textcolor{Red2}{\faIcon{times-circle} \textbf{Disadvantages}} \\
        \midrule
        \textbf{CPU} & 
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Easy to be programmed and support any programming framework.
            \item Fast design space exploration and run your applications.
        \end{itemize}
        & \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Suited only for simple AI models that do not take long to train and for small models with small training set.
        \end{itemize} \\
        \cmidrule{1-3}
        %
        \textbf{GPU} & \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Ideal for applications in which data need to be processed in parallel like the pixels of images or videos.
        \end{itemize} & \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Programmed in languages like CUDA and OpenCL and therefore provide limited flexibility compared to CPUs.
        \end{itemize} \\
        \cmidrule{1-3}
        %
        \textbf{TPU} & \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Very fast at performing dense vector and matrix computations and are specialized on running very fast programming based on Tensorflow.
        \end{itemize} & \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item For applications and models based on the TensorFlow.
            \item Lower flexibility compared to CPUs and GPUs.
        \end{itemize} \\
        \cmidrule{1-3}
        %
        \textbf{FPGA} & \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Higher performance, lower cost and lower power consumption compared to other options like CPUs and GPU.
        \end{itemize} & \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Programmed using OpenCL and High-Level Synthesis (HLS).
            \item Limited flexibility compared to other platforms.
        \end{itemize} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of CPU, GPU, TPU and FPGA.}
\end{table}

\newpage

\subsubsection{Storage (type, technology)}\label{subsubsection: Storage (type, technology)}

Data has significantly grown in the last few years due to sensors, industry 4.0, AI, etc. The growth favours the \textbf{centralized storage strategy} that is focused on the following:
\begin{itemize}
    \item \textbf{Limiting redundant data}
    \item \textbf{Automatizing replication and backup}
    \item \textbf{Reducing management costs}
\end{itemize}

\highspace
The \emph{storage technologies} are many. One of the oldest but still used is the \definition{Hard Disk Drive (HDD)}, a magnetic disk with mechanical interactions. However, the mechanical nature of HDDs imposes physical limits on access speed and reliability. In contrast, \definition{Solid-State Drive (SSD)}, which lack moving parts and are built using NAND flash memory, offer significantly faster access times and better durability. The \definition{Non-Volatile Memory express (NVMe)} also exists, which is the \textbf{latest industry standard} for running PCIe\footnote{\definition{PCIe (peripheral component interconnect express)}. is an interface standard for connecting high-speed components} SSDs.

\highspace
In terms of cost per terabyte, NVMe drives are currently the most expensive (typically €100-200 for 1 TB), followed by SSDs (€70-100), while HDDs remain the most economical option (€40-60). This price-performance hierarchy makes hybrid storage architectures (HDD + SSD) increasingly appealing:
\begin{itemize}
    \item A speed storage technology (\textbf{SSD or NVMe}) as \textbf{cache} and \textbf{several HDDs for storage}. It is a combination used by some servers: a small SSD with a large HDD to have a faster disk.
    
    \item Some HDD manufacturers produce Solid State Hybrid Disks (SSHD) that combine a small SDD with a large HDD in a single unit.
\end{itemize}

\newpage

\paragraph{Files}

The operating system views the disk as a \textbf{flat collection of independently addressable data blocks}. Each \textbf{block is assigned a unique} \definition{LBA (Logical Block Address)}\label{LBA (Logical Block Address)}, enabling efficient data referencing and organization. To streamline access and reduce management overhead, the \textbf{OS typically groups these blocks into clusters}, larger units that serve as the minimum granularity for disk I/O operations.

\highspace
Clusters typically range in size from a single disk sector (512 bytes or 4 KB) up to 128 sectors (64 KB), depending on the file system configuration. Each cluster may store either:
\begin{itemize}
    \item \textbf{File data}. The actual contents of user files.
    
    \item \textbf{Metadata}. System-level information necessary to support the file system,:
    \begin{itemize}
        \item File and directory names
        \item Folder hierarchies and symbolic links
        \item Timestamps (creation, modification, access)
        \item Ownership and access control data
        \item \textbf{Links to the LBA where the file content can be located on the disk}
    \end{itemize}
\end{itemize}
Consequently, the \textbf{disk space is divided into different cluster types} based on their purpose:
\begin{itemize}
    \item Metadata:
    \begin{itemize}
        \item \hl{Fixed-position metadata clusters}, used to initialize and mount the file system
        \item \hl{Variable-position metadata clusters}, which manage directories and symbolic links
    \end{itemize}
    
    \item \hl{File data clusters}, containing the actual contents of files
    
    \item \hl{Unused clusters}, which are free and available for future allocations
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/files-1.pdf}
    \caption{A cluster can be seen visually as an array. In this image, for example, we've shown three types of cluster: metadata fixed position (azure), metadata variable position (green), file data (orange), unused space (white).}
\end{figure}

\newpage

\noindent
The following explanation introduces some basic operations on the files to see what happens inside the disks.
\begin{itemize}
    \item \textcolor{Red2}{\underline{\textbf{File Reading}}}
    \begin{enumerate}
        \item \important{Access the Metadata}. Before the system knows \textbf{where} the file is stored on disk, it must find information about the file, called metadata. This metadata is stored in \textbf{variable-position metadata clusters}. These clusters are not always in the same place on disk, they move and grow as the system evolves.
        \item \important{Locate the File's Data Blocks (clusters)} and \important{Read the Actual Content}. Once the OS has the metadata, it now knows:
        \begin{itemize}
            \item Which \textbf{Logical Block Addresses (LBAs)} contain the data.
            \item \textbf{How many clusters need to be read} to get the full content.
        \end{itemize}
        It uses this information to issue \textbf{read commands} to the disk controller. Finally, the \textbf{disk accesses the physical sectors or clusters} indicated by the LBAs and transfers that data into main memory (RAM).
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.8\textwidth]{img/files-2.pdf}
    \end{figure}

    \item \textcolor{Red2}{\underline{\textbf{File Writing}}}
    \begin{enumerate}
        \item \important{Access Metadata to Find Free Space}. The operating system \textbf{first checks the file system's metadata} to \textbf{find a free area of disk space} where it can store our new data.
        \item Once free space is identified, the OS:
        \begin{itemize}
            \item \textbf{Allocates} one or more clusters, depending on the file size
            \item \textbf{Writes our data} into these clusters on the physical disk
        \end{itemize}
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.8\textwidth]{img/files-3.pdf}
    \end{figure}

    Since the \emph{file system can only access clusters}, the \textbf{actual space taken up by a file on a disk is always a multiple of the cluster size}. Given:
    \begin{itemize}
        \item $s$, the \emph{file size}
        \item $c$, the \emph{cluster size}
    \end{itemize}
    Then the \definition{actual size on the disk $a$} can be calculated as:
    \begin{equation}
        a = \left\lceil \dfrac{s}{c} \right\rceil \times c
    \end{equation}
    Where $\mathrm{ceil}$ rounds a number \underline{up} to the nearest integer. It's also possible to calculate the \textbf{amount of disk space wasted by organising the file into clusters} (\definition{wasted disk space $w$}):
    \begin{equation}
        w = a - s
    \end{equation}
    A formal way to refer to wasted disk space is \definition{internal fragmentation} of files.
    \begin{examplebox}[: internal fragmentation]
        \begin{itemize}
            \item File size: 27 byte
            \item Cluster size: 8 byte
        \end{itemize}
        The \emph{actual size} on the disk is:
        \begin{equation*}
            a = \left\lceil \dfrac{27}{8} \right\rceil \cdot 8 = \left\lceil 3.375 \right\rceil \cdot 8 = 4 \cdot 8 = 32 \text{ byte}
        \end{equation*}
        And the internal fragmentation $w$ is:
        \begin{equation*}
            w = 32 - 27 = 5 \text{ byte}
        \end{equation*}
    \end{examplebox}

    \item \important{\underline{\textbf{Deleting}}}
    \begin{enumerate}
        \item The file system updates its metadata structures to:
        \begin{itemize}
            \item \textbf{Remove the file name} from the directory
            \item \textbf{Mark the clusters} where the file was stored as \textbf{free or available}
            \item Optionally, update timestamps or record deletion events
        \end{itemize}
        \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Importantly}}: \textbf{The data itself is not erased at this stage}. The actual bytes remain on disk until they are overwritten by another file.
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=\textwidth]{img/files-4.pdf}
    \end{figure}
    
    \item \important{\underline{\textbf{External fragmentation}}}. It happens when there are \textbf{enough free clusters on the disk} to store a file, but \textbf{not all together (not contiguous)}. So, when the system tries to write a large file, it must \textbf{split it} into smaller parts and place them in \textbf{different, scattered locations} on the disk.
    
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why does this happen?}} Over time, as files are created, deleted, resized, or moved, the disk becomes \textbf{less organized}. Clusters are freed in different spots, and the available space is no longer one big continuous area. So:
    \begin{itemize}
        \item A new file cannot fit in one continuous chunk.
        \item The OS stores it in \textbf{multiple non-adjacent clusters}.
    \end{itemize}
    This is called \definition{external fragmentation} because: the \textbf{fragmentation is not inside the file itself, but in the way its data is laid out externally across the disk}.

    \newpage

    \begin{figure}[!htp]
        \centering
        \includegraphics[width=\textwidth]{img/files-5.pdf}
        \caption{Each number (1, 2, 3) corresponds to a portion (chunk) of a file. A file that was meant to be stored as \texttt{[1][2][3]} (contiguously) has instead been broken into parts and stored in a scattered layout across different disk clusters. This situation represents \textbf{external fragmentation}, where the file system could not find a large enough continuous block of free space to store the file all together.}
    \end{figure}
\end{itemize}

\newpage

\paragraph{HDD}\label{paragraph: HDD}

A \definition{Hard Disk Drive (HDD)} is a \textbf{data storage device that uses rotating disks (platters) coated with magnetic material}.

\highspace
\textbf{Data is read randomly}, meaning individual data blocks can be stored or retrieved in any order rather than sequentially.

\highspace
An HDD consists of one or more rigid (\emph{hard}) rotating disks (platters) with magnetic heads arranged on a moving actuator arm to read and write data to the surfaces.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/files-6.pdf}
    \caption{Hard Drive Disk anatomy.}
\end{figure}

\noindent
Externally, hard drives expose a large number of \textbf{sectors} (blocks):
\begin{itemize}
    \item Typically, 512 or 4096 bytes.
    \item Individual \textbf{sector writes} are \textbf{atomic}.
    \item Multiple sectors write it may be interrupted (\definition{torn write}\footnote{Torn writes happen when only part of a multi-sector update is written successfully to disk.}).
\end{itemize}
The geometry of the drive:
\begin{itemize}
    \item The sectors are arranged into \textbf{tracks}.
    \item A \textbf{cylinder} is a particular track on multiple platters.
    \item Tracks are arranged in concentric circles on \textbf{platters}.
    \item A disk may have multiple double-sided platters.
\end{itemize}
The \textbf{driver motor spins the platters at a constant rate}, measured in \definition{Revolutions Per Minute (RPM)}.

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/files-7.pdf}
    \caption{Example of HDD geometry.}
    \label{fig: example of hdd geometry}
\end{figure}

\noindent
The geometry of an HDD refers to the \textbf{physical layout} of data on the surface of a spinning disk. Figure \ref{fig: example of hdd geometry} shows this:
\begin{itemize}
    \item \textbf{One Platter}. The circular surface shown is a platter, which is a rigid, magnetic-coated disk where data is stored. Most HDDs have multiple platters stacked vertically, but here we focus on one for simplicity.
    \item \textbf{Tracks}. The platter is divided into concentric circles called tracks. Each track is a circular path that the read/write head can follow. In the figure \ref{fig: example of hdd geometry}, we see three tracks, each larger than the last, moving outward from the center.
    \item \textbf{Sectors}. Each track is divided into pie-like slices called sectors. A sector is the smallest physical unit that can be read or written on a disk (typically 512 bytes or 4 KB). The numbers (0 to 35) represent sector identifiers along the circular tracks. Note how the outer tracks contain more sectors because they have a larger circumference, so they can physically hold more data; this is known as Zone Bit Recording (ZBR), not mentioned in the course.
    \item \textbf{Read/Write head}. The read head is shown floating above the platter. It moves radially across the surface to switch from one track to another (this is called seek). Once on the correct track, the head waits for the desired sector to rotate beneath it (rotational latency), and then it reads/writes data.
    \item \textbf{Rotation and seek behavior}. The platter spins at high speed (e.g., 7200 RPM). As it rotates, the sectors pass under the stationary head. Data is accessed when the correct sector aligns with the head.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Types of Delay in disk Access}}
\end{flushleft}
When a file is read from or written to a disk, especially an HDD, the total time taken is influenced by several delays. These are due to the mechanical and electronic processes involved in locating and transferring the data. Exists \textbf{four types of delay}:
\begin{itemize}\label{four types of hdd delay}
    \item \definition{Rotational Delay} (a.k.a. \definition{Rotational Latency}) is the \textbf{time needed for the disk to rotate so that the desired sector aligns with the read/write head}. It depends on the RPM (Revolutions Per Minute) of the disk.


    \item \definition{Seek Delay} is the \textbf{delay caused by the mechanical movement of the read/write head as it travels from one track to another} on a spinning disk. It is the dominant mechanical delay in many HDD operations, especially for random access patterns.

    \textcolor{Green3}{\faIcon{question-circle} \textbf{How it works}}. Moving the head involves a sequence of physical phases:
    \begin{enumerate}
        \item \textbf{Acceleration}: the actuator moves the head out of its resting position.
        \item \textbf{Coasting}: the head glides at a constant speed (if the distance is large).
        \item \textbf{Deceleration}: the actuator slows down to avoid overshooting.
        \item \textbf{Settling}: a short pause to stabilize the head at the desired track.
    \end{enumerate}

   
    \item \definition{Transfer time} is the \textbf{time required to actually move the data}, once the head is correctly positioned over the desired sector. It's the final step in the I/O pipeline, where data is \textbf{read from or written to} the magnetic surface.
    
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What affects transfer time?}}
    \begin{enumerate}
        \item \textbf{Rotational Speed (RPM)}, determines how quickly sectors pass under the head.
        \item \textbf{Data density}, more tightly packed data, more data per second read.
        \item \textbf{Size of the request}, reading more data takes more time.
    \end{enumerate}
    Even though it's much shorter than seek or rotation delays, \textbf{transfer time scales with data size}. For large sequential reads, transfer time becomes more relevant, especially when seek and rotation delays are minimized.


    \item \definition{Controller Overhead} is the \textbf{non-mechanical delay introduced by the disk controller}, which is the hardware interface between the OS and the physical disk. It involves:
    \begin{itemize}
        \item \textbf{Buffer management}: transferring data between disk and system memory (often using DMA).
        \item \textbf{Interrupt processing}: informing the OS when the I/O operation is complete.
        \item \textbf{Command translation}: converting OS-level I/O requests into device specific actions (e.g., SATA/NVMe commands).
        \item \textbf{Scheduling and queueing}: organize I/O operations for efficiency.
    \end{itemize}
\end{itemize}
\hl{To see how these delays are calculated}, we suggest you refer to the performance section \ref{subsection: Disk performance}, about HDD \ref{subsubsection: HDD performance}, page \pageref{subsubsection: HDD performance}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{\speedIcon} \textbf{Cache}}
\end{flushleft}
Hard Disk Drives (HDDs) are mechanical devices, and their performance is often limited by seek times and rotational latency. To partially overcome these limits, \textbf{many HDDs integrate a small} (8, 16, 32 MB) \textbf{amount of fast memory} (RAM) on the controller board, this is the \definition{cache} or \definition{track buffer}. The key functions of HDD cache are:
\begin{enumerate}
    \item \definition{Read Caching}. When the disk reads a block from the platter, it may also load \textbf{adjacent blocks} into the cache, expecting that they might be requested next (a technique known as \textbf{read-ahead}).
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item If the next read request is for cached data, the disk can respond \textbf{instantly} from fast RAM.
            \item This avoids mechanical movement and \textbf{cuts seek and rotational delays}.
            \item Very effective for \textbf{sequential reads}.
        \end{itemize}
    \end{itemize}


    \item \definition{Write Caching}. There are two strategies here:
    \begin{enumerate}
        \item \definition{Write-Back Cache} (Faster, Riskier). The HDD \textbf{reports the completion of the write} operation as soon as the data is in the cache, \textbf{before it is actually written to the platter}. Actual writing to disk happens later, in the background.
        \begin{itemize}
            \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
            \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
                \item Faster perceived performance
                \item Useful in workloads with many small writes
            \end{itemize}
            \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
            \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
                \item If power is lost before the data is flushed to disk, the \textbf{data is lost}. It is a \textbf{file system corruption risk}!
                \item This is why it's considered \textbf{dangerous in critical systems} without battery backup or UPS.
            \end{itemize}
        \end{itemize}
        
        \item \definition{Write-Through Cache} (Safer, Slower). The HDD only reports the completion of the write operation \textbf{after the data has been fully written to disk}. Safer, but \textbf{slower}, since the OS must wait for the mechanical operation to finish.
    \end{enumerate}

    
    \item Hybrid Cache: \definition{Flash-Based Caching}. Some modern HDDs integrate \textbf{small flash memory} used for \textbf{persistent caching}:
    \begin{itemize}
        \item Data stays even if the power goes out.
        \item Combines the speed of SSD with the capacity of HDD.
        \item Great for frequently accessed blocks (e.g., boot files, apps).
    \end{itemize}
\end{enumerate}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Disk Scheduling}}
\end{flushleft}
While \textbf{caching} helps improve disk performance, it \textbf{doesn't eliminate the delays caused by seek and rotational latency}, especially during random access patterns. In systems with many I/O requests (like in a database or OS kernel), it's crucial to \textbf{choose the right order} to process requests to \textbf{minimize head movement}. This is where disk scheduling algorithms come in.

\highspace
Instead of serving I/O requests in the order they arrive, the \textbf{disk controller (or OS)} can render them to improve efficiency. It is possible:
\begin{itemize}
    \item Because every \textbf{disk request} includes the \textbf{target position} (i.e., the cylinder/track).
    \item So we can \textbf{estimate the cost (seek time)} of each request and choose the most efficient order.
\end{itemize}
Common disk scheduling algorithms are:
\begin{enumerate}
    \item \definition{First Come, First Serve (FCFC)} (the worst)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} Requests are handled in the \textbf{order they arrive}. No optimization, simple queue processing.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item (Naive) Easy to implement
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
        \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Can lead to \textbf{long seek distances} (i.e., inefficient head movement)
        \end{itemize}
    \end{itemize}

    \item \definition{Shortest Seek Time First (SSTF)} (great performance, but watch out for starvation)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} Always serve the request \textbf{closest to the current head position}.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Minimizes \textbf{total head movement}.
            \item Efficient in practice.
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
        \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Can lead to \textbf{starvation}: distant requests may never be served if new close requests keep arriving.
        \end{itemize}
    \end{itemize}

    \item \definition{SCAN (Elevator Algorithm)} (good performance as SSTF, but fairer)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} The head \textbf{moves in one direction} (like an elevator), serving all requests in that direction. When it reaches the end, it \textbf{reverses direction}.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Good worst-case behavior.
            \item \textbf{No starvation}, every request will eventually be served.
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
        \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Requests at \textbf{edges} of the disk may have \textbf{longer wait times}.
        \end{itemize}
    \end{itemize}

    \item \definition{Circular SCAN (C-SCAN)} (fair, but less efficient than SCAN in some cases)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} Like SCAN, but head \textbf{only moves in one direction}. When it reaches the end, it \textbf{jumps back} to the beginning (like a circular elevator).
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item More \textbf{uniform wait time} for all requests.
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
        \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Longer total movement (because of the jump).
        \end{itemize}
    \end{itemize}

    \item \definition{C-LOOK} (smart compromise between performance and fairness)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} Like C-SCAN, but instead of going to the physical end of the disk, th \textbf{head only goes as far as the last request in that direction}, then \textbf{jumps back to the smallest request}.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item \textbf{Saves movement} compared to full C-SCAN.
            \item Still \textbf{avoids starvation}.
        \end{itemize}
    \end{itemize}
\end{enumerate}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | l | l | l @{}}
        \toprule
        Algorithm & Fairness & Risk of Starvation & Strategy \\
        \midrule
        FCFS        & \textcolor{Green3}{\faIcon{check}} Yes   & \textcolor{Red2}{\faIcon{times}} None     & Serve in arrival order            \\ [.3em]
        SSTF        & \textcolor{Red2}{\faIcon{times}} No      & \textcolor{Red2}{\faIcon{exclamation-triangle}} Possible & Serve closest request             \\ [.3em]
        SCAN        & \textcolor{Green3}{\faIcon{check}} Yes   & \textcolor{Red2}{\faIcon{times}} None     & Elevator (back \& forth)          \\ [.3em]
        C-SCAN      & \textcolor{Green3}{\faIcon{check}} Yes   & \textcolor{Red2}{\faIcon{times}} None     & One-direction sweep (circular)    \\ [.3em]
        C-LOOK      & \textcolor{Green3}{\faIcon{check}} Yes   & \textcolor{Red2}{\faIcon{times}} None     & One-direction sweep (limited)     \\
        \bottomrule
    \end{tabular}
    \caption{Scheduling Algorithms.}
\end{table}

\newpage

\paragraph{SSD}\label{paragraph: SSD}

A \definition{Solid-State Drive (SSD)} is a \textbf{non-volatile storage device} that retains data without power. Unlike traditional Hard Disk Drives (HDDs), an SSD has \textbf{no mechanical parts}, \emph{no spinning platters}, \emph{no moving heads}. Internally, it's made of \textbf{transistors}, similar to those found in CPUs and RAM. It includes a \textbf{controller}, which \hl{manages read/write operations}, wear leveling, garbage collection, and interface emulation. SSDs often adopt HDD-compatible interfaces (e.g., SATA, PCIe/NVMe) and form factors (2.5", M.2) for backward compatibility. Offers \textbf{higher performance} than HDDs, especially in \textbf{random access latency} and IOPS.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{\speedIcon} \textbf{Flash Cell Technologies: Storing Bits in NAND}}
\end{flushleft}
Modern SSDs use \textbf{NAND Flash Memory}, where data is stored in memory \textbf{cells}. Each cell can store one or more bits:

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} c | c | l @{}}
            \toprule
            Cell Type & Bits per Cell & Characteristics \\
            \midrule
            \textbf{SLC} & 1 & Fastest, most durable (up to 100k cycles), expensive \\ [.3em]
            \textbf{MLC} & 2 & Slower than SLC, less durable, more dense            \\ [.3em]
            \textbf{TLC} & 3 & Used in most consumer SSDs; cheaper                  \\ [.3em]
            \textbf{QLC} & 4 & High density, lower endurance                        \\ [.3em]
            \textbf{PLC} & 5 & Experimental/extremely high density, low endurance   \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\noindent
\hl{Increasing} the number of \hl{bits per cell} improves \textbf{capacity and cost-efficiency}, but \textbf{reduces endurance} and \textbf{performance}. Each cell type requires more precise voltage thresholds and incurs \textbf{more error correction and wear}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Internal Organization}}
\end{flushleft}
Solid-State Drives are built on NAND flash memory, which is \textbf{internally structured in a hierarchical manner} to optimize storage density and access efficiency.
\begin{itemize}
    \item \definition{Cell}: Basic storage unit (SLC, MLC, TLC, etc.) storing bits via trapped electrons.


    \item \definition{Page}: The \textbf{smallest unit that can be read or written}. Typically 4-16 KB.

    \textbf{Pages} can be:
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Valid (In-use)}}, contain active, readable data.
        \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Dirty (Invalid)}}, hold obsolete or overwritten data.
        \item \textbf{Empty (Erased)}, ready to be programmed (written).
    \end{itemize}


    \item \definition{Block}: The \textbf{smallest unit that can be erased}. Usually contains 64-256 pages.
\end{itemize}
A block might have a capacity of 128-256 KB, composed of many smaller pages. Each \textbf{page} maps to a \definition{Logical Block Address (LBA)} visible to the OS.

\highspace
\begin{deepeningbox}[: Logical Block Address (LBA)]
    The \definition{Logical Block Address (LBA)} is a key abstraction used by operating systems and file systems to \textbf{refer to storage locations} on a disk (HDD or SSD) in a simple, sequential manner.

    \highspace
    \textbf{LBA is an index number} that uniquely identifies a fixed-size block of data (typically 512 bytes or 4 KB) on a storage device. It \textbf{hides the physical layout} of the drive (cylinders, heads, sectors) and presents a \textbf{flat address space} to the OS.

    \highspace
    \emph{How does it work with SSDs?} The OS issues read/write commands to specific LBA addresses. Internally, the SSD controller translates each LBA to a \textbf{physical location} in flash memory using a structured called the \textbf{Flash Translation Layer (FTL)}. This mapping is \textbf{dynamic} due to wear leveling, garbage collection, and bad block management.

    \highspace
    In summary, the LBA is how the operating system sees the disk. Instead, the physical address is where the data actually resides inside the SSD.
\end{deepeningbox}

\noindent
Key operations are:
\begin{itemize}
    \item \texttt{READ}: reads data from a \textbf{page}.
    \item \texttt{PROGRAM}: writes to a \textbf{page} (only if it's empty).
    \item \texttt{ERASE}: wipes an entire \textbf{block} (required before rewriting any page in that block).
\end{itemize}
The \textbf{important limitation} is that flash memory \textbf{cannot overwrite data in place}, we have to erase the whole block before reusing it. This lead to a \textbf{read-modify-erase-write cycle} even for simple updates.


\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Write Amplification Phenomenon}}
\end{flushleft}\label{Write amplification (WA)}
\definition{Write Amplification} refers to the phenomenon where \textbf{the amount of actual data written to the NAND flash is greater than what the host system requested to write}.

\highspace
\textcolor{Green3}{\faIcon{question-circle} \textbf{\emph{Why does Write Amplification happen in SSDs?}}} It stems from the \textbf{erase-before-write constraint} of NAND flash memory:
\begin{enumerate}
    \item \textbf{(Flash) Pages can only be written once}, and to change them, we must erase the \textbf{entire block} (which may contain many pages).

    \item If we modify even a \textbf{small piece of data}, the SSD:
    \begin{itemize}
        \item Allocates a new page for the updated data.
        \item Marks the old page as invalid (dirty).
        \item Eventually, \textbf{copies all valid pages} from a block, \textbf{erases the block}, and \textbf{rewrites it} (this is called \textbf{garbage collection}).
    \end{itemize}
\end{enumerate}
So a 4KB write might cause \textbf{hundreds of KBs} to be written internally!

\begin{examplebox}[: Write Amplification]
    Given a hypothetical SSD:
    \begin{itemize}
        \item Page Size: 4 KB
        \item Block Size: 5 Pages
        \item Drive Size: 1 Block
        \item Read Speed: 2 KB/s
        \item Write Speed: 1 KB/s
    \end{itemize}
    \begin{enumerate}
        \item Write a 4 KB \texttt{.txt} file.
        \begin{itemize}
            \item One page used (page size $\div$ file dimension);
            \item Time: 4 seconds (write speed $\times$ file dimension, 1 KB/s $\times$ 4 KB).
        \end{itemize}

        \item Write an 8 KB \texttt{.png}
        \begin{itemize}
            \item Takes 2 pages;
            \item There are 3 pages used in total;
            \item Time: 8 seconds.
        \end{itemize}
        \begin{center}
            \includegraphics[width=.7\textwidth]{img/SSD-1.pdf}
        \end{center}

        \item Delete the 4 KB \texttt{.txt} file
        \begin{itemize}
            \item The first page is now \textbf{invalid} (dirty), but still \textbf{physically used};
            \item The SSD cannot reuse it until the whole block is erased.
        \end{itemize}

        \item Write a 12 KB image
        \begin{itemize}
            \item OS sees ``3 free pages'', but \textbf{only 2 are truly empty};
            \item Can't fit 12 KB in 2 empty pages.
        \end{itemize}

        \newpage

        \emph{What happens internally?}
        \begin{enumerate}
            \item SSD reads the 8 KB of still-valid pages from NAND into a temporary cache.

            This step takes 4 seconds (size to read $\div$ read speed).
            \item Marks the 1st page (old \texttt{.txt}, step 1) as discarded.
            \item Places the new 12 KB into the cache.
            \item \textbf{Erases the entire 20 KB block} (this is mandatory before rewriting any page)
            \item \textbf{Rewrites}:
            \begin{itemize}
                \item 8 KB old data
                \item 12 KB new data
            \end{itemize}
            This step takes 20 seconds to write to the NAND.
        \end{enumerate}
    \end{enumerate}
    The OS thought it was just a 12 KB write and expected only 12 seconds. But the SSD actually wrote 20 KB and read 8 KB (24 seconds total).

    This is a case of write amplification caused by limited empty pages, erase-before-write constraint, and the need to preserve valid data.
\end{examplebox}

\noindent
A direct mapping between Logical and Physical pages is not feasible inside the SSD. Therefore, each SSD has an FTL component that makes the SSD \emph{look like an HDD}.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{brain} \textbf{Flash Translation Layer (FTL)}}
\end{flushleft}
The \definition{Flash Translation Layer (FTL)} is the hidden ``brain'' of an SSD, it \textbf{makes NAND flash usable in the same way as a traditional hard disk}, despite its very different constraints.

\highspace
\textbf{Translates Logical Block Addresses (LBA)} (see page \pageref{Write amplification (WA)}) from the operating system into actual \textbf{physical locations} in the NAND flash. Also, it makes the SSD behave like an HDD to the OS (abstracts away erase-before-write and wear issues).

\highspace
\textcolor{Green3}{\faIcon{question-circle} \textbf{\emph{Why We Need Translation?}}} Direct LBA to physical mapping isn't feasible because:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item Flash memory \textbf{can't overwrite in-place} (must erase first)
    \item Pages are grouped into blocks and must be programmed \textbf{in order}.
    \item Flash memory blocks \textbf{wear out over time}, requiring wear balancing.
\end{itemize}
The FTL responsibilities are:
\begin{enumerate}
    \item \important{Address Mapping}. Maintains a \textbf{mapping table}, logical to physical page. Supports dynamic remapping when data is updated (e.g., a page becomes dirty and is relocated).
    \item \important{Log-Structured Writes}. Uses log-structured techniques: \textbf{writes go to the next available page} in an erased block. It ensures writes are sequential (within a block), reducing write amplification and improving performance.
    \item \important{Garbage Collection}. Identifies blocks full of \textbf{invalid/dirty pages}. Reads out remaining valid pages, erases the block, and rewrites valid data $+$ new data elsewhere. Necessary to free up space for future writes.
    \item \important{Wear Leveling}. Flash cells can only endure a limited number of erases. FTL spreads out writes across all blocks to \textbf{prevent early death} of heavily used regions.
\end{enumerate}

\begin{examplebox}[: Log-Structured FTL]
    Setup:
    \begin{itemize}
        \item \textbf{Page size}: 4 KB
        \item \textbf{Block size}: 4 pages (total 16 KB per block)
        \item \textbf{Initial condition}: all pages are marked \textbf{INVALID}
        \item \textbf{Action}: perform a series of logical writes
    \end{itemize}
    Assume that a page size is $4$ KB and a block consists of four pages. The write list is (\texttt{Write(pageNumber, value)}):
    \begin{itemize}
        \item \texttt{Write(100, a1)}   $\rightarrow$ write value \texttt{a1} to logical page \texttt{100}
        \item \texttt{Write(101, a2)}   $\rightarrow$ write value \texttt{a2} to logical page \texttt{101}
        \item \texttt{Write(2000, b1)}  $\rightarrow$ write value \texttt{b1} to logical page \texttt{2000}
        \item \texttt{Write(2001, b2)}  $\rightarrow$ write value \texttt{b2} to logical page \texttt{2001}
        \item \texttt{Write(100, c1)}   $\rightarrow$ overwrite logical page \texttt{100} with value \texttt{c1}
        \item \texttt{Write(101, c2)}   $\rightarrow$ overwrite logical page \texttt{101} with value \texttt{c2}
    \end{itemize}
    The steps are:
    \begin{enumerate}
        \item The initial state is with all pages marked as \texttt{INVALID(i)}:
        \begin{center}
            \includegraphics[width=.7\textwidth]{img/log-structured-ftl-1.png}
        \end{center}

        \item Erase block zero:
        \begin{center}
            \includegraphics[width=.7\textwidth]{img/log-structured-ftl-2.png}
        \end{center}

        \item Program pages in order and update mapping information (first \texttt{Write(100, a1)}):
        \begin{center}
            \includegraphics[width=.8\textwidth]{img/log-structured-ftl-3.png}
        \end{center}

        \item After performing four writes (\texttt{Write(100, a1)}, \texttt{Write(101, a2)}, \texttt{Write(2000, b1)}, \texttt{Write(2001, b2)}):
        \begin{center}
            \includegraphics[width=.8\textwidth]{img/log-structured-ftl-4.png}
        \end{center}

        \item After updating $100$ and $101$:
        \begin{center}
            \includegraphics[width=.8\textwidth]{img/log-structured-ftl-5.png}
        \end{center}
    \end{enumerate}
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why Garbage Collection Exists}}
\end{flushleft}
In SSDs, data cannot be updated in place, when a page is modified:
\begin{itemize}
    \item The \textbf{old version becomes obsolete} (marked \emph{invalid}).
    \item The \textbf{new version} is written to a \textbf{fresh page}.
    \item Over time, blocks fill with \textbf{invalid pages}, this is called \emph{garbage}.
\end{itemize}
\definition{Garbage Collection (GC)} is the process of \textbf{reclaiming space} by \textbf{erasing blocks that contain invalid pages}. It works like this:
\begin{enumerate}
    \item \important{Identify a block} that has invalid (dirty) pages.
    \item \important{Copy} any \textbf{still-valid pages} into a new block.
    \item \important{Erase} the \textbf{original block} completely (erasing works only at block granularity).
    \item \important{Update} the \textbf{mapping table} to reflect new page locations.
\end{enumerate}

\begin{examplebox}[: how garbage collection works]
    The steps are:
    \begin{enumerate}
        \item Update request for existing data:
        \begin{center}
            \includegraphics[width=.2\textwidth]{img/garbage-collection-1.pdf}
        \end{center}

        \item Find a free page, and save the new data:
        \begin{center}
            \includegraphics[width=.13\textwidth]{img/garbage-collection-2.pdf}
        \end{center}

        \item This scenario may continue until there are not enough free blocks:
        \begin{center}
            \includegraphics[width=.4\textwidth]{img/garbage-collection-3.pdf}
        \end{center}

        \item Collect valid pages into a free block:
        \begin{center}
            \includegraphics[width=.4\textwidth]{img/garbage-collection-4.pdf}
        \end{center}

        \item Update the map table and erase invalid (obsolete) blocks:
        \begin{center}
            \includegraphics[width=.4\textwidth]{img/garbage-collection-5.pdf}
        \end{center}
    \end{enumerate}
\end{examplebox}

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Three Major Problems of SSD Architecture}}
\end{flushleft}
\begin{enumerate}
    \item \important{Garbage Collection Is Expensive}. Garbage collection (GC) in SSDs is \textbf{unavoidable} due to the nature of NAND flash memory: flash \textbf{cannot overwrite pages}, only erase entire blocks.

    It is so expensive because it requires reading valid data (read cost), rewriting that data (write cost), and erasing entire blocks. If blocks are \textbf{partially valid}, even small updates can trigger \textbf{large internal data movement}. This leads to write amplification, effective write speed and wear on NAND cells.

    The \textbf{more valid data} in a block, the \textbf{higher the cost} of GC.
    \begin{itemize}
        \item \textbf{Ideal case}: reclaim blocks with only invalid pages (no migration needed).
        \item \textbf{Realistic case}: migrate live data, high overhead.
    \end{itemize}

    \textcolor{Green3}{\faIcon{check-circle} \textbf{Mitigation Techniques}}
    \begin{itemize}
        \item \textbf{Overprovisioning}. Reserve extra flash capacity that the user can't see. More space, then delayed GC.
        \item \textbf{Background GC}. Perform GC during \textbf{idle periods} to avoid disrupting foreground writes.
        \item \textbf{Write buffering}. Use DRAM/SRAM to accumulate small writes and reduce fragmentation.
        \item \textbf{Hot/Cold Separation}. Store frequently updated data (hot) separately from rarely updated data (cold).
    \end{itemize}
    
    \item \important{The Ambiguity of Delete}. In traditional file systems (especially on HDDs), \textbf{deleting a file} simply:
    \begin{itemize}
        \item Removes the file's metadata (e.g., from the file system's directory tree).
        \item Does \textbf{not erase or inform the disk that the data blocks are invalid}.
    \end{itemize}
    This behavior is fine for HDDs, which can overwrite sectors anytime. But for SSDs, this \textbf{creates a serious mismatch}.

    SSDs rely on Garbage Collection (GC) to free space. GC assumes that \textbf{only invalid pages} can be discarded. However:
    \begin{itemize}
        \item The SSD sees no distinction between ``old'' and ``deleted'' data unless explicitly told.
        \item So even \textbf{deleted files look valid} to the SSD.
        \item When GC runs, it \textbf{copies all pages} it believes to be valid, including junk!
    \end{itemize}
    This causes SSDs to waste time and NAND endurance \textbf{preserving deleted data}.

    \textcolor{Green3}{\faIcon{check-circle} \textbf{How to Fix it: TRIM / UNMAP}}. Modern OSs and SSD interfaces support special commands: TRIM (SATA) and UNMAP (SCSI/NVMe). These commands allow the \textbf{OS to explicitly tell the SSD}: these Logical Block Addresses (LBAs) are no longer valid, feel free to erase them.


    \item \important{Mapping Table Size and FTL Scalability}. In an SSD, the \definition{Flash Translation Layer (FTL)} maps: \textbf{Logical Block Addresses (LBAs) from the OS to physical flash pages}. This mapping is essential because:
    \begin{itemize}
        \item Flash memory can't overwrite in place
        \item Pages must be written sequentially
        \item Blocks must be erased before reuse
    \end{itemize}
    So the SSD keeps an \textbf{internal mapping table} to know \textbf{where every logical page actually resides}.

    The mapping table \textbf{grows} proportionally with: \textbf{drive capacity}, and \textbf{granularity of mapping}.

    \textcolor{Green3}{\faIcon{check-circle} \textbf{FTL Strategies to Cope}}
    \begin{itemize}
        \item \definition{Block-Level Mapping}. The FTL maps \textbf{each logical block number (LBN)} to a \textbf{physical block number (PBN)}. All pages within that block are \textbf{assumed to map 1:1}.
        \begin{itemize}
            \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}
            \begin{itemize}
                \item \textbf{Very small mapping table}
                \item Efficient in \textbf{sequential-write} workloads (e.g. logging, archiving).
            \end{itemize}
            \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}
            \begin{itemize}
                \item \textbf{Terrible for random writes}: to update just 1 page, the entire block must be read, modified, erased, rewritten.
                \item Results in very \textbf{high write amplification}.
            \end{itemize}
        \end{itemize}
        \begin{examplebox}[: Block Mapping]
            The first four writes:
            \begin{multicols}{2}
                \begin{itemize}
                    \item \texttt{Write(2000, a)}
                    \item \texttt{Write(2001, b)}
                    \item \texttt{Write(2002, c)}
                    \item \texttt{Write(2003, d)}
                \end{itemize}
            \end{multicols}
            \begin{center}
                \includegraphics[width=\textwidth]{img/block-mapping-1.png}
            \end{center}
    
            \noindent
            And finally the last one:
            \begin{itemize}
                \item \texttt{Write(2002, c')}
            \end{itemize}
            \begin{center}
                \includegraphics[width=\textwidth]{img/block-mapping-2.png}
            \end{center}
        \end{examplebox}
        \item \definition{Hybrid FTL}. Combine the best of Block-Level Mapping for most pages and use \definition{Page-Level Mapping} (FTL maps each logical page number to a physical page number) for small updates. Often implemented using a \textbf{log block buffer}.
        \begin{itemize}
            \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}
            \begin{itemize}
                \item Lower memory usage than pure page-level
                \item Lower write amplification than pure block-level
            \end{itemize}
            \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}
            \begin{itemize}
                \item More complex logic (copy-back handling, log merging)
                \item Still suffers from some write amplification during \textbf{log cleaning}
            \end{itemize}
        \end{itemize}
        \begin{examplebox}[: Hybrid Mapping]
            Let's suppose the following sequence:
            \begin{multicols}{2}
                \begin{itemize}
                    \item \texttt{Write(1000, a)}
                    \item \texttt{Write(1001, b)}
                    \item \texttt{Write(1002, c)}
                    \item \texttt{Write(1003, d)}
                \end{itemize}
            \end{multicols}
            \begin{center}
                \includegraphics[width=\textwidth]{img/hybrid-mapping-1.png}
            \end{center}
            Let's update some pages:
            \begin{itemize}
                \item \texttt{Write(1000, a')}
                \item \texttt{Write(1001, b')}
                \item \texttt{Write(1002, c')}
                \item FTL updates only the page mapping information
            \end{itemize}
            \begin{center}
                \includegraphics[width=\textwidth]{img/hybrid-mapping-2.png}
            \end{center}
            When needed, FTL can perform MERGE operations:
            \begin{center}
                \includegraphics[width=\textwidth]{img/hybrid-mapping-3.png}
            \end{center}
        \end{examplebox}
        \item \definition{Page mapping plus caching} (a.k.a. \definition{Demand-based FTL (DFTL)}). Keep \textbf{only a portion} of the page-level mapping table in DRAM (a cache). \textbf{Store} the full mapping table in \textbf{flash itself}. On cache miss, \textbf{read the mapping} from flash into RAM (like a page table swap-in).
        \begin{itemize}
            \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}
            \begin{itemize}
                \item \textbf{Scales to very large SSDs} with \textbf{low DRAM footprint}
                \item Maintains page-level flexibility without storing entire table in RAM
            \end{itemize}
            \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}
            \begin{itemize}
                \item Mapping lookup incurs \textbf{read latency} on cache misses
                \item More complex metadata management (must protect flash-stored tables)
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{enumerate}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{The importance of Wear Leveling}}
\end{flushleft}
Flash memory has a \textbf{limited number of erase/write (EW) cycles}:
\begin{itemize}
    \item Each block can only sustain $\approx$3,000 to 100,000 cycles (depending on type: SLC, MLC, TLC, QLC).
    \item If some blocks are used more than others (write \textbf{skew}), they \textbf{wear out faster}.
\end{itemize}
Without intervention, the SSD's lifespan would be determined by its \textbf{most heavily used block}. Ensure that \textbf{all blocks} in the SSD \textbf{wear out evenly} to: maximize \textbf{overall drive lifetime}, avoid \textbf{early failure} due to localized hot spots.

\highspace
\textcolor{Green3}{\faIcon{question-circle} \textbf{How Wear Leveling works?}} The \textbf{Flash Translation Layer (FTL)}:
\begin{enumerate}
    \item Monitors \textbf{erase/write cycles} per block.
    \item Identifies \textbf{cold blocks} (rarely updated, long-lived data).
    \item Periodically:
    \begin{itemize}
        \item \textbf{Reads} valid data from cold blocks
        \item \textbf{Moves} it to fresher blocks
        \item \textbf{Erases} and \textbf{reuses} the original blocks
    \end{itemize}
    Even clod blocks are ``rotated'' to ensure wear balance.
\end{enumerate}

\newpage

\noindent
There are three types of Wear Leveling:
\begin{itemize}
    \item \textbf{Dynamic}: Spread writes among blocks not currently holding static data
    \item \textbf{Static}: Occasionally move long-lived (cold) data to give its block a rest
    \item \textbf{Hybrid}: Combine both (used in most SSDs)
\end{itemize}
\textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Wear Leveling Disadvantages}}
\begin{itemize}
    \item \important{Increases Write Amplification}: moving cold data incurs extra writes.
    \item \important{Consumes bandwidth}: periodic copying reduces performance.
    \item \important{Complexity}: requires tracking per-block wear counts and scheduling background operations.
\end{itemize}
However, to \textbf{partially fix} this, a simple policy to apply is that each flash block has an \definition{Erase/Write Cycle Counter} and maintains the value of:
\begin{equation}
    \left| \mathrm{Max}\left(\text{EW cycle}\right) - \mathrm{Min}\left(\text{EW cycle}\right) \right| < \varepsilon
\end{equation}
Where $\varepsilon$ is a system-defined Wear Leveling threshold.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{balance-scale} \textbf{HDD vs SSD}}
\end{flushleft}
UBER and TBW are two metrics help quantify how \textbf{reliable and durable a storage device is over time and under heavy use}.
\begin{itemize}
    \item \definition{Unrecoverable Bit Error Ratio (UBER)}. The probability that a bit cannot be recovered correctly by the device, even after error correction.
    \begin{equation}
        \text{UBER} = \dfrac{\text{Number of unrecoverable bit errors}}{\text{Total bits read}}
    \end{equation}
    A \textbf{lower} UBER means \textbf{better data reliability}.
    
    \item \definition{Endurance rating: Terabytes Written (TBW)}. The total amount of data we can write to the SSD over its warrantied lifetime before cells are expected to wear out.
    \begin{equation}
        \text{TBW} = \text{Endurance rating of the SSD (from manufacturer)}
    \end{equation}
    For example, a 250 GB SSD with TBW $= 70$ TB, we can write: $70 \div 365 = 190$ GB/day.
\end{itemize}

\newpage

\paragraph{RAID}\label{paragraph: RAID}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{history} \textbf{How is RAID born?}}
\end{flushleft}
Before 1980, \definition{JBOD (Just a Bunch Of Disks)} was a common setup before the advent of disk arrays and RAID. JBOD simply \textbf{connects multiple physical disks to a system}, but each disk operates \textbf{independently}. There's \textbf{no striping} and \textbf{no redundancy}. Each disk has its \textbf{own mount point}, and the operating system or user decides where to store data. If one disk fails, \textbf{only the data on that disk is lost}, no protection or recovery is provided. \textbf{Performance bottlenecks} also occurred as CPUs became faster than disk I/O.

\highspace
Researchers at UC Berkeley (notably Patterson, Gibson and Katz) proposed using \textbf{multiple cheaper disks} together: treat \textbf{many small, inexpensive disks} as \textbf{one large logical disk}. These were called \definition{Disk Arrays}. The main idea was to achieve higher performance and more memory through parallelism.
\begin{itemize}
    \item Disk Arrays appear as a \textbf{single logical high-performance disk}.
    \item Use \textbf{data striping} and \textbf{parallel access}, but these \textbf{weren't formalized or optimized} in the early implementation. This is because the core idea was to spread data across multiple disks to improve throughput (multiple disks serving different I/Os) and latency (multiple I/Os in parallel reduce queue wait time). But the \textbf{parallelism was often implicit}, not exposed to the software or the user. Some arrays allowed \textbf{concurrent I/O by accident}, not design. \textbf{I/O scheduling} was hardware-specific, \textbf{not standardized}.
    \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Key improvements}}:
    \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
        \item \textcolor{Green3}{\textbf{Parallel Access}}: Multiple disks could serve I/O requests simultaneously.
        \item \textcolor{Green3}{\textbf{Performance Boost}}: Higher bandwidth and lower latency via concurrent disk activity.
        \item \textcolor{Green3}{\textbf{Scalability}}: Easy to increase capacity by adding disks.
        \item \textcolor{Green3}{\textbf{Lower Cost per GB}}: Used \textbf{multiple small, cheap disks} instead of one large expensive disk.
        \item \textcolor{Green3}{\textbf{Modular Architecture}}: Allowed easier maintenance, replacement, and upgrades.
    \end{itemize}
    \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Limitations}}:
    \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
        \item \textcolor{Red2}{\textbf{No Standard Striping}}: Striping (if present) was non-standard and not guaranteed.
        \item \textcolor{Red2}{\textbf{No Redundancy}}: If a disk failed, \textbf{data was lost}, arrays were vulnerable.
        \item \textcolor{Red2}{\textbf{No RAID Levels}}: No formal trade-offs between performance/reliability (e.g. RAID 0-6).
        \item \textcolor{Red2}{\textbf{No Fault Tolerance Mechanism}}: No automatic reconstruction, hot spares, or parity.
        \item \textcolor{Red2}{\textbf{Limited Write Optimization}}: RAID improved performance (esp. for writes) using smart parity layouts.
    \end{itemize}
\end{itemize}

\highspace
In 1988, the same researchers published the seminal paper: ``A Case for Redundant Arrays of Inexpensive Disks (RAID)''\cite{Patterson:CSD-87-391}. They made two key contributions:
\begin{itemize}
    \item They \textbf{formally introduced RAID}: not just disk arrays, but structured ones that also \textbf{handle reliability} via redundancy.
    \item They \textbf{defined RAID levels} (RAID 0 to RAID 5 in the original paper), each with different trade-offs between: \emph{performance}, \emph{reliability} and \emph{storage efficiency}.
\end{itemize}
RAID was important because it \textbf{solved the reliability problem introduced by disk arrays}. Disk Arrays solved performance and capacity, but RAID solved reliability too, combining \textbf{Data Striping} (performance) and \textbf{Redundancy} (reliability).

\begin{definitionbox}[: RAID]
    \definition{RAID (Redundant Array of Independent Disks)} is a \textbf{storage technology} that \textbf{combines multiple physical disks into a single logical unit} to improve performance, reliability, or both, by using data striping, redundancy, or both.

    \highspace
    RAID is based on two core techniques: \textbf{Data Striping} (for performance, splits data across disks), and \textbf{Redundancy} (for reliability, adds fault tolerance via mirroring or parity).

    \highspace
    Exists different RAID levels. Each level defines a strategy for:
    \begin{itemize}
        \item \emph{How data is striped}
        \item \emph{How redundancy is added}
        \item \emph{What failure modes are tolerated}
        \item \emph{How performance and capacity are affected}
    \end{itemize}
\end{definitionbox}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Data Striping}}
\end{flushleft}
\definition{Data Striping} is the technique of \textbf{splitting data into small chunks} (called \textbf{stripe units}) and distributing them \textbf{across multiple disks in a round-robin manner}. Used primarily to \textbf{improve performance} by \hl{enabling parallel disk access}. It does \textbf{not add redundancy} by itself.
\begin{itemize}
    \item \definition{Stripe Unit}: The amount of \textbf{data written to a single disk} before moving to the next one. Can be in \textbf{bits}, \textbf{bytes}, \textbf{blocks} or \textbf{KBs}. Small stripe unit, better parallelism for large files. Large stripe unit, fewer disk operations for small reads.

    \item \definition{Stripe Width}: The \textbf{number of disks} over which the data is stripped. If we have 4 disks participating in stripping, stripe width is 4. \textbf{Determines how many disks work in parallel} for a given I/O.
    
    \item How are \definition{Multiple Independent I/O Requests} processed? \textbf{Small}, random I/O \textbf{requests} from \textbf{different applications or users}. RAID can \textbf{process them in parallel} on different disks. As result: \textbf{lower disk queue lengths and faster response times}.
    
    \item How is a \definition{Single Multiple-Block I/O Request} processed? A \textbf{large read or write operation} on a big file (e.g., a video or a database query). Striping lets \textbf{multiple disks work together} to serve this one big request. As result, \textbf{much faster data transfer} compared to using one disk.
\end{itemize}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Redundancy}}
\end{flushleft}
\definition{Redundancy} is the method of \textbf{adding extra information to protect data from disk failures}. If a disk fails, the system can \hl{reconstruct the missing data using the redundant information}. Redundancy is \emph{essential} because:
\begin{itemize}
    \item Disk arrays use many disk, and \textbf{more disk}, \textbf{higher risk of failure}.
    \item RAID introduces redundancy to \textbf{tolerate and recover from failures}.
\end{itemize}
There are two types of redundancy:
\begin{enumerate}
    \item \definition{Data Duplication (Mirroring)}. Every block of data is \textbf{copied exactly} to another disk. Used in RAID 1 and RAID 10.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] Can survive \textbf{complete disk failures}.
        \item[\textcolor{Green3}{\faIcon{check}}] Fast reads (can load-balance between mirrors).
        \item[\textcolor{Red2}{\faIcon{times}}] \textbf{Wastes 50\%} of storage (one disk stores only a copy).
    \end{itemize}
    
    \item \definition{Parity-Based Redundancy}. Adds a \textbf{calculated parity block} (usually XOR) that allows \textbf{rebuilding lost data}. Used in RAID 5 and RAID 6.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] More \textbf{storage-efficient} than mirroring.
        \item[\textcolor{Green3}{\faIcon{check}}] Can tolerate 1 (RAID 5) or 2 (RAID 6) disk failures.
        \item[\textcolor{Red2}{\faIcon{times}}] \textbf{Writes are slower} (need to read/update parity on each write).
    \end{itemize}
\end{enumerate}
\textcolor{Green3}{\faIcon{question-circle} \textbf{Why is it called ``Redundant Array''?}} Because RAID \textbf{stores more than just our data}, it stores the \textbf{redundant data necessary to protect it}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{RAID levels}}
\end{flushleft}
A \definition{RAID level} defines a specific way to organize \textbf{striping} and \textbf{redundancy} across multiple disks. Each level balances \textbf{three key trade-offs}:
\begin{itemize}
    \item[\faIcon{\speedIcon}] \textbf{Performance}: \emph{How fast are reads/writes?}
    \item[\faIcon{shield-alt}] \textbf{Fault Tolerance}: \emph{Can it survive disk failure?}
    \item[\faIcon{boxes}] \textbf{Storage Efficiency}: \emph{How much usable space is left?}
\end{itemize}

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} c | c | c | c | c @{}}
            \toprule
            RAID & Striping & Redundancy & Tolerates Failure? & Main Benefit \\
            \midrule
            0   & \textcolor{Green3}{\faIcon{check}}    & \textcolor{Red2}{\faIcon{times}} None                 & \textcolor{Red2}{\faIcon{times}} No           & Max speed, no protection  \\ [.3em]
            1   & \textcolor{Red2}{\faIcon{times}}      & \textcolor{Green3}{\faIcon{check}} Mirroring          & \textcolor{Green3}{\faIcon{check}} 1 disk     & Reliability via copies    \\ [.3em]
            5   & \textcolor{Green3}{\faIcon{check}}    & \textcolor{Green3}{\faIcon{check}} Parity (rotated)   & \textcolor{Green3}{\faIcon{check}} 1 disk     & Balance of perf $+$ space \\ [.3em]
            6   & \textcolor{Green3}{\faIcon{check}}    & \textcolor{Green3}{\faIcon{check}} Dual Parity        & \textcolor{Green3}{\faIcon{check}} 2 disks    & Extra protection          \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Standard RAID Levels: RAID 4 is not shown because it is rarely used; RAID 5 has the same concept but eliminates RAID 4's limitations. RAID 2 and 3 are also not shown because they are not covered in this course.}
\end{table}

\noindent
Each RAID level is a \textbf{design pattern} that answer: ``\emph{How can I spread and protect data across multiple disks to meet my performance, capacity, and reliability goals?}''.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        \textbf{Topic} & \textbf{Page} \\
        \midrule
        \texttt{RAID 0} & \hyperlink{RAID 0}{\hypergetpageref{RAID 0}} \\
        \texttt{RAID 1} & \hyperlink{RAID 1}{\hypergetpageref{RAID 1}} \\
        \texttt{RAID 0 + 1} & \hyperlink{RAID 0 + 1}{\hypergetpageref{RAID 0 + 1}} \\
        \texttt{RAID 1 + 0} & \hyperlink{RAID 1 + 0}{\hypergetpageref{RAID 1 + 0}} \\
        \texttt{RAID 4} & \hyperlink{RAID 4}{\hypergetpageref{RAID 4}} \\
        \texttt{RAID 5} & \hyperlink{RAID 5}{\hypergetpageref{RAID 5}} \\
        \texttt{RAID 6} & \hyperlink{RAID 6}{\hypergetpageref{RAID 6}} \\
        Comparison and characteristics of RAID levels & \hyperlink{Comparison and characteristics of RAID levels}{\hypergetpageref{Comparison and characteristics of RAID levels}} \\
        \bottomrule
    \end{tabular}
    \caption{RAID - Table of Contents.}
\end{table}

\newpage

\begin{center}\label{RAID 0}
    \large
    \hypertarget{RAID 0}{\textcolor{Red2}{\textbf{RAID 0 - Striping Without Redundancy}}}
\end{center}
\definition{RAID 0} \textbf{splits (stripes) data across multiple disks} to increase performance. It \textbf{does not provide redundancy}: if one disk fails, \textbf{all data is lost}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{toolbox} \textbf{Structure}}
\end{flushleft}
\begin{itemize}
    \item \important{Data Striping}: \textcolor{Green3}{\faIcon{check}} Yes (block-level)
    \item \important{Redundancy}: \textcolor{Red2}{\faIcon{times}} None
    \item \important{Minimum disks}: 2
    \item \important{Fault Tolerance}: \textcolor{Red2}{\faIcon{times}} 0 disks, a single failure is fatal
    \item \important{Storage Efficiency}: \textcolor{Green3}{\faIcon{check}} 100\% (all capacity is usable)
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When is it used?}}
\end{flushleft}
\textbf{High performance environments} where \textbf{data loss is acceptable}, such as temporary data storage, video editing, gaming, scratch disks or caches.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{\speedIcon} \textbf{Performance}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        Operation & Result \\
        \midrule
        \textbf{Reads}      & \textcolor{Green3}{\faIcon{check}} Much faster (parallel access)  \\ [.3em]
        \textbf{Writes}     & \textcolor{Green3}{\faIcon{check}} Much faster (split writes)     \\ [.3em]
        \textbf{Failure}    & \textcolor{Red2}{\faIcon{times}} One disk dies, \textbf{all lost} \\ 
        \bottomrule
    \end{tabular}
\end{table}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Lower cost} because it does not employ redundancy (no error-correcting codes are computed and stored).
    \item \textbf{Best read/write performance} (it does not need to update redundant data and is parallelized).
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Disadvantages}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Zero fault tolerance}.
    \item High risk: the failure rate is equal to the sum of all disk failure probabilities.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
The main idea of RAID 0 is to use several physical disks as \textbf{a single logical disk} by \textbf{striping} data across them (splitting data into small blocks and writing them in \textbf{round-robin fashion} across all $N$ disks). This approach improves performance:
\begin{itemize}
    \item \textbf{Sequential access} is distributed across all disks, increased transfer rate.
    \item \textbf{Random access} naturally spreads I/O load across disks, lower latency and better throughput.
\end{itemize}
The system uses the following formulas to \textbf{determine which disk and offset holds a given logical block}:
\begin{equation}
    \begin{array}{rcl}
        \texttt{Disk index}     &=& \texttt{logical\_block\_number} \: \% \: \texttt{number\_of\_disks} \\ [.5em]
        \texttt{Block Offset}   &=& \dfrac{\texttt{logical\_block\_number}}{\texttt{number\_of\_disks}}
    \end{array}
\end{equation}
For example, the command \texttt{read block 11} in a 4-disk array gives disk 3 as the index and block 2 as the offset on that disk.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{img/raid-1.pdf}
\end{figure}

\noindent
This formula \textbf{allows the RAID controller to locate any block \hl{without metadata}}, just with arithmetic.

\highspace
The choice of chunk size is critical because it affects performance at this level. The \definition{Chunk Size} (also called \definition{Stripe Unit} size) is the amount of \textbf{data written to each disk before moving to the next one}.

\highspace
The impact of chunk size is evident:
\begin{itemize}
    \item \textbf{Small} chunks $\rightarrow$ better \textbf{parallelism} (each access touches more disks).
    \item \textbf{Large} chunks $\rightarrow$ lower \textbf{seek overhead} (each file uses fewer disks).
\end{itemize}
In practice, typical RAID arrays use 64 KB chunks as a \textbf{balanced} (tradeoff) default.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-bar} \textbf{Measuring Performance}}
\end{flushleft}
\begin{itemize}
    \item \important{Measuring \emph{Sequential} Transfer Rate}. For \textbf{large sequential I/O}, the transfer time includes:
    \begin{enumerate}
        \item \textbf{Seek time} (page \pageref{def: Seek Time})
        \item \textbf{Rotational latency}
        \item \definition{Actual Data Transfer Time}:
        \begin{equation}
            S = \dfrac{\texttt{transfer\_size}}{\texttt{time\_to\_access}} = \dfrac{\texttt{data\_size}}{\texttt{disk\_rate}}
        \end{equation}
    \end{enumerate}
    For example, suppose there is 7 ms of \emph{seek time}, 3 ms of \emph{rotational latency}, and 10 MB of \emph{data to transfer} at a \emph{disk rate} of 50 MB/s. The total time is given by the seek time, rotational latency, and the time taken to transfer the data:
    \begin{equation*}
        \text{Time} = 7\,\text{ms} + 3\,\text{ms} + \left( 10\,\text{MB} \div 50\,\text{MB/s} \right) = 210\,\text{ms}
    \end{equation*}
    Thus, the sequential throughput is:
    \begin{equation*}
        S = 10\,\text{MB} \div 0.21\,\text{s} = 47.62\,\text{MB/s}
    \end{equation*}
    In RAID 0 with $N$ disks, the \definitionWithSpecificIndex{Total Sequential Throughput}{RAID 0 - Total Sequential Throughput}{} is:
    \begin{equation}
        \text{Total Sequential Throughput} = N \times S
    \end{equation}
    $S$ is the sequential throughput for a single disk in the system.


    \item \important{Measuring \emph{Random} Transfer Rate}. For \textbf{small random accesses}, latency dominates over transfer. Using the same example as before, but changing the file size to 10 KB, the time is as follows:
    \begin{equation*}
        \text{Time} = 7\,\text{ms} + 3\,\text{ms} + \left( 10\,\text{KB} \div 50\,\text{MB/s} \right) = 0.98\,\text{MB/s}
    \end{equation*}
    Resulting random throughput:
    \begin{equation*}
        R = 10\,\text{KB} \div 10.2\,\text{ms} = 0.98\,\text{MB/s}
    \end{equation*}
    In RAID 0 with $N$ disks, the \definitionWithSpecificIndex{Total Random Throughput}{RAID 0 - Total Random Throughput}{} is:
    \begin{equation}
        \text{Total Random Throughput} = N \times R
    \end{equation}
    $R$ is the random throughput for a single disk in the system.
\end{itemize}

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={0.9\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l | p{27em} @{}}
            \toprule
            Feature & Description \\
            \midrule
            \textbf{Capacity}       & $N$, all disk space is usable (no redundancy) \\ [.3em]
            \textbf{Reliability}    & 0, MTTDL (Mean Time To Data Loss) = MTTF (Mean Time To Failure) \\ [.3em]
            \textbf{Performance}    & It provides full parallelization for both random and sequential throughput \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\newpage

\begin{center}\label{RAID 1}
    \large
    \hypertarget{RAID 1}{\textcolor{Red2}{\textbf{RAID 1}}}
\end{center}
RAID 0 offers performance, but \textbf{no protection} against failures. \textbf{\definition{RAID 1} solves this} by maintaining \textbf{two identical copies of all data} (mirroring).
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{High reliability}}: if one disk fails, the second copy is used seamlessly.
    \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Fast reads}}: can read from either disk (or balance across them).
    \item[\textcolor{Red2}{\faIcon{exclamation-triangle}}] The \textcolor{Red2}{\textbf{write speed}} is slightly slower than that of a single disk because the \textbf{data must be written to both disks}.
    \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{High cost}}: only \textbf{50\% of total disk space} is usable.
\end{itemize}
In theory, RAID 1 can use \textbf{more than 2 disks} to mirror data, but rarely done due to \textbf{very high costs} (only $1 \div N$ capacity used), and too much overhead for typical storage needs. In other words, more than 2-way mirroring is possible but \textbf{not practical}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why combined RAID levels at all?}}
\end{flushleft}
RAID 1 is great, but it has its \textbf{limits}. It mirrors data, meaning one disk has the data and the other disk has a copy of it. Therefore, we always need pairs of disks. With two disks, RAID 1 is simple and reliable. But what if we want to use more than two disks and want both high reliability and high performance? We want to \textbf{combine the reliability that RAID 1 provides with the performance and scalability that RAID 0 provides}. In other words, how do we structure the disks as their number increases to get the best of both RAID 0 and RAID 1? The answer lies in the RAID 10 and RAID 01 techniques.

\highspace
The goal of combining RAID levels is to achieve both reliability and performance. The combination is \textbf{generally written as} RAID $x+y$ (sometimes called RAID $xy$), meaning we \textbf{first} apply RAID $x$ to small groups of disks and \textbf{then} apply RAID $y$ on top of those groups.

\highspace
We divide $n \times m$ total disks into $m$ groups, each containing $n$ disks.
\begin{enumerate}
    \item Apply RAID $x$ to each group.
    \item Apply RAID $y$ to the $m$ groups, treating each group as a ``logical disk''.
\end{enumerate}
There are two possible combinations at this level:
\begin{itemize}
    \item \label{RAID 0 + 1} \definition{RAID 0 $+$ 1} is a \textbf{striped array of mirrored sets}. It combines the performance of RAID 0 and the fault tolerance of RAID 1, but with \textbf{some limitations}.

    The structure consists of a \textbf{minimum of 4 disks}:
    \begin{enumerate}
        \item \important{Start with striping} (RAID 0): split data across 2 or more disks for performance.
        \item \important{Then mirror the whole striped group} (RAID 1).
    \end{enumerate}

    \newpage

    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.39\textwidth]{img/raid-3.pdf}
    \end{figure}

    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{RAID 0 $+$ 1: Failure Behavior}}. If \textbf{any one disk fails}, the system still works using the mirrored group. But if \textbf{a disk fails in each stripe}, \textbf{all data is lost}. After the \textbf{first failure}, the array \textbf{degrades into pure RAID 0}, losing redundancy.

    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | l @{}}
            \toprule
            Aspect & Value \\
            \midrule
            Usable space    & 50\% (half is mirror) \\ [.3em]
            Read speed      & \textcolor{Green3}{\faIcon{check}} High (parallel reads) \\ [.3em]
            Write speed     & \textcolor{Red2}{\faIcon{exclamation-triangle}} Slower (must write both copies) \\ [.3em]
            Reliability     & \textcolor{Green3}{\faIcon{check}} Tolerates 1 disk failure (in same group) \\
            \bottomrule
        \end{tabular}
    \end{table}


    \item \label{RAID 1 + 0} \definition{RAID 1 $+$ 0} combines \textbf{mirroring (RAID 1)} and \textbf{striping (RAID 0)}. First, data is \textbf{mirrored}, then it is \textbf{striped} across mirrored pairs. It offers \textbf{high performance (like RAID 0)}, \textbf{high fault tolerance (like RAID 1)}.
    
    The structure consists of a \textbf{minimum of 4 disks}:
    \begin{enumerate}
        \item \important{Create mirrored pairs} (RAID 1)
        \item \important{Stripe data across pairs} (RAID 0)
    \end{enumerate}

    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.39\textwidth]{img/raid-4.pdf}
    \end{figure}

    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{RAID 1 $+$ 0: Failure Tolerance}}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] Can survive \textbf{multiple disk failures}, as long as \textbf{only one per mirrored pair}.
        \item[\textcolor{Red2}{\faIcon{times}}] If both disks in a mirrored pair fail, data loss.
    \end{itemize}
    RAID 10 is \textbf{\underline{more fault-tolerance than RAID 01}}, especially in cases of multiple failures. Because each mirrored pair in RAID 10 is self-contained:
    \begin{itemize}
        \item If one disk in a pair fails, only \textbf{that pair} needs to be rebuilt (not the entire array).
        \item The rest of the array \textbf{remains fully operational} and can keep serving requests.
    \end{itemize}
    In RAID 01:
    \begin{itemize}
        \item A single disk failure disables \textbf{half of the array}.
        \item A second failure in the other half $=$ total data loss.
    \end{itemize}

    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | l @{}}
            \toprule
            Aspect & Value \\
            \midrule
            Usable space    & 50\% of total (because of mirroring) \\ [.3em]
            Read speed      & \textcolor{Green3}{\faIcon{check}} Very fast (parallel reads from mirrors) \\ [.3em]
            Write speed     & \textcolor{Green3}{\faIcon{check}} Faster than RAID 5/6 (no parity calc.) \\ [.3em]
            Reliability     & \textcolor{Green3}{\faIcon{check}} High (survives multiple failures safely) \\
            \bottomrule
        \end{tabular}
    \end{table}

    Widely used in databases, transactional systems, and virtualized environments where both performance and fault tolerance are critical.
\end{itemize}
In conclusion, RAID 10 is created by \textbf{first mirroring disks} and \textbf{then striping} across the mirrored pairs. This configuration provides \textbf{faster read and write} speeds and \textbf{greater fault tolerance} than RAID 01, making it ideal for high-load systems.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-bar} \textbf{Measuring Performance}}
\end{flushleft}
\begin{itemize}
    \item \definitionWithSpecificIndex{Capacity}{RAID 1 - Capacity}{}. Total usable space is $N \div 2$, since each block is stored twice, only \textbf{50\% of disk space} is usable.
    \item \important{Reliability}. Can \textbf{survive 1 disk failure} guaranteed. In bets-case scenarios, can survive \textbf{up to $N \div 2$ disk failures}, if no two are mirrors of each other. However, if \textbf{both disks in a mirrored pair fail}, data is lost.
    \item \important{\emph{Sequential} Performance}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{\speedIcon}}] \definitionWithSpecificIndex{Sequential Write}{RAID 1 - Sequential Write}{}. Writes must go to both copies, then halved throughput:
        \begin{equation}
            \text{Sequential Write} = \left(\dfrac{N}{2}\right) \times S
        \end{equation}
        \item[\textcolor{Green3}{\faIcon{\speedIcon}}] \definitionWithSpecificIndex{Sequential Read}{RAID 1 - Sequential Read}{}. In theory, reads could be optimized, but usually reads are directed to \textbf{only one disk per mirror}. So, \textbf{half of the disks are idle} during reads:
        \begin{equation}
            \text{Sequential Read} = \left(\dfrac{N}{2}\right) \times S
        \end{equation}
    \end{itemize}
    \item \important{\emph{Random} Access Performance}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{\speedIcon}}] \definitionWithSpecificIndex{Random Read}{RAID 1 - Random Read}{}. \textbf{Best case} for RAID 1. Reads can be \textbf{load-balanced} across all $N$ disks. System can choose which mirror is less busy, then fully parallel.
        \begin{equation}
            \text{Random Read} = N \times R
        \end{equation}
        \item[\textcolor{Red2}{\faIcon{exclamation-triangle}}] \definitionWithSpecificIndex{Random Write}{RAID 1 - Random Write}{}. Writes must go to both disks in each pair. Only $N \div 2$ mirrors, then only $N \div 2$ writes happen in parallel.
        \begin{equation}
            \text{Random Write} = \left(\dfrac{N}{2}\right) \times R
        \end{equation}
    \end{itemize}
\end{itemize}
RAID 1 provides \textbf{excellent reliability} and \textbf{great random read performance}. Its main limitations are only 50\% of storage is usable, and sequential throughput is limited to half the potential.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{The Consistent Update Problem}}
\end{flushleft}
In RAID 1, every write must be done \textbf{twice} (once per mirror). This raises a \textbf{consistency issue}: \emph{what if the system crashes or loses power after writing to one disk, but before writing to the other?}
\begin{itemize}
    \item One mirror has the \textbf{updated data}
    \item The other mirror has \textbf{state (old) data}
    \item The two copies are now \textbf{out of sync}
\end{itemize}
This violates the \textbf{atomicity} principle (a write must be \textbf{all or nothing}, either all mirrors are updated or none).

\highspace
\textcolor{Green3}{\faIcon{tools} \textbf{How it's handled}}. Many RAID controllers implement a \definition{Write-Ahead Log (WAL)}:
\begin{itemize}
    \item A special \textbf{non-volatile memory area} (battery-backed cache).
    \item Writes are first saved into the log \textbf{before} writing to disks.
    \item If power fails, the controller can use the log to:
    \begin{itemize}
        \item Complete the write
        \item Or roll it back (to restore consistency)
    \end{itemize}
\end{itemize}
This ensures that mirrored copies stay \textbf{synchronized}, even after failure.

\newpage

\begin{center}\label{RAID 4}
    \large
    \hypertarget{RAID 4}{\textcolor{Red2}{\textbf{RAID 4}}}
\end{center}
\definition{RAID 4} uses \textbf{block-level striping} (like RAID 0), but adds a \textbf{dedicated parity disk} to provide \textbf{fault tolerance}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Parity Calculation}}
\end{flushleft}
Parity is computed using the \textbf{XOR (exclusive OR)} operation. For example, 4 blocks per stripe:
\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/raid-6.pdf}
    \caption{RAID 4 - \emph{How does it work?}}
\end{figure}

\noindent
It is used XOR because it has a special property: if we know all but one value, we can \textbf{reconstruct the missing one}. For example, if Disk 2 is lost, it can be reconstructed using an XOR operation with Disks 0, 1, and 3 and the Parity unit.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Pros}} and \textcolor{Red2}{\faIcon{times-circle} \textbf{Cons}}
\end{flushleft}
\begin{itemize}
    \item[\textcolor{Green3}{\faIcon{check-circle}}] Fault tolerance: can survive \textbf{1 disk failure}.
    \item[\textcolor{Green3}{\faIcon{check-circle}}] More \textbf{space-efficient} than RAID 1 (only 1 disk used for redundancy).
    \item[\textcolor{Red2}{\faIcon{times-circle}}] \textbf{Parity disk is a bottleneck}. Every write must update parity, then a single disk becomes overloaded.
    \item[\textcolor{Red2}{\faIcon{times-circle}}] Write performance is poor compared to RAID 0 or RAID 10.
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
\begin{itemize}
    \item \important{Updating Parity on Write in RAID 4}. When we \textbf{modify a block} in RAID 4, we must also update the \textbf{parity block} in that stripe. There are two ways to do it:
    \begin{enumerate}
        \item \definitionWithSpecificIndex{Additive Parity}{RAID 4 - Additive Parity}{}. Recalculate parity using all the blocks in the stripe. Steps:
        \begin{enumerate}
            \item \textbf{Read all other data blocks} in the stripe.
            \item XOR them together to get the \textbf{new parity}.
        \end{enumerate}
        \textcolor{Green3}{\faIcon{check-circle} \textbf{Simple and correct}}, but \textcolor{Red2}{\faIcon{times-circle} \textbf{expensive}} because must \textbf{read all blocks} to recompute.
        \item \definitionWithSpecificIndex{Subtractive Parity}{RAID 4 - Subtractive Parity}{} (Efficient: modify incrementally). Update the parity by using a \textbf{mathematical trick}:
        \begin{equation}
            \text{New Parity} = \text{Old Parity} \oplus \text{Old Data} \oplus \text{New Data}
        \end{equation}
        This works thanks to the \textbf{reversible} XOR property:
        \begin{equation*}
            A \oplus A = 0,\quad  A \oplus 0 = A
        \end{equation*}
        \textcolor{Green3}{\faIcon{check-circle} \textbf{Very efficient}} because only need to read the block being written (\textbf{old value}) and the \textbf{old parity block}.
    \end{enumerate}


    \item \important{Reads in RAID 4}. Reads in RAID 4 are \textbf{fast and efficient}, especially compared to writes. Data is \textbf{striped across all data disks}, and the \textbf{parity disk is \underline{not involved}} in normal reads. There are two possible read scenarios:
    \begin{enumerate}
        \item \important{Sequential Read}. Large, continuous read (e.g., loading big file). Data is read in \textbf{parallel} from all data disks. It is \textbf{fast}, each disk reads part of the file simultaneously.

        \item \important{Random Read}. Small, scattered reads (e.g., database lookups). Still efficient because:
        \begin{itemize}
            \item Each disk can handle \textbf{its own read} in parallel.
            \item Parity disk is \textbf{not touched} during a normal read.
        \end{itemize}
    \end{enumerate}
    As we saw at the beginning, if a data \textbf{disk fails}, the missing block is \textbf{reconstructed using XOR}:
    \begin{equation*}
        D_{\text{missing}} = P \oplus D_{1} \oplus D_{2} \oplus D_{3} \oplus \dots
    \end{equation*}
    Read recovery is still possible, it is just slower.


    \item \important{Serial Writes in RAID 4}. Serial writes mean writing \textbf{a sequence of blocks} that all belong to the \textbf{same stripe}, typically during large, contiguous writes (e.g., streaming, backups).
    
    In RAID 4, data is \textbf{striped} across all data disks. The \textbf{parity for each stripe} is stored in a \textbf{dedicated parity disk}.

    During serial writes, each data disk receives \textbf{one block} from the stripe. The \textbf{parity disk} is updated \textbf{once per stripe}. Since all writes hit the same stripe, the \textbf{parity disk becomes a bottleneck}.

    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | l @{}}
            \toprule
            Operation & Description \\
            \midrule
            Data writes     & \textcolor{Green3}{\faIcon{check}} Parallel across data disks \\ [.3em]
            Parity update   & \textcolor{Red2}{\faIcon{times}} Centralized \\ [.3em]
            Bottleneck      & Parity disk handles \textbf{every stripe's write} \\ [.3em]
            Comparison      & Same structure as read, then but \textbf{parity slows it} \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    Even though data writes are parallel, all serial writes in the same stripe \textbf{converge on the parity disk}, making it the \textbf{main point of contention}.


    \item \important{Random Writes in RAID 4}. A \textbf{random write} updates \textbf{one block} somewhere on disk, not the entire stripe. This is common in databases, file systems, and OS workloads.

    The writing process for each block is as follows:
    \begin{enumerate}
        \item \important{Read} the \textbf{old data block} and the \textbf{old parity block}.
        \item \important{Compute} the parity block:
        \begin{equation*}
            P_{\text{new}} = D_{\text{old}} \oplus D_{\text{new}} \oplus P_{\text{old}}
        \end{equation*}
        This efficiently updates parity using subtraction (XOR).
        \item \important{Write} the \textbf{new data block} and the \textbf{updated parity block}
    \end{enumerate}
    \textcolor{Red2}{\faIcon{exclamation-triangle}} The main problem here is the \textbf{bottleneck} caused by the parity disk. Even though the data block may change on \textbf{any disk}, the \textbf{parity block is always on the same disk}. \hl{Each write operation} must \textbf{read from} and \textbf{write to} that disk. This creates \textbf{serialization}, only one parity update can occur at a time. Even with multiple writes to different data disks, they all \textbf{wait for the parity disk}.

    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | l @{}}
            \toprule
            Operation & Behavior \\
            \midrule
            Data writes     & \textcolor{Green3}{\faIcon{check}} Fast (per disk) \\ [.3em]
            Parity update   & \textcolor{Red2}{\faIcon{times}} Always goes to the \textbf{same disk} \\ [.3em]
            Bottleneck      & \textcolor{Red2}{\faIcon{exclamation-triangle}} Parity disk handles \textbf{every write} \\ [.3em]
            Overall         & \textcolor{Red2}{\faIcon{times-circle}} \textbf{Terrible performance} on random writes \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-bar} \textbf{Measuring Performance}}
\end{flushleft}
\begin{itemize}
    \item \definitionWithSpecificIndex{Capacity}{RAID 4 - Capacity}{}: $N - 1$. One disk is used exclusively for parity, so it is \textbf{not usable for data}.
    \item \definitionWithSpecificIndex{Reliability}{RAID 4 - Reliability}{}: can tolerate \textbf{1 disk failure} (any one of the data or parity disks). But if a disk fails, the system can still serve data using parity reconstruction. However, write and read performance becomes \textbf{massively degraded} (especially if parity disk is affected).
    \item \textbf{Read/Write Performance}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | c | p{18em} @{}}
            \toprule
            Operation & Performance & Notes \\
            \midrule
            \definitionWithSpecificIndex{Sequential Read}{RAID 4 - Sequential Read}{}   & $(N - 1) \times S$ & \textcolor{Green3}{\faIcon{check}} Full parallelism across all data disks \\ [.3em]
            \definitionWithSpecificIndex{Sequential Write}{RAID 4 - Sequential Write}{} & $(N - 1) \times S$ & \textcolor{Green3}{\faIcon{check}} Parallel as long as writes hit different stripes \\ [.3em]
            \definitionWithSpecificIndex{Random Read}{RAID 4 - Random Read}{}           & $(N - 1) \times R$ & \textcolor{Green3}{\faIcon{check}} Reads avoid parity disk, then high parallel performance \\ [.3em]
            \definitionWithSpecificIndex{Random Write}{RAID 4 - Random Write}{}         & $R \div 2$         & \textcolor{Red2}{\faIcon{times}} Terrible, every write must access parity disk twice \\
            \bottomrule
        \end{tabular}
    \end{table}

    Parity disk becomes a \textbf{point of contention for every write}. Causes \textbf{serialization of random writes}, reducing overall throughput drastically.
\end{itemize}
In conclusion, RAID 4 provides ample capacity and quick sequential input/output (I/O) thanks to block-level striping. It uses one dedicated parity disk for fault recovery. However, its \textbf{main weakness is its poor random write performance}, as all writes target the same parity disk. This is why RAID 4 is rarely used in practice and \textbf{led to the development of RAID 5, which solves this issue by distributing parity}.

\newpage

\begin{center}\label{RAID 5}
    \large
    \hypertarget{RAID 5}{\textcolor{Red2}{\textbf{RAID 5}}}
\end{center}

\noindent
\definition{RAID 5} extends RAID 4 by using \textbf{distributed (rotating) parity}. This removes the \textbf{bottleneck of the single parity disk}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
In RAID 4, parity is always stored on a \textbf{dedicated parity disk}, causes a bottleneck. In RAID 5, parity is \textbf{spread (rotated)} across \textbf{all disks}, \textbf{one stripe at a time}.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/raid-9.pdf}
\end{figure}

\noindent
\textbf{Parity position rotates} from one disk to the next. So \textbf{each disk holds both data and some parity}; there's no dedicated parity disk. However, parity is still computed using the XOR operation, and the operand depends on the disks that calculate it. For example, Disk 4 will calculate the XOR using Disks 0, 1, 2, and 3. Disk 3 will use disks 0, 1, 2, and 4, and so on for the others.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What happens during a Random Write?}}
\end{flushleft}
Like in RAID 4, RAID 5 must \textbf{preserve parity} whenever data changes. A \textbf{random write} modifies only one block in a stripe, so we use an efficient \textbf{subtractive parity update}.
\begin{enumerate}
    \item \textbf{Read}
    \begin{enumerate}
        \item Read the \emph{old data block}
        \item Read the \emph{old parity block}
    \end{enumerate}
    \item \textbf{Compute}. Use XOR subtraction:
    \begin{equation*}
        P_{\text{new}} = D_{\text{old}} \oplus D_{\text{new}} \oplus P_{\text{old}}
    \end{equation*}
    
    \newpage
    
    \item \textbf{Write}
    \begin{enumerate}
        \item Write the \emph{new data block}
        \item Write the \emph{updated parity block}
    \end{enumerate}
\end{enumerate}
In total, there are \textbf{4 disk operations}: 2 reads (old data and old parity) and 2 writes (new data and new parity).

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{\speedIcon} \textbf{RAID 5 improvement over RAID 4}}
\end{flushleft}
In \textbf{RAID 4}, all parity writes go the \textbf{same disk}, and this creates a \textcolor{Red2}{\textbf{bottleneck}}. \textbf{RAID 5 solves} this slowdown by \textcolor{Green3}{\textbf{rotating the parity}}. Parity updates are spread across all disks, which distributes the load evenly. This allows \textbf{random writes to occur in parallel}, eliminating serialization due to a single disk. This is obviously much better than RAID 4.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-bar} \textbf{Analysis}}
\end{flushleft}
\begin{itemize}
    \item \definitionWithSpecificIndex{Capacity}{RAID 5 - Capacity}{}: $N - 1$ disks. Same as RAID 4, one disk's worth of capacity is used for \textbf{\underline{distributed} parity}.

    \item \definitionWithSpecificIndex{Reliability}{RAID 5 - Reliability}{}: tolerates $1$ \textbf{disk failure}. However, during partial outage the degraded mode phenomena is highlighted, because all missing data must be reconstructed on-the-fly using parity, which causes massive performance degradation.
    
    \item \textbf{Read/Write Performance}
    \begin{table}[!htp]
        \centering
        \begin{tabular}{@{} l | c | p{18em} @{}}
            \toprule
            Operation & Performance & Notes \\
            \midrule
            \definitionWithSpecificIndex{Sequential Read}{RAID 5 - Sequential Read}{}   & $(N - 1) \times S$        & \textcolor{Green3}{\faIcon{equals}} Same as RAID 4, striping across data disks \\ [.3em]
            \definitionWithSpecificIndex{Sequential Write}{RAID 5 - Sequential Write}{} & $(N - 1) \times S$        & \textcolor{Green3}{\faIcon{check}} Fully parallel if stripes span across disks \\ [.3em]
            \definitionWithSpecificIndex{Random Read}{RAID 5 - Random Read}{}           & $N \times R$              & \textcolor{Green3}{\faIcon{\speedIcon}} Better than RAID 4, all disks can be read in parallel \\ [.4em]
            \definitionWithSpecificIndex{Random Write}{RAID 5 - Random Write}{}         & $\dfrac{N}{4} \times R$    & \textcolor{Green3}{\faIcon{\speedIcon}} Better than RAID 4, writes are distributed. However, each write requires an additional 2 reads and 2 writes for parity updates. \\
            \bottomrule
        \end{tabular}
    \end{table}

    For each random write, we have 2 reads and 2 writes with $N$ disks and parallelism. The \definitionWithSpecificIndex{throughput}{RAID 5 - Throughput}{} is:
    \begin{equation}
        \text{Throughput} \approx \dfrac{N}{4} \times R
    \end{equation}
\end{itemize}
In conclusion, RAID 5 provides a good \textbf{trade-off between capacity}, \textbf{reliability}, and \textbf{performance}. It avoids RAID 4's parity bottleneck by using \textbf{rotating parity}, enabling better parallelism for \textbf{random writes}. However, in case of a disk failure, \textbf{performance degrades} significantly due to \textbf{on-the-fly parity reconstruction}.

\newpage

\begin{center}\label{RAID 6}
    \large
    \hypertarget{RAID 6}{\textcolor{Red2}{\textbf{RAID 6}}}
\end{center}
RAID 6 can tolerate multiple disk faults (\textbf{more fault tolerance}) concerning RAID 5. It tolerates two concurrent failures.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
It uses Solomon-Reeds codes with two redundancy schemes and requires $N + 2$ disks (where $N$ is the number of disks). Note that the \textbf{minimum set is 4 data disks}.

Unfortunately, it has a \textbf{high overhead for writes} (computation of parities) because each write requires six disk accesses due to the need to update both the P and Q parity blocks (slow writes).

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/raid-010.pdf}
\end{figure}

\newpage

\begin{center}\label{Comparison and characteristics of RAID levels}
    \large
    \hypertarget{Comparison and characteristics of RAID levels}{\textcolor{Red2}{\textbf{Comparison and characteristics of RAID levels}}}
\end{center}

\noindent
The following table shows eight fundamental properties of the RAID levels.
\begin{itemize}
    \item $N$: number of drives
    \item $R$: random access speed
    \item $S$: sequential access speed
    \item $D$: latency to access a single disk
\end{itemize}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l l l l @{}}
        \toprule
        & \texttt{RAID 0} & \texttt{RAID 1} & \texttt{RAID 4} & \texttt{RAID 5} \\
        \midrule
        Capacity & $N$ & $N \div 2$ & $N - 1$ & $N - 1$ \\
        Reliability & $0$ & $1 \: \lor : N \div 2$ & $1$ & $1$ \\
        \cmidrule{1-5}
        Sequential Read & $N \times S$ & $\left(N \div 2\right) \times S$ & $\left(N - 1\right) \times S$ & $\left(N - 1\right) \times S$ \\
        Sequential Write & $N \times S$ & $\left(N \div 2\right) \times S$ & $\left(N - 1\right) \times S$ & $\left(N - 1\right) \times S$ \\
        Random Read & $N \times R$ & $N \times R$ & $\left(N - 1\right) \times R$ & $N \times R$ \\
        Random Write & $N \times R$ & $\left(N \div 2\right) \times R$ & $R \div 2$ & $\left(N \div 4\right) \times R$ \\
        \cmidrule{1-5}
        Read & $D$ & $D$ & $D$ & $D$ \\
        Write & $D$ & $D$ & $2 \times D$ & $2 \times D$ \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of RAID levels.}
\end{table}

\noindent
Where the throughput is:
\begin{itemize}
    \item Sequential Read
    \item Sequential Write
    \item Random Read
    \item Random Write
\end{itemize}
And the latency is:
\begin{itemize}
    \item Read
    \item Write
\end{itemize}

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/raid-011.pdf}
    \caption{Characteristics of RAID levels.}
\end{figure}

\noindent
\textbf{RAID 0} has the \underline{best performance} and \underline{most capacity}.

\highspace
\textbf{RAID 1} (10 better than 01) or \textbf{RAID 6} have the \underline{greatest error recovery}.

\highspace
\textbf{RAID 5} has the better \textbf{\emph{balance}} between \underline{space}, \underline{performance}, and \underline{recoverability}.

\newpage

\paragraph{DAS, NAS and SAN}\label{paragraph: DAS, NAS and SAN}

As the last argument, we introduce \textbf{three different typologies} of storage systems:
\begin{itemize}
	\item \definition{Direct Attached Storage (DAS)} is a \textbf{storage system directly attached to a server or workstation}. They are visible as disks/volumes by the client OS.
	
	\item \definition{Network Attached Storage (NAS)} is a \textbf{computer connected to a network that provides only file-based data storage services} (e.g. FTP, Network File System) to other devices on the network and is visible as File Server to the client OS.
	
	\item \definition{Storage Area Networks (SAN)} are \textbf{remote storage units connected to a PC using a specific networking technology} (e.g. Fiber Channel) and are visible as disks/volumes by the client OS.
\end{itemize}
In the following schema, we can see a simple architectural comparison.
\begin{figure}[!htp]
	\centering
	\includegraphics[width=.8\textwidth]{img/nas-san-das-1.pdf}
\end{figure}

\begin{flushleft}
	\textcolor{Red2}{\faIcon{check} \textbf{DAS features}}
\end{flushleft}
DAS is a \textbf{storage system directly attached to a server or workstation}. The term is used to differentiate non-networked storage from SAN and NAS. The \textbf{main features} are:
\begin{itemize}
	\item Limited scalability.
	\item Complex manageability.
	\item Limited performance.
	\item To read files in other machines (the "file sharing" protocol of the OS must be used).
\end{itemize}
Note that all the \textbf{external disks connected to a PC with a point-to-point protocol can be considered DAS}.
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{img/nas-san-das-2.pdf}
	\caption{DAS architecture.}
\end{figure}

\highspace
\begin{flushleft}
	\textcolor{Red2}{\faIcon{check} \textbf{NAS features}}
\end{flushleft}
A NAS unit is a \textbf{computer connected to a network that provides only file-based data storage services to other devices on the network}. NAS systems contain one or more hard disks, often organized into logical redundant storage containers or RAID. Finally, \textbf{NAS provides file-access services to the hosts connected to a TCP/IP network through Networked File Systems/SAMBA}. Each NAS element has its IP address. Furthermore, each NAS has good scalability.

\highspace
The \textbf{main differences between DAS and NAS} are:
\begin{itemize}
	\item DAS is simply an \textbf{extension of an existing server} and is \textbf{not necessarily networked}.
	\item NAS is designed as an easy and self-contained solution for \textbf{sharing files over the network}.
\end{itemize}
Regarding \textbf{performance}, NAS depends mainly on the speed and congestion of the network.
\begin{figure}[!htp]
	\centering
	\includegraphics[width=.7\textwidth]{img/nas-san-das-3.pdf}
	\caption{NAS architecture.}
\end{figure}

\highspace
\begin{flushleft}
	\textcolor{Red2}{\faIcon{check} \textbf{SAN features}}
\end{flushleft}
SANs are remote \textbf{storage units connected to servers using a specific networking technology}. SANs have a particular network dedicated to accessing storage devices. It has two distinct networks: one TCP/IP and another dedicated network (e.g. Fiber Channel). It has a high scalability.

\highspace
The \textbf{main difference between a NAS and a SAN} is that:
\begin{itemize}
	\item \textbf{NAS appears to the client OS as a file server}. Then, the client can map network drives to shares on that server.
	
	\item \textbf{A disk available through a SAN still appears to the client OS as a disk}. It will be visible in the disks and volumes management utilities (along with the client's disks) and available to be formatted with a file system.
\end{itemize}
\begin{figure}[!htp]
	\centering
	\includegraphics[width=.8\textwidth]{img/nas-san-das-4.png}
	\caption{SAN architecture.}
\end{figure}

\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{img/nas-san-das-5.pdf}
	\caption{DAS vs. NAS vs. SAN}
\end{figure}

\newpage

\subsubsection{Networking (architecture and technology)}\label{subsubsection: Networking (architecture and technology)}

\paragraph{Fundamental concepts}

In the data centre, servers' \emph{performance increases} over time, and the demand for inter-server \emph{bandwidth also increases}.

\highspace
A \example{solution} can be to double the aggregate compute capacity or the aggregate storage simply by \textbf{doubling the number of compute or storage elements}.

\highspace
The \definition{doubling leaf bandwidth} is used since the networking has no straightforward horizontal scaling solution. Then, with \textbf{twice as many servers}, we will have \textbf{twice as many network ports and thus twice as much bandwidth}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is a bisection bandwidth?}}
\end{flushleft}
\definition{Bisection bandwidth} is a \textbf{measure of network performance}, \textbf{defined as the bandwidth available between two equal-sized partitions when a network is bisected}. This measure accounts for the \emph{bottleneck bandwidth of the entire network}, providing a representation of the actual bandwidth available in the system. The bisection should be done to minimize the bandwidth between the two partitions. It is often used to evaluate and compare networks for parallel architectures, including point-to-point communication systems or on-chip micro-networks.

\highspace
\textbf{Assuming that every server needs to talk to every other server, we need to double not just leaf bandwidth but bisection bandwidth}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How to design a data centre network?}}
\end{flushleft}
There are many design principles to follow:
\begin{itemize}
    \item \textbf{Very scalable} in order to support a vast number of servers;
    \item \textbf{Minimum cost} in terms of basic building blocks (e.g. switches);
    \item \textbf{Modular} to reuse simple basic modules;
    \item \textbf{Reliable} and \textbf{resilient};
    \item It may exploit novel/proprietary technologies and protocols incompatible with legacy Internet.
\end{itemize}

\newpage

\noindent
The \definition{Data Center Network (DCN)} connects a data centre's computing and storage units to achieve optimum performance. It can be \textbf{classified} into \textbf{three main categories}:
\begin{itemize}
    \item \definition{DCN Switch-centric architectures}. DCN uses \textbf{switches to perform packet forwarding}.\footnote{\definition{Packet forwarding} is the passing of packets from one network segment to another by nodes on a computer network.}

    \item \definition{DCN Server-centric architectures}. DCN \textbf{uses servers with multiple Network Interface Cards (NICs)}\footnote{A \definition{Network Interface Cards (NICs)} is a computer hardware component that connects a computer to a computer network.} \textbf{to act as switches} and perform other computational functions.

    \item \definition{DCN Hybrid architectures}. DCN \textbf{combines switches and servers} for packet forwarding.
\end{itemize}

\newpage

\paragraph{Switch-centric: classical Three-Tier architecture}

The \definition{Three-Tier architecture}, also called \definition{Three Layer architecture}, \textbf{configures the network in three different layers}:
\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/networking-1.pdf}    
\end{figure}

\noindent
It is a simple Data Center Network topology. 
\begin{itemize}
    \item The \textbf{servers} are \textbf{connected to the DCN through access switches}.
    \item \textbf{Each access-level switch is connected to at least two aggregation-level switches}.
    \item \textbf{Aggregation-level switches are connected to core-level switches} (gateways).
\end{itemize}
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Bandwidth can be increased} by \textbf{increasing the switches at the core and aggregation layers}, and by using routing protocols such as Equal Cost Multiple Path (ECMP) that equally shares the traffic among different routes.
    \item \textbf{Very simple} solution.
\end{enumerate}
\begin{flushleft}
    \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Cons}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Very expensive} in large data centers because the upper layers require faster network equipments.
    \item \textbf{Cost very high} in term of acquisition and energy consumption.
\end{enumerate}
In the \textbf{access layer}, there are two possible architectures:
\begin{itemize}
    \item \definition{ToR (Top-of-Rack) architecture}. All servers in a rack are connected to a ToR access switch within the same rack. The aggregation switches are in dedicated racks or shared racks with other ToR switches and servers.\label{ToR (Top-of-Rack) architecture}
    
    \begin{flushleft}
        \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
    \end{flushleft}
    \begin{enumerate}
        \item \textbf{Simpler cabling} because the number of cables is limited.
        \item \textbf{Lower costs} because the number of ports per switch is limited.
    \end{enumerate}

    \begin{flushleft}
        \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Cons}}
    \end{flushleft}
    \textbf{Higher complexity} for switch management.

    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.9\textwidth]{img/networking-2.pdf}
        \caption{ToR (Top-of-Rack) architecture.}
    \end{figure}

    
    \item \definition{EoR (End-of-Row) architecture}. Aggregation switcher are positioned one per corridor, at the end of a line of rack. Servers in a racks are connected directly to the aggregation switch in another rack. Exists a patch panel to connect the servers to the aggregation switch.\label{EoR (End-of-Row) architecture}
    
    \begin{flushleft}
        \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
    \end{flushleft}
    \textbf{Simpler switch management}.

    \begin{flushleft}
        \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Cons}}
    \end{flushleft}
    The aggregation switches must have a larger number of ports, then:
    \begin{enumerate}
        \item \textbf{Complex cabling}.
        \item \textbf{Longer cables} then \textbf{higher costs}.
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.9\textwidth]{img/networking-3.pdf}
        \caption{EoR (End-of-Row) architecture.}
    \end{figure}
\end{itemize}

\newpage

\paragraph{Switch-centric: Leaf-Spine architectures}

In the following section we present two Leaf-Spine architectures: the Leaf-Spine model and the Pod-based model (Fat Tree Network).

\highspace
The \definition{Leaf-Spine architecture} consists of \textbf{two levels of interconnection}:
\begin{enumerate}
    \item The \textbf{leaf} (which is a ToR switch);
    \item The \textbf{spine} (which has dedicated switches, aggregation switches).
\end{enumerate}
In practice, servers have two interfaces connected to two ToR switches to provide fault tolerance.
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.7\textwidth]{img/networking-4.png}
    \caption{Leaf-Spine architecture.}
\end{figure}

\noindent
Now we will explain the Leaf-Spine architecture. If $m$ is the number of \textbf{mid-stage switches} and $n$ is the \textbf{number of inputs and outputs}, the Leaf-Spine topology is as follows:
\begin{itemize}
    \item Each switch module is bi-directional. 
    \begin{itemize}
        \item \emph{Leaf} has $2k$ bidirectional ports per module;
        \item \emph{Spine} has $k$ bidirectional ports per module.
    \end{itemize}

    \item Each path traverses either 1 or 3 modules.
\end{itemize}
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.3\textwidth]{img/networking-5.pdf}
    \caption{Explanation of Leaf-Spine architecture.}
\end{figure}

\noindent
The \textbf{advantages} are: use of homogeneous equipment, simple routing, the number of hops is the same for any pair of nodes, small blast radius.

\begin{examplebox}
    There are $3072$ servers and $3072$ ports available at 10 Gbit/s. Provides a leaf-spine design.

    There are \textbf{two possible designs}.
    \begin{enumerate}
        \item The first consists of 96 switches with 64 ports and 32 switches with 96 ports.
        \begin{center}
            \includegraphics[width=.4\textwidth]{img/networking-6.pdf}
        \end{center}
        
        \item The second has only 8 switches but they have more ports: 384 ($8 \times 384 = 3072$).
        \begin{center}
            \includegraphics[width=.5\textwidth]{img/networking-7.pdf}
        \end{center}
    \end{enumerate}
\end{examplebox}

\begin{examplebox}
    There are $6144$ servers and $6144$ ports available at 10 Gbit/s. Provides a leaf-spine design.

    There are \textbf{two possible designs}.
    \begin{enumerate}
        \item The first consists of 192 switches with 64 ports and 32 switches with 192 ports.
        \begin{center}
            \includegraphics[width=.4\textwidth]{img/networking-8.pdf}
        \end{center}
        
        \item The second has only 16 switches but they have more ports: 384 ($16 \times 384 = 6144$).
        \begin{center}
            \includegraphics[width=.5\textwidth]{img/networking-9.pdf}
        \end{center}
    \end{enumerate}
\end{examplebox}

\noindent
The \textbf{Pod-based model}, also called \definition{Fat Tree Network}, is another network architecture used to \textbf{increase the scaling} feature respecting the leaf-spine.

\highspace
It transforms each group of spine-leaf into a \definition{PoD (Point of Delivery)}\footnote{A Point Of Delivery is a module or group of network, compute, storage and application components that work together to deliver a network service. The PoD is a repeatable pattern and its components increase the modularity, scalability and manageability of data.} and adds a super spine layer.

\highspace
It is a \textbf{highly scalable} and \textbf{cost-effective} DCN architecture designed to \textbf{maximise bisection bandwidth}. It can be built using standard Gigabit Ethernet switches with the same number of ports.

\highspace
It is composed by a \textbf{\emph{leaf}} of $2k^{2}$ bidirectional ports:
\begin{itemize}
    \item $k^{2}$ ports to the servers;
    \item $k^{2}$ ports to the data center network.
\end{itemize}
In general, let $k^{2}P$ servers: there are $2kP$ switches with $2k$ ports and $k^{2}$ switches with $P$ ports. Using the Fat-Tree model, \textbf{the $\mathbf{P}$ value is $\mathbf{2k}$, so for $\mathbf{2k^{3}}$ servers, there are $\mathbf{5k^{2}}$ switches with $\mathbf{2k}$ ports} ($k^{2} + 2k \cdot 2k$).

\highspace
At the \textbf{edge layer}, there are $2k$ pods (groups of servers), each with $k^{2}$ servers.
\begin{itemize}
    \item Each edge switch is directly connected to $k$ servers in a pod and $k$ aggregation switches. 
    \item A Fat-Tree network with $2k$-port commodity switches can accomodate $2k^{3}$ servers in total.
    \item $k^{2}$ core switches with $2k$-port each, each one connected to $2k$ pods.
    \item Each aggregation switch is connected to $k$ core switches.
\end{itemize}

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/networking-11.png}
    \caption{Fat-Tree Network, with $k=2$, 4 pods, 16 servers, 20 switches.}
\end{figure}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.4\textwidth]{img/networking-10.pdf}
    \caption{Explanation of Fat-Tree Network.}
\end{figure}

\newpage

\paragraph{Server-centric and hybrid architectures}

\definition{CamCube} is a \textbf{server-centric architecture} typically proposed for building container-sized data centres.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
It can \textbf{reduce implementation and maintenance costs} by using only servers to build the Data Center Network. It also exploits network locality to \textbf{increase communication efficiency}.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Disadvantages}}
\end{flushleft}
It \textbf{requires servers with Multiple Network Interface cards} to build a 3D tours network, \textbf{long paths} and \textbf{high routing complexity}.

\highspace
The hybrid architectures are \textbf{DCell}, \textbf{BCube} and \textbf{MDCube}.

\highspace
A \definition{DCell} is a \textbf{scalable and cost-effective hybrid architecture} that uses switches and servers for packet forwarding. It is a recursive architecture and uses a basic building block called $DCell_{0}$ to construct larger DCells.

\highspace
$DCell_{k}$ ($k > 0$) denotes a level-$k$ DCell \textbf{constructed by combining} $n+1$ \textbf{servers} in $DCell_{0}$. A $DCell_{0}$ has $n$ ($n < 8$) \textbf{servers directly connected by a commodity switch}.

\highspace
\emph{Disadvantages}: \textbf{long communication paths}, many required Network Interface Cards and \textbf{increased cabling costs}.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.6\textwidth]{img/networking-12.png}
    \caption{DCell hybrid architecture.}
\end{figure}

\newpage

\noindent
\definition{BCube} is a \textbf{hybrid and cost-effective architecture} that scales through recursion. It provides \textbf{high bisection bandwidth} and \textbf{graceful throughput degradation}.

\highspace
It uses BCube as a building block, consisting of $n$ servers connected to an $n$-port switch.

\highspace
A $BCube_{k}$ ($k>0$) is constructed with $n$ $BCube_{k-1}s$ and $n^{k}$ $n$-port switches. In a $BCube_{k}$ there are $n^{\left(k+1\right)}$ $k+1$-port servers and $k+1$ layers of switches.

\highspace
\emph{Disadvantages}: \textbf{limited scalability} and \textbf{high cabling costs} (NICs reason).

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/networking-13.png}
    \caption{BCube hybrid architecture.}
\end{figure}

\newpage

\noindent
\definition{MDCube} is designed to \textbf{reduce the number of cables used to connect containers}. 
\begin{itemize}
    \item Each container has an ID which is mapped to a multidimensional tuple.
    \item Each container is connected to a neighbouring container with a different tuple in one dimension.
    \item There are two types of connections: Intra-container links and high-speed inter-container links.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/networking-14.png}
    \caption{MDCube hybrid architecture.}
\end{figure}