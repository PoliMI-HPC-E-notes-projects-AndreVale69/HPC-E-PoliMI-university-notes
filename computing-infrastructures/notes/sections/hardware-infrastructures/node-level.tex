\subsection{Node-level}

\subsubsection{Server (computation, HW accelerators)}\label{subsubsection: Server (computation, HW accelerators)}

A \definition{Server} is a computing system designed to manage, process, and deliver data or services to other computers (clients) over a network. In the context of datacenters and Warehouse-Scale Computers (WSCs), servers are the \textbf{atomic units of computation}, the fundamental building blocks of the entire system architecture.

\highspace
Though \textbf{conceptually similar to a desktop PC}, servers \textbf{differ} in critical ways:
\begin{itemize}
    \item They are \textbf{significantly more powerful}, scalable, and modular.
    \item They are designed for \textbf{continuous operation}, high availability, and\break \textbf{dense physical packaging} within a rack structure.
\end{itemize}
Servers in modern datacenters must balance \textbf{performance}, \textbf{density}, \textbf{power efficiency}, and \textbf{maintainability}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{stream} \textbf{Server Types}}
\end{flushleft}
\textbf{Server form} factors \textbf{define how} servers are \textbf{physically organized} and \textbf{deployed} in datacenter environments. There are three principal types:
\begin{enumerate}
    \item \definition{Tower Server}s (section \ref{paragraph: Tower Server}, page \pageref{paragraph: Tower Server}). Resemble traditional desktop PCs. They are ideal for small-scale deployments or low-density use cases.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}: Easy to upgrade, good cooling, low cost.
        \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}: Large footprint, not optimized for rack deployment.
    \end{itemize}

    \item \definition{Rack Server}s (section \ref{paragraph: Rack Servers}, page \pageref{paragraph: Rack Servers}). Designed to slide \textbf{into a rack} (standardized shelves) in units (U); e.g., 1U = 1.75 inches. It is the most common server format in datacenters.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}: High compute density, easy to cable and scale.
        \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}: Requires dedicated infrastructure (rack, cooling, power).
    \end{itemize}
    
    \item \definition{Blade Server}s (section \ref{paragraph: Blade Servers}, page \pageref{paragraph: Blade Servers}). Extremely compact and multiple blades share power, cooling, and networking through a \textbf{blade enclosure}. It is excellent for environments where space and energy are at a premium.
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Pros}}: Highest density and modularity, centralized management.
        \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Cons}}: Higher initial cost, vendor lock-in, increased heat density.
    \end{itemize}
\end{enumerate}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Server Architecture}}
\end{flushleft}
Servers are typically \textbf{integrated into a tray or blade enclosure}, which contains:
\begin{itemize}
    \item \important{Motherboard}: The central PCB that \textbf{interconnects all components}.
    \item \important{Chipset}: \textbf{Manages data flow} between CPU, RAM, storage, and peripherals.
    \item \important{Expansion slots}: For GPUs, network cards, and other \textbf{accelerators}.
\end{itemize}
Servers in WSCs tend to \textbf{use homogeneous hardware/software platforms} to simplify large-scale orchestration and maintenance.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{microchip} \textbf{Server Architecture}}
\end{flushleft}
The \definition{Motherboard} acts as a \textbf{central nervous system for the server}, it hosts:
\begin{itemize}
    \item \textbf{CPU sockets} (e.g., up to 2 for dual Xeon systems)
    \item \textbf{DIMM slots} for RAM
    \item \textbf{Storage connectors} (e.g., SATA, NVMe)
    \item \textbf{NIC slots} (Network Interface Cards)
\end{itemize}
This level of configurability allows tailoring servers for compute-heavy, memory-bound, or I/O-intensive applications.

\newpage

\paragraph{Tower Server}\label{paragraph: Tower Server}

A \definition{Tower Server} is a type of server designed in a vertical, standalone chassis that closely resembles a \textbf{standard tower desktop computer}. Unlike blade or rack servers, which are designed for high-density environments, tower servers prioritize \textbf{simplicity and accessibility}, often at the cost of physical footprint.
\begin{itemize}
    \item \important{Structure}: Independent, \textbf{vertical} case (not meant for rack mounting).
    \item \important{Deployment}: Common in small businesses, branch offices, or settings where only a few servers are needed.
    \item \important{Internal layout}: Lots of \textbf{space for expansion components} like disks or PCIe cards.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Scalability \& Ease of Upgrade}}. \textbf{Easy} to open and \textbf{upgrade}, users can add storage, memory, or cards as needed.
    \item \textcolor{Green3}{\textbf{Cost-Effective}}. Usually the \textbf{cheapest server type}, suitable for budget-constrained environments.
    \item \textcolor{Green3}{\textbf{Easy Cooling}}. Due to \textbf{low component density}, natural airflow is often sufficient. Less need for specialized cooling systems.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{times-circle} \textbf{Limitations}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textcolor{Red2}{\textbf{Space Consumption}} Tower servers consume \textbf{significant physical\break space} and don't scale well in quantity.
    \item \textcolor{Red2}{\textbf{Basic Performance}} They usually \textbf{offer lower performance and redundancy} compared to enterprise-grade rack or blade servers.
    \item \textcolor{Red2}{\textbf{Cable Management}} Not ideal for structured environments, cables can become messy and hard to manage.
\end{itemize}

\newpage

\paragraph{Rack Servers}\label{paragraph: Rack Servers}

A \definition{Rack Server} is a server built specifically to be \textbf{mounted vertically in standardized racks}, which are metallic shelves designed to hold multiple servers and IT components. Rack servers are the \textbf{default choice} in medium to large-scale datacenters, balancing compute density, modularity, and serviceability.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Physical Standardization}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Servers are stored in racks} which follow a global standard:
    \begin{itemize}
        \item \textbf{1U (\definition{Rack Unit})} = 1.75 inches (44.45 mm) in height.
        \item Servers may come in 1U, 2U, 4U, up to 10U formats depending on power and component density.
    \end{itemize}

    \item Racks also house other components: networking switches, storage arrays, power distribution units (PDUs), and cooling units.
\end{itemize}
This \textbf{standardization allows for efficient vertical stacking} of servers, optimizing physical space and simplifying cabling.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Racks as More Than Just Shelves}}
\end{flushleft}
A rack is not just a mechanical holder, it is \textbf{part of the power, networking, and management infrastructure of the datacenter}:
\begin{itemize}
    \item \important{Power Infrastructure}:
    \begin{itemize}
        \item Shared power distribution units.
        \item Battery backup (UPS).
        \item Power conversion units.
    \end{itemize}
    
    \item \important{Networking}:
    \begin{itemize}
        \item Top of Rack (ToR) switches connect all servers in the rack to the datacenter network fabric.
        \item Simplifies cabling and reduces latency.
    \end{itemize}
  
    \item \important{Cooling}: designed for front-to-back airflow, aligned with datacenter cooling strategy (e.g., cold aisle containment).
  
    \item \important{Dimensions} can vary, but the classic rack is 19 inches wide and up to 48 inches deep.
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Modularity}}: Individual servers can be \textbf{hot-swapped}, upgraded, or replaced \textbf{without disrupting others}.
    \item \textcolor{Green3}{\textbf{Failure Containment}}: Easy to \textbf{isolate and service a failed node} without bringing down the system.
    \item \textcolor{Green3}{\textbf{Cable Management}}: Organized by rear/backplanes or Top-of-Rack\break (ToR) switches.
    \item \textcolor{Green3}{\textbf{Cost-Efficient Scaling}}: \textbf{Scales vertically} at relatively lower incremental cost compared to other formats.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{times-circle} \textbf{Challenges}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textcolor{Red2}{\textbf{High Power Demand}}: \textbf{Higher component density} requires more energy and advanced cooling systems.
    \item \textcolor{Red2}{\textbf{Thermal Hotspots}}: Tight stacking can cause \textbf{hot zones}, especially with accelerator-heavy nodes.
    \item \textcolor{Red2}{\textbf{Maintenance Overhead}}: Large racks with tens of servers can become \textbf{complex to manage} physically as systems scale.
\end{itemize}

\newpage

\paragraph{Blade Servers}\label{paragraph: Blade Servers}

\definition{Blade Server}s represent the \textbf{most advanced evolution} in server form factors. They are designed to \textbf{maximize space efficiency} and \textbf{centralized manageability}, making them ideal for \textbf{large-scale enterprise datacenters} and \textbf{high-performance computing environments}.

\highspace
A blade server is essentially a \textbf{stripped-down}, \textbf{ultra-thin server board} (the ``blade'') that fits into a blade enclosure, a shared chassis providing:
\begin{multicols}{2}
    \begin{itemize}
        \item Power
        \item Cooling
        \item Networking
        \item Centralized management
    \end{itemize}
\end{multicols}
\noindent
The enclosure conforms to the same \textbf{rack unit standard} (U), allowing it to integrate seamlessly with existing rack infrastructure. We can think of a blade system as a server equivalent of a modular bookshelf, where each ``book'' is a full server, and the ``bookshelf'' provides shared power, ventilation, and data connectivity.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Compactness \& Density}}: The \textbf{smallest physical form factor} among all servers, allowing high-density deployments within a minimal footprint.
    \item \textcolor{Green3}{\textbf{Minimal Cabling}}: The \textbf{shared backplane} removes the need for complex cabling; power and network connections are centralized.
    \item \textcolor{Green3}{\textbf{Centralized Management}}: Blade systems typically include \textbf{unified management interfaces} (e.g., iLO, iDRAC) to monitor and configure blades collectively.
    \item \textcolor{Green3}{\textbf{Scalability \& Reliability}}: New \textbf{blades can be added} with minimal disruption; enclosures support \textbf{load balancing} and \textbf{failover mechanisms}.
    \item \textcolor{Green3}{\textbf{Uniform Infrastructure}}: Simplifies deployment with \textbf{shared cooling}, \textbf{network fabrics}, and \textbf{power redundancy}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{times-circle} \textbf{Disadvantages}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item \textcolor{Red2}{\textbf{High Initial Cost}}: Blade enclosures and vendor-specific blades often demand \textbf{significant upfront investment}.
    \item \textcolor{Red2}{\textbf{Vendor Lock-In}}: Typically, only blades from the \textbf{same manufacturer} (e.g., HPE, Dell, Cisco) \textbf{are compatible} with a given enclosure.
    \item \textcolor{Red2}{\textbf{Thermal Density}}: The compact form causes \textbf{higher heat output per rack unit}, requiring advanced HVAC design and monitoring.
    \item \textcolor{Red2}{\textbf{Limited Flexibility}}: While modular, blade systems trade off flexibility for density, upgrades and replacements may be \textbf{constrained by the enclosure's architecture}.
\end{itemize}

\newpage

\paragraph{Machine Learning}

While Moore's Law historically predicted that transistor density would double every 18-24 months, the \textbf{growth in ML model complexity} has surpassed this pace. Since 2013, compute demand for AI training has doubled approximately \textbf{every 3.5 months}. This exponential curve far exceeds the capabilities of general-purpose CPUs, triggering a renaissance in specialized hardware.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is Machine Learning?}}
\end{flushleft}
At its core, \definition{Machine Learning (ML)} refers to \textbf{computational methods} that enable systems to \textbf{learn from data} without being explicitly programmed. Rather than defining rules manually, ML allows a system to build a model from \textbf{patterns observed in examples}.

\highspace
In supervised learning:
\begin{itemize}
    \item A system learns a \textbf{target function} $y = f\left(x\right)$ that \textbf{maps inputs} (features) \textbf{to outputs} (labels).
    \item This is \textbf{done using a training dataset} $\left(x_{1}, y_{1}\right), \dots, \left(x_{N}, y_{N}\right)$, and the model is later tested on unseen inputs.
\end{itemize}
Applications include: classification (e.g., cat vs. dog), regression (e.g., predicting flight delays), image recognition, speech synthesis, fraud detection, etc.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Artificial Neural Networks (ANNs)}}
\end{flushleft}
\definition{Artificial Neural Networks (ANNs)} are a \textbf{subset of ML models} inspired by the human brain. They consist of \textbf{layers of interconnected neurons}, including:
\begin{itemize}
    \item \textbf{Input layer}: receives the data
    \item \textbf{Hidden layers}: transform data using weighted functions and nonlinear activations
    \item \textbf{Output layer}: produces the prediction
\end{itemize}
The key learning mechanisms are:
\begin{itemize}
    \item \textbf{Backpropagation}: adjusts weights based on the error between prediction and actual target.
    \item \textbf{Gradient descent}: optimizes the model parameters iteratively.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Hardware Acceleration: Why ML Needs More Than CPUs}}
\end{flushleft}
Modern ML, particularly \textbf{deep learning}, is computationally expensive. Training models like GPT or ResNet involves processing \textbf{billions of parameters} across massive datasets. To meet these demands, \textbf{Warehouse-Scale Computers (WSCs) integrate specialized accelerators such as}:
\begin{itemize}
    \item \definition{Graphics Processing Units (GPUs)}. GPUs are highly parallel processors originally designed for graphics rendering but are now extensively used for ML because they:
    \begin{itemize}
        \item Execute the \textbf{same operation across many data elements in parallel} (SIMD).
        \item Accelerate matrix operations central to deep learning.
        \item Support ML frameworks via CUDA, OpenCL, OpenMP, SYCL, etc.
    \end{itemize}
    GPUs are often housed in \textbf{PCIe-attached trays}, interconnected via NVLink or NVSwitch for ultra-fast data exchange.
    
    Distributed training across multiple GPUs requires \textbf{low-latency}, \textbf{high-bandwidth interconnects}. Performance can also be bottlenecked by slowest learner or network synchronization delays. 


    \item \definition{Tensor Processing Units (TPUs)}. Developed by Google, TPUs are \textbf{domain-specific architectures} designed \textbf{specifically for ML workloads}. TPU generations:
    \begin{itemize}
        \item \textbf{TPUv1}: Inference-only, connected via PCIe.
        \item \textbf{TPUv2}: Supports both training and inference; includes MXUs (matrix units) and high-bandwidth memory (HBM).
        \item \textbf{TPUv3}: Liquid-cooled, supercomputing-class performance. Up to 100 PFLOPS per pod.
        \item \textbf{TPUv4/TPUv5}:
        \begin{itemize}
            \item v4 pod: 4096 devices
            \item v5e: cost-efficient variant
            \item v5p: high-performance variant scalable to 8000+ devices
            \item Used in global data centers since 2023
        \end{itemize}
    \end{itemize}
    A \textbf{TPU Pod} aggregates \textbf{hundreds of TPU cores with shared memory} and custom high-speed networks for massive parallelism.

    
    \item \definition{Field-Programmable Gate Arrays (FPGAs)}. FPGAs offer \textbf{customizable digital logic} that can be reprogrammed after manufacturing.
    \begin{itemize}
        \item Flexible hardware, can be reconfigured for different algorithms.
        \item Suitable for:
        \begin{itemize}
            \item Network acceleration
            \item Security tasks (e.g., encryption)
            \item Data analytics
            \item Specialized ML inference
        \end{itemize}
    \end{itemize}
    For example, Microsoft Azure integrates FPGAs for infrastructure efficiency, lowering carbon footprint and improving hardware reuse.
\end{itemize}

\newpage

\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l | l | l | l @{}}
            \toprule
            \textbf{Feature} & \textbf{GPU} & \textbf{TPU} & \textbf{FPGA} \\
            \midrule
            Purpose         & General-purpose ML compute    & ML-specific acceleration (esp. DL) & Flexible, reconfigurable hardware  \\ [.5em]
            Programmability & CUDA, OpenCL, etc.            & TensorFlow, PyTorch (high-level)   & VHDL, Verilog (low-level, HDL)     \\ [.5em]
            Flexibility     & High                          & Medium (optimized for tensors)     & Very high (reprogrammable)         \\ [.5em]
            Efficiency      & Good                          & Excellent (for tensor ops)         & Excellent (for specific pipelines) \\ [.5em]
            Use Case        & Training \texttt{+} Inference & Training \texttt{+} Inference      & Offloading, network, analytics     \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Summary: GPU vs TPU vs FPGA.}
\end{table}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} c p{14em} p{14em} @{}}
        \toprule
        & \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}} & \textcolor{Red2}{\faIcon{times-circle} \textbf{Disadvantages}} \\
        \midrule
        \textbf{CPU} & 
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Easy to be programmed and support any programming framework.
            \item Fast design space exploration and run your applications.
        \end{itemize}
        & \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Suited only for simple AI models that do not take long to train and for small models with small training set.
        \end{itemize} \\
        \cmidrule{1-3}
        %
        \textbf{GPU} & \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Ideal for applications in which data need to be processed in parallel like the pixels of images or videos.
        \end{itemize} & \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Programmed in languages like CUDA and OpenCL and therefore provide limited flexibility compared to CPUs.
        \end{itemize} \\
        \cmidrule{1-3}
        %
        \textbf{TPU} & \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Very fast at performing dense vector and matrix computations and are specialized on running very fast programming based on Tensorflow.
        \end{itemize} & \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item For applications and models based on the TensorFlow.
            \item Lower flexibility compared to CPUs and GPUs.
        \end{itemize} \\
        \cmidrule{1-3}
        %
        \textbf{FPGA} & \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Higher performance, lower cost and lower power consumption compared to other options like CPUs and GPU.
        \end{itemize} & \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Programmed using OpenCL and High-Level Synthesis (HLS).
            \item Limited flexibility compared to other platforms.
        \end{itemize} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of CPU, GPU, TPU and FPGA.}
\end{table}

\newpage

\subsubsection{Storage (type, technology)}\label{subsubsection: Storage (type, technology)}

Data has significantly grown in the last few years due to sensors, industry 4.0, AI, etc. The growth favours the \textbf{centralized storage strategy} that is focused on the following:
\begin{itemize}
    \item \textbf{Limiting redundant data}
    \item \textbf{Automatizing replication and backup}
    \item \textbf{Reducing management costs}
\end{itemize}

\highspace
The \emph{storage technologies} are many. One of the oldest but still used is the \definition{Hard Disk Drive (HDD)}, a magnetic disk with mechanical interactions. However, the mechanical nature of HDDs imposes physical limits on access speed and reliability. In contrast, \definition{Solid-State Drive (SSD)}, which lack moving parts and are built using NAND flash memory, offer significantly faster access times and better durability. The \definition{Non-Volatile Memory express (NVMe)} also exists, which is the \textbf{latest industry standard} for running PCIe\footnote{\definition{PCIe (peripheral component interconnect express)}. is an interface standard for connecting high-speed components} SSDs.

\highspace
In terms of cost per terabyte, NVMe drives are currently the most expensive (typically €100-200 for 1 TB), followed by SSDs (€70-100), while HDDs remain the most economical option (€40-60). This price-performance hierarchy makes hybrid storage architectures (HDD + SSD) increasingly appealing:
\begin{itemize}
    \item A speed storage technology (\textbf{SSD or NVMe}) as \textbf{cache} and \textbf{several HDDs for storage}. It is a combination used by some servers: a small SSD with a large HDD to have a faster disk.
    
    \item Some HDD manufacturers produce Solid State Hybrid Disks (SSHD) that combine a small SDD with a large HDD in a single unit.
\end{itemize}

\newpage

\paragraph{Files}

The operating system views the disk as a \textbf{flat collection of independently addressable data blocks}. Each \textbf{block is assigned a unique} \definition{LBA (Logical Block Address)}\label{LBA (Logical Block Address)}, enabling efficient data referencing and organization. To streamline access and reduce management overhead, the \textbf{OS typically groups these blocks into clusters}, larger units that serve as the minimum granularity for disk I/O operations.

\highspace
Clusters typically range in size from a single disk sector (512 bytes or 4 KB) up to 128 sectors (64 KB), depending on the file system configuration. Each cluster may store either:
\begin{itemize}
    \item \textbf{File data}. The actual contents of user files.
    
    \item \textbf{Metadata}. System-level information necessary to support the file system,:
    \begin{itemize}
        \item File and directory names
        \item Folder hierarchies and symbolic links
        \item Timestamps (creation, modification, access)
        \item Ownership and access control data
        \item \textbf{Links to the LBA where the file content can be located on the disk}
    \end{itemize}
\end{itemize}
Consequently, the \textbf{disk space is divided into different cluster types} based on their purpose:
\begin{itemize}
    \item Metadata:
    \begin{itemize}
        \item \hl{Fixed-position metadata clusters}, used to initialize and mount the file system
        \item \hl{Variable-position metadata clusters}, which manage directories and symbolic links
    \end{itemize}
    
    \item \hl{File data clusters}, containing the actual contents of files
    
    \item \hl{Unused clusters}, which are free and available for future allocations
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/files-1.pdf}
    \caption{A cluster can be seen visually as an array. In this image, for example, we've shown three types of cluster: metadata fixed position (azure), metadata variable position (green), file data (orange), unused space (white).}
\end{figure}

\newpage

\noindent
The following explanation introduces some basic operations on the files to see what happens inside the disks.
\begin{itemize}
    \item \textcolor{Red2}{\underline{\textbf{File Reading}}}
    \begin{enumerate}
        \item \important{Access the Metadata}. Before the system knows \textbf{where} the file is stored on disk, it must find information about the file, called metadata. This metadata is stored in \textbf{variable-position metadata clusters}. These clusters are not always in the same place on disk, they move and grow as the system evolves.
        \item \important{Locate the File's Data Blocks (clusters)} and \important{Read the Actual Content}. Once the OS has the metadata, it now knows:
        \begin{itemize}
            \item Which \textbf{Logical Block Addresses (LBAs)} contain the data.
            \item \textbf{How many clusters need to be read} to get the full content.
        \end{itemize}
        It uses this information to issue \textbf{read commands} to the disk controller. Finally, the \textbf{disk accesses the physical sectors or clusters} indicated by the LBAs and transfers that data into main memory (RAM).
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.8\textwidth]{img/files-2.pdf}
    \end{figure}

    \item \textcolor{Red2}{\underline{\textbf{File Writing}}}
    \begin{enumerate}
        \item \important{Access Metadata to Find Free Space}. The operating system \textbf{first checks the file system's metadata} to \textbf{find a free area of disk space} where it can store our new data.
        \item Once free space is identified, the OS:
        \begin{itemize}
            \item \textbf{Allocates} one or more clusters, depending on the file size
            \item \textbf{Writes our data} into these clusters on the physical disk
        \end{itemize}
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.8\textwidth]{img/files-3.pdf}
    \end{figure}

    Since the \emph{file system can only access clusters}, the \textbf{actual space taken up by a file on a disk is always a multiple of the cluster size}. Given:
    \begin{itemize}
        \item $s$, the \emph{file size}
        \item $c$, the \emph{cluster size}
    \end{itemize}
    Then the \definition{actual size on the disk $a$} can be calculated as:
    \begin{equation}
        a = \left\lceil \dfrac{s}{c} \right\rceil \times c
    \end{equation}
    Where $\mathrm{ceil}$ rounds a number \underline{up} to the nearest integer. It's also possible to calculate the \textbf{amount of disk space wasted by organising the file into clusters} (\definition{wasted disk space $w$}):
    \begin{equation}
        w = a - s
    \end{equation}
    A formal way to refer to wasted disk space is \definition{internal fragmentation} of files.
    \begin{examplebox}[: internal fragmentation]
        \begin{itemize}
            \item File size: 27 byte
            \item Cluster size: 8 byte
        \end{itemize}
        The \emph{actual size} on the disk is:
        \begin{equation*}
            a = \left\lceil \dfrac{27}{8} \right\rceil \cdot 8 = \left\lceil 3.375 \right\rceil \cdot 8 = 4 \cdot 8 = 32 \text{ byte}
        \end{equation*}
        And the internal fragmentation $w$ is:
        \begin{equation*}
            w = 32 - 27 = 5 \text{ byte}
        \end{equation*}
    \end{examplebox}

    \item \important{\underline{\textbf{Deleting}}}
    \begin{enumerate}
        \item The file system updates its metadata structures to:
        \begin{itemize}
            \item \textbf{Remove the file name} from the directory
            \item \textbf{Mark the clusters} where the file was stored as \textbf{free or available}
            \item Optionally, update timestamps or record deletion events
        \end{itemize}
        \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Importantly}}: \textbf{The data itself is not erased at this stage}. The actual bytes remain on disk until they are overwritten by another file.
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=\textwidth]{img/files-4.pdf}
    \end{figure}
    
    \item \important{\underline{\textbf{External fragmentation}}}. It happens when there are \textbf{enough free clusters on the disk} to store a file, but \textbf{not all together (not contiguous)}. So, when the system tries to write a large file, it must \textbf{split it} into smaller parts and place them in \textbf{different, scattered locations} on the disk.
    
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Why does this happen?}} Over time, as files are created, deleted, resized, or moved, the disk becomes \textbf{less organized}. Clusters are freed in different spots, and the available space is no longer one big continuous area. So:
    \begin{itemize}
        \item A new file cannot fit in one continuous chunk.
        \item The OS stores it in \textbf{multiple non-adjacent clusters}.
    \end{itemize}
    This is called \definition{external fragmentation} because: the \textbf{fragmentation is not inside the file itself, but in the way its data is laid out externally across the disk}.

    \newpage

    \begin{figure}[!htp]
        \centering
        \includegraphics[width=\textwidth]{img/files-5.pdf}
        \caption{Each number (1, 2, 3) corresponds to a portion (chunk) of a file. A file that was meant to be stored as \texttt{[1][2][3]} (contiguously) has instead been broken into parts and stored in a scattered layout across different disk clusters. This situation represents \textbf{external fragmentation}, where the file system could not find a large enough continuous block of free space to store the file all together.}
    \end{figure}
\end{itemize}

\newpage

\paragraph{HDD}\label{paragraph: HDD}

A \definition{Hard Disk Drive (HDD)} is a \textbf{data storage device that uses rotating disks (platters) coated with magnetic material}.

\highspace
\textbf{Data is read randomly}, meaning individual data blocks can be stored or retrieved in any order rather than sequentially.

\highspace
An HDD consists of one or more rigid (\emph{hard}) rotating disks (platters) with magnetic heads arranged on a moving actuator arm to read and write data to the surfaces.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/files-6.pdf}
    \caption{Hard Drive Disk anatomy.}
\end{figure}

\noindent
Externally, hard drives expose a large number of \textbf{sectors} (blocks):
\begin{itemize}
    \item Typically, 512 or 4096 bytes.
    \item Individual \textbf{sector writes} are \textbf{atomic}.
    \item Multiple sectors write it may be interrupted (\definition{torn write}\footnote{Torn writes happen when only part of a multi-sector update is written successfully to disk.}).
\end{itemize}
The geometry of the drive:
\begin{itemize}
    \item The sectors are arranged into \textbf{tracks}.
    \item A \textbf{cylinder} is a particular track on multiple platters.
    \item Tracks are arranged in concentric circles on \textbf{platters}.
    \item A disk may have multiple double-sided platters.
\end{itemize}
The \textbf{driver motor spins the platters at a constant rate}, measured in \definition{Revolutions Per Minute (RPM)}.

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/files-7.pdf}
    \caption{Example of HDD geometry.}
    \label{fig: example of hdd geometry}
\end{figure}

\noindent
The geometry of an HDD refers to the \textbf{physical layout} of data on the surface of a spinning disk. Figure \ref{fig: example of hdd geometry} shows this:
\begin{itemize}
    \item \textbf{One Platter}. The circular surface shown is a platter, which is a rigid, magnetic-coated disk where data is stored. Most HDDs have multiple platters stacked vertically, but here we focus on one for simplicity.
    \item \textbf{Tracks}. The platter is divided into concentric circles called tracks. Each track is a circular path that the read/write head can follow. In the figure \ref{fig: example of hdd geometry}, we see three tracks, each larger than the last, moving outward from the center.
    \item \textbf{Sectors}. Each track is divided into pie-like slices called sectors. A sector is the smallest physical unit that can be read or written on a disk (typically 512 bytes or 4 KB). The numbers (0 to 35) represent sector identifiers along the circular tracks. Note how the outer tracks contain more sectors because they have a larger circumference, so they can physically hold more data; this is known as Zone Bit Recording (ZBR), not mentioned in the course.
    \item \textbf{Read/Write head}. The read head is shown floating above the platter. It moves radially across the surface to switch from one track to another (this is called seek). Once on the correct track, the head waits for the desired sector to rotate beneath it (rotational latency), and then it reads/writes data.
    \item \textbf{Rotation and seek behavior}. The platter spins at high speed (e.g., 7200 RPM). As it rotates, the sectors pass under the stationary head. Data is accessed when the correct sector aligns with the head.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Types of Delay in disk Access}}
\end{flushleft}
When a file is read from or written to a disk, especially an HDD, the total time taken is influenced by several delays. These are due to the mechanical and electronic processes involved in locating and transferring the data. Exists \textbf{four types of delay}:
\begin{itemize}\label{four types of hdd delay}
    \item \definition{Rotational Delay} (a.k.a. \definition{Rotational Latency}) is the \textbf{time needed for the disk to rotate so that the desired sector aligns with the read/write head}. It depends on the RPM (Revolutions Per Minute) of the disk.


    \item \definition{Seek Delay} is the \textbf{delay caused by the mechanical movement of the read/write head as it travels from one track to another} on a spinning disk. It is the dominant mechanical delay in many HDD operations, especially for random access patterns.

    \textcolor{Green3}{\faIcon{question-circle} \textbf{How it works}}. Moving the head involves a sequence of physical phases:
    \begin{enumerate}
        \item \textbf{Acceleration}: the actuator moves the head out of its resting position.
        \item \textbf{Coasting}: the head glides at a constant speed (if the distance is large).
        \item \textbf{Deceleration}: the actuator slows down to avoid overshooting.
        \item \textbf{Settling}: a short pause to stabilize the head at the desired track.
    \end{enumerate}

   
    \item \definition{Transfer time} is the \textbf{time required to actually move the data}, once the head is correctly positioned over the desired sector. It's the final step in the I/O pipeline, where data is \textbf{read from or written to} the magnetic surface.
    
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What affects transfer time?}}
    \begin{enumerate}
        \item \textbf{Rotational Speed (RPM)}, determines how quickly sectors pass under the head.
        \item \textbf{Data density}, more tightly packed data, more data per second read.
        \item \textbf{Size of the request}, reading more data takes more time.
    \end{enumerate}
    Even though it's much shorter than seek or rotation delays, \textbf{transfer time scales with data size}. For large sequential reads, transfer time becomes more relevant, especially when seek and rotation delays are minimized.


    \item \definition{Controller Overhead} is the \textbf{non-mechanical delay introduced by the disk controller}, which is the hardware interface between the OS and the physical disk. It involves:
    \begin{itemize}
        \item \textbf{Buffer management}: transferring data between disk and system memory (often using DMA).
        \item \textbf{Interrupt processing}: informing the OS when the I/O operation is complete.
        \item \textbf{Command translation}: converting OS-level I/O requests into device specific actions (e.g., SATA/NVMe commands).
        \item \textbf{Scheduling and queueing}: organize I/O operations for efficiency.
    \end{itemize}
\end{itemize}
\hl{To see how these delays are calculated}, we suggest you refer to the performance section \ref{subsection: Disk performance}, about HDD \ref{subsubsection: HDD performance}, page \pageref{subsubsection: HDD performance}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{\speedIcon} \textbf{Cache}}
\end{flushleft}
Hard Disk Drives (HDDs) are mechanical devices, and their performance is often limited by seek times and rotational latency. To partially overcome these limits, \textbf{many HDDs integrate a small} (8, 16, 32 MB) \textbf{amount of fast memory} (RAM) on the controller board, this is the \definition{cache} or \definition{track buffer}. The key functions of HDD cache are:
\begin{enumerate}
    \item \definition{Read Caching}. When the disk reads a block from the platter, it may also load \textbf{adjacent blocks} into the cache, expecting that they might be requested next (a technique known as \textbf{read-ahead}).
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item If the next read request is for cached data, the disk can respond \textbf{instantly} from fast RAM.
            \item This avoids mechanical movement and \textbf{cuts seek and rotational delays}.
            \item Very effective for \textbf{sequential reads}.
        \end{itemize}
    \end{itemize}


    \item \definition{Write Caching}. There are two strategies here:
    \begin{enumerate}
        \item \definition{Write-Back Cache} (Faster, Riskier). The HDD \textbf{reports the completion of the write} operation as soon as the data is in the cache, \textbf{before it is actually written to the platter}. Actual writing to disk happens later, in the background.
        \begin{itemize}
            \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
            \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
                \item Faster perceived performance
                \item Useful in workloads with many small writes
            \end{itemize}
            \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
            \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
                \item If power is lost before the data is flushed to disk, the \textbf{data is lost}. It is a \textbf{file system corruption risk}!
                \item This is why it's considered \textbf{dangerous in critical systems} without battery backup or UPS.
            \end{itemize}
        \end{itemize}
        
        \item \definition{Write-Through Cache} (Safer, Slower). The HDD only reports the completion of the write operation \textbf{after the data has been fully written to disk}. Safer, but \textbf{slower}, since the OS must wait for the mechanical operation to finish.
    \end{enumerate}

    
    \item Hybrid Cache: \definition{Flash-Based Caching}. Some modern HDDs integrate \textbf{small flash memory} used for \textbf{persistent caching}:
    \begin{itemize}
        \item Data stays even if the power goes out.
        \item Combines the speed of SSD with the capacity of HDD.
        \item Great for frequently accessed blocks (e.g., boot files, apps).
    \end{itemize}
\end{enumerate}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Disk Scheduling}}
\end{flushleft}
While \textbf{caching} helps improve disk performance, it \textbf{doesn't eliminate the delays caused by seek and rotational latency}, especially during random access patterns. In systems with many I/O requests (like in a database or OS kernel), it's crucial to \textbf{choose the right order} to process requests to \textbf{minimize head movement}. This is where disk scheduling algorithms come in.

\highspace
Instead of serving I/O requests in the order they arrive, the \textbf{disk controller (or OS)} can render them to improve efficiency. It is possible:
\begin{itemize}
    \item Because every \textbf{disk request} includes the \textbf{target position} (i.e., the cylinder/track).
    \item So we can \textbf{estimate the cost (seek time)} of each request and choose the most efficient order.
\end{itemize}
Common disk scheduling algorithms are:
\begin{enumerate}
    \item \definition{First Come, First Serve (FCFC)} (the worst)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} Requests are handled in the \textbf{order they arrive}. No optimization, simple queue processing.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item (Naive) Easy to implement
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
        \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Can lead to \textbf{long seek distances} (i.e., inefficient head movement)
        \end{itemize}
    \end{itemize}

    \item \definition{Shortest Seek Time First (SSTF)} (great performance, but watch out for starvation)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} Always serve the request \textbf{closest to the current head position}.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Minimizes \textbf{total head movement}.
            \item Efficient in practice.
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
        \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Can lead to \textbf{starvation}: distant requests may never be served if new close requests keep arriving.
        \end{itemize}
    \end{itemize}

    \item \definition{SCAN (Elevator Algorithm)} (good performance as SSTF, but fairer)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} The head \textbf{moves in one direction} (like an elevator), serving all requests in that direction. When it reaches the end, it \textbf{reverses direction}.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item Good worst-case behavior.
            \item \textbf{No starvation}, every request will eventually be served.
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
        \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Requests at \textbf{edges} of the disk may have \textbf{longer wait times}.
        \end{itemize}
    \end{itemize}

    \item \definition{Circular SCAN (C-SCAN)} (fair, but less efficient than SCAN in some cases)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} Like SCAN, but head \textbf{only moves in one direction}. When it reaches the end, it \textbf{jumps back} to the beginning (like a circular elevator).
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item More \textbf{uniform wait time} for all requests.
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{times-circle}}] \textcolor{Red2}{\textbf{Cons}}
        \begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
            \item Longer total movement (because of the jump).
        \end{itemize}
    \end{itemize}

    \item \definition{C-LOOK} (smart compromise between performance and fairness)
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{How it works?}} Like C-SCAN, but instead of going to the physical end of the disk, th \textbf{head only goes as far as the last request in that direction}, then \textbf{jumps back to the smallest request}.
        \item[\textcolor{Green3}{\faIcon{check-circle}}] \textcolor{Green3}{\textbf{Pros}}
        \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
            \item \textbf{Saves movement} compared to full C-SCAN.
            \item Still \textbf{avoids starvation}.
        \end{itemize}
    \end{itemize}
\end{enumerate}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | l | l | l @{}}
        \toprule
        Algorithm & Fairness & Risk of Starvation & Strategy \\
        \midrule
        FCFS        & \textcolor{Green3}{\faIcon{check}} Yes   & \textcolor{Red2}{\faIcon{times}} None     & Serve in arrival order            \\ [.3em]
        SSTF        & \textcolor{Red2}{\faIcon{times}} No      & \textcolor{Red2}{\faIcon{exclamation-triangle}} Possible & Serve closest request             \\ [.3em]
        SCAN        & \textcolor{Green3}{\faIcon{check}} Yes   & \textcolor{Red2}{\faIcon{times}} None     & Elevator (back \& forth)          \\ [.3em]
        C-SCAN      & \textcolor{Green3}{\faIcon{check}} Yes   & \textcolor{Red2}{\faIcon{times}} None     & One-direction sweep (circular)    \\ [.3em]
        C-LOOK      & \textcolor{Green3}{\faIcon{check}} Yes   & \textcolor{Red2}{\faIcon{times}} None     & One-direction sweep (limited)     \\
        \bottomrule
    \end{tabular}
    \caption{Scheduling Algorithms.}
\end{table}

\newpage

\paragraph{SSD}\label{paragraph: SSD}

The \definition{solid-state drive (SSD)} does not have mechanical or moving parts like an HDD. It is built out of transistors (like memory and processors). It has higher performance than HDD.

\highspace
It stores bits in cells. Each cell can have:
\begin{itemize}
    \item Single-Level Cell (SLC), a single bit per cell.
    \item Multi-Level Cell (MLC), two bits per cell.
    \item Triple-Level Cell (TLC), three bits per cell.
    \item And so on... QLC, PLC, etcetera.
\end{itemize}
Internally, the SSD has a lot of NAND flashes, which are organized into Pages and Blocks. Some terminology:
\begin{itemize}
    \item A \definition{Page} contains \textbf{multiple logical block} (e.g. 512 B - 4 KB) \textbf{addresses} (LBAs). It is the \textbf{smallest unit that can be read/written}. It is a sub-unit of an erase block and consists of the number of bytes which can be read/written in a single operation. The states of each page are:
    \begin{itemize}
        \item \index{Erased Page}\index{Empty Page} \textbf{Empty} (\texttt{ERASED}): it does not contain data.
        \item \index{Invalid Page}\index{Dirty Page} \textbf{Dirty} (\texttt{INVALID}): it contains data, but this data is no longer in use (or never used).
        \item \index{In Use Page}\index{Valid Page} \textbf{In use} (\texttt{VALID}): the page contains data that can be read.
    \end{itemize}

    \item A Block (or \definition{Erase Block}) typically consists of \textbf{multiple pages} (e.g. 64) with a total capacity of around 128-256 KB. It is the \textbf{smallest unit that can be erased}.
\end{itemize}

\highspace
When passing from the HDD to SDD, there is a problem known as Write Amplification (WA). \definition{Write amplification (WA)}\label{Write amplification (WA)} is an \textbf{undesirable phenomenon associated with flash memory and solid-state drives (SSDs)} where the actual amount of information physically written to the storage media is a multiple of the logical amount intended to be written.

\begin{examplebox}
    Given a hypothetical SSD:
    \begin{itemize}
        \item Page Size: 4 KB
        \item Block Size: 5 Pages
        \item Drive Size: 1 Block
        \item Read Speed: 2 KB/s
        \item Write Speed: 1 KB/s
    \end{itemize}
    Let us write a 4 KB text file to the brand-new SSD. The overall writing time is 4 seconds (write speed $\times$ file dimension, 1 KB/s $\times$ 4 KB).

    \newpage

    Now, let us write an 8 KB picture file for the almost brand-new SSD; thankfully, there is space. The overall Writing time is 8 seconds, and the calculation is the same as above.

    \begin{center}
        \includegraphics[width=.7\textwidth]{img/SSD-1.pdf}
    \end{center}
    Now, consider that the first file inserted on the first page is unnecessary.

    \highspace
    Finally, let's write a 12 KB pic to the SSD. Theoretically, the image should take 12 seconds. However, it is wrong! The SSD has only two empty pages and one dirty page (invalid). Then, the operations are:
    \begin{enumerate}
        \item Read block into cache.
        \item Delete page from cache (set dirty page).
        \item Write a new picture into the cache.
        \item Erase the old block on the SSD.
        \item Write cache to SSD.
    \end{enumerate}
    The OS only thought it was writing 12 KBs of data when the SSD had to read 8 KBs (2 KB/s) and then write 20 KBs (1 KB/s), the entire block. The writing should have taken 12 seconds but took 4 + 20 = 24 seconds, resulting in a write speed of 0.5 KB/s, not 1 KB/s.
\end{examplebox}

\noindent
A direct mapping between Logical and Physical pages is not feasible inside the SSD. Therefore, each SSD has an FTL component that makes the SSD \emph{look like an HDD}.

\highspace
The \definition{Flash Translation Layer (FTL)} is placed in the hierarchy between the \textbf{File System} and \textbf{Flash Memory}. It aims to do \textbf{three main actions}:
\begin{enumerate}
    \item \textbf{Data Allocation} and \textbf{Address Translation}: It efficiently reduces Write Amplification effects (see page~\pageref{Write amplification (WA)}); the program pages within an erased block in order (from low to high pages), called \definition{Log-Structured FTL}.

    \item \textbf{Garbage collection}: reuse of pages with old data (Dirty/Invalid).

    \item \textbf{Wear levelling}: FTL should spread across the blocks of the flash, ensuring that all of the blocks of the device wear out roughly simultaneously.
\end{enumerate}

\begin{examplebox}[: Log-Structured FTL]
    Assume that a page size is $4$ KB and a block consists of four pages. The write list is (\texttt{Write(pageNumber, value)}):
    \begin{itemize}
        \item \texttt{Write(100, a1)}
        \item \texttt{Write(101, a2)}
        \item \texttt{Write(2000, b1)}
        \item \texttt{Write(2001, b2)}
        \item \texttt{Write(100, c1)}
        \item \texttt{Write(101, c2)}
    \end{itemize}
    The steps are:
    \begin{enumerate}
        \item The initial state is with all pages marked as \texttt{INVALID(i)}:
        \begin{center}
            \includegraphics[width=.7\textwidth]{img/log-structured-ftl-1.png}
        \end{center}

        \item Erase block zero:
        \begin{center}
            \includegraphics[width=.7\textwidth]{img/log-structured-ftl-2.png}
        \end{center}

        \item Program pages in order and update mapping information (\texttt{Write(100, a1)}):
        \begin{center}
            \includegraphics[width=.8\textwidth]{img/log-structured-ftl-3.png}
        \end{center}

        \item After performing four writes (\texttt{Write(100, a1)}, \texttt{Write(101, a2)}, \texttt{Write(2000, b1)}, \texttt{Write(2001, b2)}):
        \begin{center}
            \includegraphics[width=.8\textwidth]{img/log-structured-ftl-4.png}
        \end{center}

        \item After updating $100$ and $101$:
        \begin{center}
            \includegraphics[width=.8\textwidth]{img/log-structured-ftl-5.png}
        \end{center}
    \end{enumerate}
\end{examplebox}

\noindent
When an existing page is updated, then the old data becomes obsolete. The \textbf{old versions of data are called garbage}, and (sooner or later) garbage pages must be reclaimed for new writes to take place.

\highspace
The \definition{Garbage Collection} is the \textbf{process of finding garbage blocks and reclaiming them}. It is a simple process for fully garbage blocks but more complex for partial cases.

\begin{examplebox}[: how garbage collection works]
    The steps are:
    \begin{enumerate}
        \item Update request for existing data:
        \begin{center}
            \includegraphics[width=.24\textwidth]{img/garbage-collection-1.pdf}
        \end{center}

        \item Find a free page, and save the new data:
        \begin{center}
            \includegraphics[width=.15\textwidth]{img/garbage-collection-2.pdf}
        \end{center}

        \item This scenario may continue until there are not enough free blocks:
        \begin{center}
            \includegraphics[width=.5\textwidth]{img/garbage-collection-3.pdf}
        \end{center}

        \item Collect valid pages into a free block:
        \begin{center}
            \includegraphics[width=.5\textwidth]{img/garbage-collection-4.pdf}
        \end{center}

        \item Update the map table and erase invalid (obsolete) blocks:
        \begin{center}
            \includegraphics[width=.5\textwidth]{img/garbage-collection-5.pdf}
        \end{center}
    \end{enumerate}
\end{examplebox}

\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Problem 1: the Garbage Collection is too expensive!}}
\end{flushleft}
The Garbage Collection is \textbf{expensive}. It requires reading and rewriting of live data. Ideal garbage collection is a reclamation of a block that consists of only dead pages.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Partial solution}}
\end{flushleft}
Garbage Collection \textbf{costs depend on the amount of data blocks that must be migrated}. The \underline{\textbf{solution}} to alleviate the problem is to overprovision the device by \textbf{adding extra flash capacity} (cleaning can be delayed) and \textbf{running the garbage collection in the background} using less busy periods for the disk.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Problem 2: the Ambiguity of Delete}}
\end{flushleft}
When performing background Garbage Collection, the SSD assumes to know which pages are invalid. However, most file systems do not truly delete data. For example, on Linux, the \dquotes{delete} function is \texttt{unlink()}, removing the file meta-data but not the file itself.
\begin{enumerate}
    \item File is written on SSD
    \item File is deleted
    \item The Garbage Collection executes:
    \begin{itemize}
        \item 9 pages look valid to the SSD;
        \item BUT the OS knows only 2 pages are valid.
    \end{itemize}
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Partial solution}}
\end{flushleft}
New SSD SATA command TRIM (SCSI - UNMAP). The \textbf{OS tells the SSD that specific LBAs} (page \pageref{LBA (Logical Block Address)}) \textbf{are invalid and may be garbaged by the Garbage Collection}.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Problem 3: Mapping Table Size}}
\end{flushleft}
The \textbf{size of the page-level mapping table is too large}. In fact, with a 1 TB SSD with a 4-byte entry per 4 KB page, 1 GB of DRAM is needed for mapping!

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Partial solution}}
\end{flushleft}
Exists some approaches to reduce the costs of mapping:
\begin{itemize}
    \item \definition{Block Mapping} (block-based mapping). Mapping at block granularity to reduce the size of a mapping table. With this technique, there is a small writing problem: the FTL must read a large amount of live data from the old block and copy it into a new one.
    \begin{examplebox}[: Block Mapping]
        The first four writes:
        \begin{multicols}{2}
            \begin{itemize}
                \item \texttt{Write(2000, a)}
                \item \texttt{Write(2001, b)}
                \item \texttt{Write(2002, c)}
                \item \texttt{Write(2003, d)}
            \end{itemize}
        \end{multicols}
        \begin{center}
            \includegraphics[width=\textwidth]{img/block-mapping-1.png}
        \end{center}

        \noindent
        And finally the last one:
        \begin{itemize}
            \item \texttt{Write(2002, c')}
        \end{itemize}
        \begin{center}
            \includegraphics[width=\textwidth]{img/block-mapping-2.png}
        \end{center}
    \end{examplebox}
    
    \item \definition{Hybrid Mapping}. FTL maintains two tables: \textbf{log blocks} (page mapped) and \textbf{data blocks} (block mapped). The FTL will consult the page mapping table and block mapping table when looking for a particular logical block.
    \begin{examplebox}[: Hybrid Mapping]
        Let's suppose the following sequence:
        \begin{multicols}{2}
            \begin{itemize}
                \item \texttt{Write(1000, a)}
                \item \texttt{Write(1001, b)}
                \item \texttt{Write(1002, c)}
                \item \texttt{Write(1003, d)}
            \end{itemize}
        \end{multicols}
        \begin{center}
            \includegraphics[width=\textwidth]{img/hybrid-mapping-1.png}
        \end{center}
        Let's update some pages:
        \begin{itemize}
            \item \texttt{Write(1000, a')}
            \item \texttt{Write(1001, b')}
            \item \texttt{Write(1002, c')}
            \item FTL updates only the page mapping information
        \end{itemize}
        \begin{center}
            \includegraphics[width=\textwidth]{img/hybrid-mapping-2.png}
        \end{center}
        When needed, FTL can perform MERGE operations:
        \begin{center}
            \includegraphics[width=\textwidth]{img/hybrid-mapping-3.png}
        \end{center}
    \end{examplebox}
    
    \item \definition{Page Mapping plus Caching}. The basic idea is to \textbf{cache the active part of the page-mapped FTL}. If a given workload only accesses a small set of pages, the translations of those pages will be stored in the FTL memory. It will perform well without high memory cost if the cache can contain the necessary working set. Cache miss overhead exists; we need to accept it.
\end{itemize}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{The importance of Wear Leveling}}
\end{flushleft}
As we have mentioned, the wear leveling is essential. The erase/write cycle is limited in Flash Memory. All blocks should wear out roughly at the same time.

\highspace
The log-structured approach and garbage collection help spread writing. However, a block may contain cold data: the FTL must periodically read all the live data from such blocks and re-write it elsewhere.
A \textbf{disadvantage} is that the wear levelling \textbf{increases the write amplification} of the SSD and consequently decreases performance. However, to \textbf{partially fix} this, a simple policy to apply is that each flash block has an \definition{Erase/Write Cycle Counter} and maintains the value of:
\begin{equation}
    \left| \mathrm{Max}\left(\text{EW cycle}\right) - \mathrm{Min}\left(\text{EW cycle}\right) \right| < e
\end{equation}

\begin{flushleft}
    \textcolor{Red2}{\textbf{HDD vs SSD}}
\end{flushleft}
Exists two metrics:
\begin{itemize}
    \item \definition{Unrecoverable Bit Error Ratio (UBER)}. A metric for the rate of occurrence of data errors, equal to the \textbf{number of data errors per bits read}.
    
    \item \definition{Endurance rating: Terabytes Written (TBW)}. It is the total amount of data that can be written into an SSD before it is likely to fail. \textbf{The number of terabytes that may be written to the SSD while still meeting the requirements}.
\end{itemize}
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.7\textwidth]{img/hdd-vs-ssd.png}
\end{figure}

\newpage

\paragraph{RAID}\label{paragraph: RAID}

\definition{RAID (Redundant Array of Independent Disks)} is a \textbf{data storage virtualization technology}\footnote{I/O virtualization: data are distributed transparently over the disks, then no action is required of the users.} that \textbf{combines multiple physical disk drive components into one or more logical units for the purposes of data redundancy}, \textbf{performance improvement}, or both. This contrasts the previous concept of highly reliable mainframe disk drives, which were referred to as  \definition{Single Large Expensive Disks (SLED)}, also called \definition{Just a Bunch of Disks (JBOD)} method where each disk is a separate device with a different mount point.

\highspace
The data are striped across several disks accessed in parallel:
\begin{itemize}
    \item \textbf{High data transfer rate}: large data accesses (heavy I/O operations).
    \item \textbf{High I/O rate}: small but frequent data accesses (light I/O operations).
    \item \textbf{Load Balancing} across the disks.
\end{itemize}
Two techniques exist to guarantee these features: \textbf{data striping} (improve performance) and \textbf{redundancy} (improve reliability).

\highspace
\definition{Data Striping} is the technique of \textbf{segmenting logically sequential data}, such as a file, so that \textbf{consecutive segments are stored on different physical storage devices}. A small quantity of terminology:
\begin{itemize}
    \item \definition{Striping}: \textbf{data are written sequentially} (a vector, a file, a table, etc) in units (stripe units such as bit, byte, and blocks) \textbf{on multiple disks} according to a cyclic algorithm (round robin).

    \item \definition{Stripe unit}: the \textbf{dimension of the data unit written on a single disk}.

    \item \definition{Stripe width}: number of \textbf{disks considered by the striping algorithm}:
    \begin{enumerate}
        \item \textbf{Multiple independent I/O requests} will be executed in parallel by several disks, decreasing the disks' queue length (and time).

        \item Multiple disks will execute \textbf{single Multiple-Block I/O requests} in parallel, increasing the transfer rate of a single request.
    \end{enumerate}
\end{itemize}

\highspace
The \textbf{redundancy technique is introduced because} the more physical disks in the array, the more significant the size and performance gains, but the \textbf{larger the probability of failure of a disk}.

In fact, the \emph{probability of a failure} (assuming independent failures) in an array of 100 disks is 100 higher than the probability of a failure of a single disk! For \textcolor{Green4}{\textbf{example}}, if a disk has a \definition{Mean Time To Failure (MTTF)} of 200.000 hours (23 years), an array of 100 disks will show an MTTF of 2000 hours (3 months).

\highspace
The \definition{Redundancy} is the \textbf{technique of data duplication or error correcting codes} (stored on disks different from the ones with the data) \textbf{that are computed to tolerate loss due to disk failures}. Since write operations must also update the redundant information, their \textbf{performance is worse than traditional writing}.

\highspace
Data is distributed across the drives in one of several ways, referred to as \definition{RAID levels}, depending on the required level of redundancy and performance. The different schemes, or data distribution layouts, are named by the word \emph{RAID} followed by a number, for example RAID 0 or RAID 1. Each scheme, or RAID level, provides a different balance among the key goals: reliability, availability, performance, and capacity. The RAID levels are:
\begin{itemize}
    \item \texttt{RAID 0} striping only
    \item \texttt{RAID 1} mirroring only
    \begin{itemize}
        \item \texttt{RAID 0+1} nested levels
        \item \texttt{RAID 1+0} nested levels
    \end{itemize}
    \item \texttt{RAID 2} bit interleaving (not used)
    \item \texttt{RAID 3} byte interleaving - redundancy (parity disk)
    \item \texttt{RAID 4} block interleaving - redundancy (parity disk)
    \item \texttt{RAID 5} block interleaving - redundancy (parity block distributed)
    \item \texttt{RAID 6} greater redundancy (2 failed disks are tolerated)
\end{itemize}
Note: these notes do \underline{not} study the levels \texttt{RAID 2} and \texttt{RAID 3}.

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        \textbf{Topic} & \textbf{Page} \\
        \midrule
        \texttt{RAID 0} & \hyperlink{RAID 0}{\hypergetpageref{RAID 0}} \\
        \texttt{RAID 1} & \hyperlink{RAID 1}{\hypergetpageref{RAID 1}} \\
        \texttt{RAID 0 + 1} & \hyperlink{RAID 0 + 1}{\hypergetpageref{RAID 0 + 1}} \\
        \texttt{RAID 1 + 0} & \hyperlink{RAID 1 + 0}{\hypergetpageref{RAID 1 + 0}} \\
        \texttt{RAID 4} & \hyperlink{RAID 4}{\hypergetpageref{RAID 4}} \\
        \texttt{RAID 5} & \hyperlink{RAID 5}{\hypergetpageref{RAID 5}} \\
        \texttt{RAID 6} & \hyperlink{RAID 6}{\hypergetpageref{RAID 6}} \\
        Comparison and characteristics of RAID levels & \hyperlink{Comparison and characteristics of RAID levels}{\hypergetpageref{Comparison and characteristics of RAID levels}} \\
        \bottomrule
    \end{tabular}
    \caption{RAID - Table of Contents.}
\end{table}

\newpage

\begin{center}\label{RAID 0}
    \large
    \hypertarget{RAID 0}{\textcolor{Red2}{\textbf{RAID 0}}}
\end{center}

\noindent
In RAID 0, the data are \textbf{written on a single logical disk and split into several blocks distributed across the disks} according to a striping algorithm.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When is it used?}}
\end{flushleft}
It is used where \textbf{performance} and \textbf{capacity} are the primary concerns. These mean that a minimum of two drives is required.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Lower cost} because it does not employ redundancy (no error-correcting codes are computed and stored).
    \item \textbf{Best write performance} (it does not need to update redundant data and is parallelized).
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Disadvantages}}
\end{flushleft}
\textbf{Single disk failure} will result in data loss.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
The key idea is to present an \textbf{array of disks as a single large disk} and maximize parallelism by striping data across all $N$ disks.

\highspace
Now, we will give some metrics to understand how it is possible to \textbf{calculate access to a specific data block} and compare the \textbf{performance} of different RAID technologies.

\highspace
To \textbf{access to a specific data blocks}, the formulas are:
\begin{equation}
    \begin{array}{rcl}
        \texttt{Disk}    &=& \texttt{logical\_block\_number} \: \% \: \texttt{number\_of\_disks} \\ [.5em]
        \texttt{Offset}  &=& \dfrac{\texttt{logical\_block\_number}}{\texttt{number\_of\_disks}}
    \end{array}
\end{equation}
For example, given the following schema:
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{img/raid-1.pdf}
\end{figure}

\noindent
If it is requested, the logical block is 11, and the disks are 4:
\begin{equation*}
    \begin{array}{rcl}
        \texttt{Disk}    &=& 11 \% 4 = 3 \\ [.5em]
        \texttt{Offset}  &=& 11 \div 4 = 2.75 \approx 3, \text{ then physical block 2 starting from 0}
    \end{array}
\end{equation*}
Note that the \textbf{chunk size is critical} because it impacts disk array performance. With \textbf{smaller chunks}, there is \textbf{greater parallelism} than with \textbf{big chunks}, which have \textbf{reduced seek times}. The typical arrays use 64 KB chunks.

\highspace
To measure RAID \textbf{performance}, we focus on sequential and random workloads. Assume disks in the array have sequential transfer rate S, and the info about the disk is:
\begin{itemize}
    \item Average seek time: 7 ms
    \item Average rotational delay: 3 ms
    \item Transfer rate: 50 MB/s
\end{itemize}
For a single large transfer (10 MB):
\begin{equation*}
    \begin{array}{rcl}
        S &=& \dfrac{\texttt{transfer\_size}}{\texttt{time\_to\_access}} \\ [1.2em]
        S &=& 10 \text{ MB} \div \left(7 \text{ ms} + 3 \text{ ms} + 10 \text{ MB} \div 50 \text{ MB/s}\right) = 47.62 \text{ MB/s}
    \end{array}
\end{equation*}
If the disks in the array have a random transfer rate R, and for a set of small files (10 KB):
\begin{equation*}
    \begin{array}{rcl}
        R &=& \dfrac{\texttt{transfer\_size}}{\texttt{time\_to\_access}} \\ [1.2em]
        R &=& 10 \text{ KB} \div \left(7 \text{ ms} + 3 \text{ ms} + 10 \text{ KB} \div 50 \text{ MB/s}\right) = 0.98 \text{ MB/s}
    \end{array}
\end{equation*}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-bar} \textbf{Analysis}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Capacity}: $N$, where $N$ is the number of disks. Then, everything can be filled with data.

    \item \textbf{Reliability}: non-existent. If any drive fails, data is permanently lost. Then, the Mean Time To Failure (MTTF) equals the \definition{Mean Time To Data Loss (MTTDL)}:
    \begin{equation*}
        \texttt{MTTF} = \texttt{MTTDL}
    \end{equation*}

    \item \textbf{Sequential read and write}: $N \times S$

    \item \textbf{Random read and write}: $N \times R$
\end{itemize}
Where $N$ is the number of disks, $S$ the sequential transfer rate and $R$ the random transfer rate.

\newpage

\begin{center}\label{RAID 1}
    \large
    \hypertarget{RAID 1}{\textcolor{Red2}{\textbf{RAID 1}}}
\end{center}

\noindent
Although RAID 0 offers high performance, it has zero error recovery. For this reason, RAID 1 makes \textbf{two copies of all data} (again, a minimum of 2 disk drives are required).

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When is it used?}}
\end{flushleft}
It is used when \textbf{zero error recovery is not allowed}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}
    \item \textbf{High reliability}. When a disk fails, the \emph{second copy is used}.
    \item \textbf{Read of a data}. It can be \emph{retrieved} from the \emph{disk with shorter queueing, seek, and latency delays}.
    \item \textbf{Fast writes} (no error correcting code should be computed). But \emph{still slower than standard disks} (due to duplication).
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Disadvantages}}
\end{flushleft}
\begin{itemize}
    \item \textbf{High costs} because only 50\% of the capacity is used.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
In principle, a RAID 1 can mirror the content over more than one disk. This feature gives resiliency to errors even if more than one disk breaks. Also, it allows a \textbf{voting mechanism to identify errors not reported by the disk controller}. Unfortunately, this is \textbf{never used in practice because the overhead and costs are \underline{too high}}.

\highspace
However, disks could be coupled if several disks are available (always in an \underline{even} number). Nevertheless, the total capacity is halved, and each disk has a mirror. Then, the question is simple: \emph{How do we organize this architecture?} The answer is the nested RAIDs: \texttt{RAID 0 + 1} and \texttt{RAID 1 + 0}.

\highspace
We define the \texttt{RAID x + y} (or \texttt{RAID xy}) as:
\begin{itemize}
    \item $n \times m$ disks in total
    \item $m$ groups of $n$ disks
    \item Apply \texttt{RAID x} to each group of $n$ disks
    \item Apply \texttt{RAID y} considering the $m$ groups as single disks
\end{itemize}
\newpage
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.6\textwidth]{img/raid-2.pdf}
    \caption{\texttt{RAID x + y} general architecture.}
\end{figure}

\label{RAID 0 + 1}\noindent
The \hypertarget{RAID 0 + 1}{\texttt{\textbf{RAID 0 + 1}}} is a \textbf{group of striped disks (RAID 0) that are then mirrored (RAID 1)}. So:
\begin{enumerate}
    \item The mirroring first (RAID 1)
    \item Then the striping (RAID 0)
\end{enumerate}
There are necessary almost four drives. Note that after the first failure, the model becomes a RAID 0.
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{img/raid-3.pdf}
\end{figure}

\label{RAID 1 + 0}\highspace
The \hypertarget{RAID 1 + 0}{\texttt{\textbf{RAID 1 + 0}}} is a \textbf{group of mirrored disks (RAID 1) that are then striped (RAID 0)}. So: 
\begin{enumerate}
    \item The striping first (RAID 0)
    \item Then the mirroring (RAID 1)
\end{enumerate}
There are necessary almost four drives. Usually, it is used in databases with very high workloads (fast writes).
\newpage
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{img/raid-4.pdf}
\end{figure}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{not-equal} \textbf{Differences between RAID 01 and RAID 10}}
\end{flushleft}
Look at the following architectures:
\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/raid-5.pdf}
\end{figure}

\noindent
What we can say is:
\begin{itemize}
    \item The performance of RAID 01 and RAID 10 are the same.
    \item The \textbf{main difference is the fault tolerance}\footnote{Fault tolerance is the ability of a system to maintain proper operation despite failures or faults in or more of its components.}!
\end{itemize}
On most implementations of RAID controllers, \textbf{RAID 01 fault tolerance is less}. Since we have only two groups of RAID 0, if two drives (one in each group) fail, the entire RAID 01 will fail. Looking at the architecture above, if Disk 1 and 4 fail, both groups will be down. So, the whole RAID 01 will fail.

On the contrary, RAID 10 since there are many groups (as the individual group is only two disks), even if three disks fail (one in each group), the RAID 10 is still functional. Looking at the architecture above, even if Disk 1, 3, and 5 fail, the RAID 10 will still be functional.
\begin{equation*}
    \text{Fault Tolerance: } \texttt{RAID 01} \ggg \texttt{RAID 10}
\end{equation*}
So, given a choice between RAID 10 and RAID 01, it should be better to choose RAID 10 to have more fault tolerance.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-bar} \textbf{Analysis}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Capacity}: $N \div 2$, where $N$ is the number of disks. Then, two copies of all data, thus half capacity.

    \item \textbf{Reliability}: $1$ drive can fail, sometimes more! In an optimistic scenario, $N \div 2$ drives can fail without data loss.

    \item \textbf{Sequential write}: $\left(N \div 2\right) \times S$; two copies of all data, thus half throughput.

    \item \textbf{Sequential read}: $\left(N \div 2\right) \times S$; half of the read blocks are wasted, thus halving throughput.

    \item \textbf{Random read}: $N \times R$; it is the best scenario for RAID 1 because the read can be parallelized across all disks.

    \item \textbf{Random write}: $\left(N \div 2\right) \times R$; two copies of all data, thus half throughput.
\end{itemize}
Where $N$ is the number of disks, $S$ is the sequential transfer rate, and $R$ is the random transfer rate.

\highspace
It is essential to observe RAID 1. There is a notorious \textbf{problem} called the \definition{Consistent Update Problem}.

Mirrored writes \underline{should be atomic}. Then, all copies are written, or none are written. Unfortunately, this is very \textbf{difficult to guarantee} sometimes, for example, in a power failure scenario. To \textbf{solve} this problem, many RAID controllers include a \definition{write-ahead log}, a \textbf{battery backend}, and \textbf{non-volatile storage of pending writes}. With this system, a \textbf{recovery procedure ensures the recovery of the out-of-sync mirrored copies}.

\newpage

\begin{center}\label{RAID 4}
    \large
    \hypertarget{RAID 4}{\textcolor{Red2}{\textbf{RAID 4}}}
\end{center}

\noindent
RAID 4 consists of \textbf{block-level striping with a dedicated parity disk}.

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When is it used?}}
\end{flushleft}
RAID 1 offers highly reliable data storage, but it uses half the space of the array capacity. To achieve the \textbf{same level of reliability without wasting capacity}, it is possible to use RAID 4, which uses \textbf{information coding techniques to build lightweight error recovery mechanisms}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Good performance} of random reads.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Disadvantages}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Random Write performance is terrible} due to being \emph{bottlenecked} by the parity drive.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
Disk $N$ only stores parity information for the other $N-1$ disks. The parity is calculated using the \texttt{XOR} operation\footnote{\dquotes{A or B, but not A and B}}.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/raid-6.pdf}
    \caption{RAID 4 - \emph{How does it work?}}
\end{figure}

\noindent
The \textbf{\underline{serial}} or \textbf{\underline{random read}} is not a problem in RAID 4 because there is a \textbf{parallelization across all non-parity blocks} in the stripe despite a tiny performance reduction due to the parity disk.

\newpage

\noindent
During the parity writing, the blocks are updated. The \textbf{\underline{random write}} performance is affected by the approach used:
\begin{itemize}
    \item \definition{Additive parity} (as known as reconstruct-writes):\index{reconstruct-writes}
    \begin{enumerate}
        \item Read all other blocks in a stripe in parallel;
        \item \texttt{XOR} those with the new block to form a new parity block;
        \item Write the new data block and new parity block to disks.
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.8\textwidth]{img/raid-7.pdf}
    \end{figure}

    \item \definition{Subtractive parity} (as known as read-modify-writes):\index{read-modify-writes}
    \begin{enumerate}
        \item Read only the old data block to be updated and the old parity block;
        \item Compute the new parity block using:
        \begin{equation*}
            P_{new} = \left(D_{new} \:\veebar\: D_{old}\right) \:\veebar\: P_{old}
        \end{equation*}
        Where $\veebar$ is the logical \texttt{XOR}.
        \item Write the new data block and new parity block to disks.
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.8\textwidth]{img/raid-8.pdf}
    \end{figure}
\end{itemize}
Despite the \textbf{\underline{sequential write}} does \textbf{not suffer any performance effect} from the parity disk. Because it uses full-stripe write:
\begin{enumerate}
    \item Buffer all data blocks of a stripe
    \item Compute the parity block
    \item Write all data and parity blocks in parallel\cite{raid-and-data-integrity-slides}
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-bar} \textbf{Analysis}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Capacity}: $N - 1$, where $N$ is the number of disks, and the $-1$ is because one is dedicated to the parity disk.

    \item \textbf{Reliability}: $1$ drive can fail. Massive performance degradation during partial outage.

    \item \textbf{Sequential read/write}: $\left(N - 1\right) \times S$; parallelization across all non-parity blocks in the stripe (parity disk has no effect).

    \item \textbf{Random read}: $\left(N - 1\right) \times R$; reads parallelize over all but the parity drive (parity disk has no effect).

    \item \textbf{Random write}: $R \div 2$; writes serialize due to the parity drive, and each write requires one read and one write of the parity drive.
\end{itemize}
Where $N$ is the number of disks, $S$ is the sequential transfer rate, and $R$ is the random transfer rate.

\newpage

\begin{center}\label{RAID 5}
    \large
    \hypertarget{RAID 5}{\textcolor{Red2}{\textbf{RAID 5}}}
\end{center}

\noindent
RAID 5 \textbf{rotates parity blocks across stripes}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When is it used?}}
\end{flushleft}
Unlike in RAID 4, parity information is distributed among the drives. This technique is \textbf{used to improve significantly the random write throughput} against the RAID 4 system.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
\end{flushleft}
\begin{itemize}
    \item Improved random write despite the RAID 4 system.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
The writes are spread roughly evenly across all drives.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/raid-9.pdf}
\end{figure}

\noindent
The \textbf{\underline{random write}} in RAID 5 is:
\begin{enumerate}
    \item Read the target block and the parity block
    \item Use subtraction to calculate the new parity block
    \item Write the target block and the parity block
\end{enumerate}
Thus, \textbf{four total operations} (two reads, two writes) \textbf{are distributed across all drives}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{chart-bar} \textbf{Analysis}}
\end{flushleft}
\begin{itemize}
    \item \textbf{Capacity}: $N - 1$ (\textbf{same as RAID 4}), where $N$ is the number of disks.
    \item \textbf{Reliability}: $1$ drive can fail (\textbf{same as RAID 4}). Massive performance degradation during partial outage.
    \item \textbf{Sequential read/write}: (N - 1) * S (\textbf{same as RAID 4}); parallelization across all non-parity blocks in the stripe (parity disk has no effect).
    \item \textbf{Random read}: $N \times R$; unlike RAID 4, \textbf{reads parallelize over all drives}.
    \item \textbf{Random write}: $\left(N \div 4\right) \times R$; unlike RAID 4, \textbf{writes parallelize over all drives}, and each write requires two reads and two writes, hence $N \div 4$.
\end{itemize}
Where $N$ is the number of disks, $S$ is the sequential transfer rate, and $R$ is the random transfer rate.

\longline

\begin{center}\label{RAID 6}
    \large
    \hypertarget{RAID 6}{\textcolor{Red2}{\textbf{RAID 6}}}
\end{center}
RAID 6 can tolerate multiple disk faults (\textbf{more fault tolerance}) concerning RAID 5. It tolerates two concurrent failures.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{How does it work?}}
\end{flushleft}
It uses Solomon-Reeds codes with two redundancy schemes and requires $N + 2$ disks (where $N$ is the number of disks). Note that the \textbf{minimum set is 4 data disks}.

Unfortunately, it has a \textbf{high overhead for writes} (computation of parities) because each write requires six disk accesses due to the need to update both the P and Q parity blocks (slow writes).

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/raid-010.pdf}
\end{figure}

\newpage

\begin{center}\label{Comparison and characteristics of RAID levels}
    \large
    \hypertarget{Comparison and characteristics of RAID levels}{\textcolor{Red2}{\textbf{Comparison and characteristics of RAID levels}}}
\end{center}

\noindent
The following table shows eight fundamental properties of the RAID levels.
\begin{itemize}
    \item $N$: number of drives
    \item $R$: random access speed
    \item $S$: sequential access speed
    \item $D$: latency to access a single disk
\end{itemize}
\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l l l l @{}}
        \toprule
        & \texttt{RAID 0} & \texttt{RAID 1} & \texttt{RAID 4} & \texttt{RAID 5} \\
        \midrule
        Capacity & $N$ & $N \div 2$ & $N - 1$ & $N - 1$ \\
        Reliability & $0$ & $1 \: \lor : N \div 2$ & $1$ & $1$ \\
        \cmidrule{1-5}
        Sequential Read & $N \times S$ & $\left(N \div 2\right) \times S$ & $\left(N - 1\right) \times S$ & $\left(N - 1\right) \times S$ \\
        Sequential Write & $N \times S$ & $\left(N \div 2\right) \times S$ & $\left(N - 1\right) \times S$ & $\left(N - 1\right) \times S$ \\
        Random Read & $N \times R$ & $N \times R$ & $\left(N - 1\right) \times R$ & $N \times R$ \\
        Random Write & $N \times R$ & $\left(N \div 2\right) \times R$ & $R \div 2$ & $\left(N \div 4\right) \times R$ \\
        \cmidrule{1-5}
        Read & $D$ & $D$ & $D$ & $D$ \\
        Write & $D$ & $D$ & $2 \times D$ & $2 \times D$ \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of RAID levels.}
\end{table}

\noindent
Where the throughput is:
\begin{itemize}
    \item Sequential Read
    \item Sequential Write
    \item Random Read
    \item Random Write
\end{itemize}
And the latency is:
\begin{itemize}
    \item Read
    \item Write
\end{itemize}

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/raid-011.pdf}
    \caption{Characteristics of RAID levels.}
\end{figure}

\noindent
\textbf{RAID 0} has the \underline{best performance} and \underline{most capacity}.

\highspace
\textbf{RAID 1} (10 better than 01) or \textbf{RAID 6} have the \underline{greatest error recovery}.

\highspace
\textbf{RAID 5} has the better \textbf{\emph{balance}} between \underline{space}, \underline{performance}, and \underline{recoverability}.

\newpage

\paragraph{DAS, NAS and SAN}\label{paragraph: DAS, NAS and SAN}

As the last argument, we introduce \textbf{three different typologies} of storage systems:
\begin{itemize}
	\item \definition{Direct Attached Storage (DAS)} is a \textbf{storage system directly attached to a server or workstation}. They are visible as disks/volumes by the client OS.
	
	\item \definition{Network Attached Storage (NAS)} is a \textbf{computer connected to a network that provides only file-based data storage services} (e.g. FTP, Network File System) to other devices on the network and is visible as File Server to the client OS.
	
	\item \definition{Storage Area Networks (SAN)} are \textbf{remote storage units connected to a PC using a specific networking technology} (e.g. Fiber Channel) and are visible as disks/volumes by the client OS.
\end{itemize}
In the following schema, we can see a simple architectural comparison.
\begin{figure}[!htp]
	\centering
	\includegraphics[width=.8\textwidth]{img/nas-san-das-1.pdf}
\end{figure}

\begin{flushleft}
	\textcolor{Red2}{\faIcon{check} \textbf{DAS features}}
\end{flushleft}
DAS is a \textbf{storage system directly attached to a server or workstation}. The term is used to differentiate non-networked storage from SAN and NAS. The \textbf{main features} are:
\begin{itemize}
	\item Limited scalability.
	\item Complex manageability.
	\item Limited performance.
	\item To read files in other machines (the "file sharing" protocol of the OS must be used).
\end{itemize}
Note that all the \textbf{external disks connected to a PC with a point-to-point protocol can be considered DAS}.
\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{img/nas-san-das-2.pdf}
	\caption{DAS architecture.}
\end{figure}

\highspace
\begin{flushleft}
	\textcolor{Red2}{\faIcon{check} \textbf{NAS features}}
\end{flushleft}
A NAS unit is a \textbf{computer connected to a network that provides only file-based data storage services to other devices on the network}. NAS systems contain one or more hard disks, often organized into logical redundant storage containers or RAID. Finally, \textbf{NAS provides file-access services to the hosts connected to a TCP/IP network through Networked File Systems/SAMBA}. Each NAS element has its IP address. Furthermore, each NAS has good scalability.

\highspace
The \textbf{main differences between DAS and NAS} are:
\begin{itemize}
	\item DAS is simply an \textbf{extension of an existing server} and is \textbf{not necessarily networked}.
	\item NAS is designed as an easy and self-contained solution for \textbf{sharing files over the network}.
\end{itemize}
Regarding \textbf{performance}, NAS depends mainly on the speed and congestion of the network.
\begin{figure}[!htp]
	\centering
	\includegraphics[width=.7\textwidth]{img/nas-san-das-3.pdf}
	\caption{NAS architecture.}
\end{figure}

\highspace
\begin{flushleft}
	\textcolor{Red2}{\faIcon{check} \textbf{SAN features}}
\end{flushleft}
SANs are remote \textbf{storage units connected to servers using a specific networking technology}. SANs have a particular network dedicated to accessing storage devices. It has two distinct networks: one TCP/IP and another dedicated network (e.g. Fiber Channel). It has a high scalability.

\highspace
The \textbf{main difference between a NAS and a SAN} is that:
\begin{itemize}
	\item \textbf{NAS appears to the client OS as a file server}. Then, the client can map network drives to shares on that server.
	
	\item \textbf{A disk available through a SAN still appears to the client OS as a disk}. It will be visible in the disks and volumes management utilities (along with the client's disks) and available to be formatted with a file system.
\end{itemize}
\begin{figure}[!htp]
	\centering
	\includegraphics[width=.8\textwidth]{img/nas-san-das-4.png}
	\caption{SAN architecture.}
\end{figure}

\begin{figure}[!htp]
	\centering
	\includegraphics[width=\textwidth]{img/nas-san-das-5.pdf}
	\caption{DAS vs. NAS vs. SAN}
\end{figure}

\newpage

\subsubsection{Networking (architecture and technology)}\label{subsubsection: Networking (architecture and technology)}

\paragraph{Fundamental concepts}

In the data centre, servers' \emph{performance increases} over time, and the demand for inter-server \emph{bandwidth also increases}.

\highspace
A \example{solution} can be to double the aggregate compute capacity or the aggregate storage simply by \textbf{doubling the number of compute or storage elements}.

\highspace
The \definition{doubling leaf bandwidth} is used since the networking has no straightforward horizontal scaling solution. Then, with \textbf{twice as many servers}, we will have \textbf{twice as many network ports and thus twice as much bandwidth}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is a bisection bandwidth?}}
\end{flushleft}
\definition{Bisection bandwidth} is a \textbf{measure of network performance}, \textbf{defined as the bandwidth available between two equal-sized partitions when a network is bisected}. This measure accounts for the \emph{bottleneck bandwidth of the entire network}, providing a representation of the actual bandwidth available in the system. The bisection should be done to minimize the bandwidth between the two partitions. It is often used to evaluate and compare networks for parallel architectures, including point-to-point communication systems or on-chip micro-networks.

\highspace
\textbf{Assuming that every server needs to talk to every other server, we need to double not just leaf bandwidth but bisection bandwidth}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How to design a data centre network?}}
\end{flushleft}
There are many design principles to follow:
\begin{itemize}
    \item \textbf{Very scalable} in order to support a vast number of servers;
    \item \textbf{Minimum cost} in terms of basic building blocks (e.g. switches);
    \item \textbf{Modular} to reuse simple basic modules;
    \item \textbf{Reliable} and \textbf{resilient};
    \item It may exploit novel/proprietary technologies and protocols incompatible with legacy Internet.
\end{itemize}

\newpage

\noindent
The \definition{Data Center Network (DCN)} connects a data centre's computing and storage units to achieve optimum performance. It can be \textbf{classified} into \textbf{three main categories}:
\begin{itemize}
    \item \definition{DCN Switch-centric architectures}. DCN uses \textbf{switches to perform packet forwarding}.\footnote{\definition{Packet forwarding} is the passing of packets from one network segment to another by nodes on a computer network.}

    \item \definition{DCN Server-centric architectures}. DCN \textbf{uses servers with multiple Network Interface Cards (NICs)}\footnote{A \definition{Network Interface Cards (NICs)} is a computer hardware component that connects a computer to a computer network.} \textbf{to act as switches} and perform other computational functions.

    \item \definition{DCN Hybrid architectures}. DCN \textbf{combines switches and servers} for packet forwarding.
\end{itemize}

\newpage

\paragraph{Switch-centric: classical Three-Tier architecture}

The \definition{Three-Tier architecture}, also called \definition{Three Layer architecture}, \textbf{configures the network in three different layers}:
\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/networking-1.pdf}    
\end{figure}

\noindent
It is a simple Data Center Network topology. 
\begin{itemize}
    \item The \textbf{servers} are \textbf{connected to the DCN through access switches}.
    \item \textbf{Each access-level switch is connected to at least two aggregation-level switches}.
    \item \textbf{Aggregation-level switches are connected to core-level switches} (gateways).
\end{itemize}
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Bandwidth can be increased} by \textbf{increasing the switches at the core and aggregation layers}, and by using routing protocols such as Equal Cost Multiple Path (ECMP) that equally shares the traffic among different routes.
    \item \textbf{Very simple} solution.
\end{enumerate}
\begin{flushleft}
    \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Cons}}
\end{flushleft}
\begin{enumerate}
    \item \textbf{Very expensive} in large data centers because the upper layers require faster network equipments.
    \item \textbf{Cost very high} in term of acquisition and energy consumption.
\end{enumerate}
In the \textbf{access layer}, there are two possible architectures:
\begin{itemize}
    \item \definition{ToR (Top-of-Rack) architecture}. All servers in a rack are connected to a ToR access switch within the same rack. The aggregation switches are in dedicated racks or shared racks with other ToR switches and servers.\label{ToR (Top-of-Rack) architecture}
    
    \begin{flushleft}
        \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
    \end{flushleft}
    \begin{enumerate}
        \item \textbf{Simpler cabling} because the number of cables is limited.
        \item \textbf{Lower costs} because the number of ports per switch is limited.
    \end{enumerate}

    \begin{flushleft}
        \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Cons}}
    \end{flushleft}
    \textbf{Higher complexity} for switch management.

    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.9\textwidth]{img/networking-2.pdf}
        \caption{ToR (Top-of-Rack) architecture.}
    \end{figure}

    
    \item \definition{EoR (End-of-Row) architecture}. Aggregation switcher are positioned one per corridor, at the end of a line of rack. Servers in a racks are connected directly to the aggregation switch in another rack. Exists a patch panel to connect the servers to the aggregation switch.\label{EoR (End-of-Row) architecture}
    
    \begin{flushleft}
        \textcolor{Green3}{\faIcon{check} \textbf{Advantages}}
    \end{flushleft}
    \textbf{Simpler switch management}.

    \begin{flushleft}
        \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Cons}}
    \end{flushleft}
    The aggregation switches must have a larger number of ports, then:
    \begin{enumerate}
        \item \textbf{Complex cabling}.
        \item \textbf{Longer cables} then \textbf{higher costs}.
    \end{enumerate}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.9\textwidth]{img/networking-3.pdf}
        \caption{EoR (End-of-Row) architecture.}
    \end{figure}
\end{itemize}

\newpage

\paragraph{Switch-centric: Leaf-Spine architectures}

In the following section we present two Leaf-Spine architectures: the Leaf-Spine model and the Pod-based model (Fat Tree Network).

\highspace
The \definition{Leaf-Spine architecture} consists of \textbf{two levels of interconnection}:
\begin{enumerate}
    \item The \textbf{leaf} (which is a ToR switch);
    \item The \textbf{spine} (which has dedicated switches, aggregation switches).
\end{enumerate}
In practice, servers have two interfaces connected to two ToR switches to provide fault tolerance.
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.7\textwidth]{img/networking-4.png}
    \caption{Leaf-Spine architecture.}
\end{figure}

\noindent
Now we will explain the Leaf-Spine architecture. If $m$ is the number of \textbf{mid-stage switches} and $n$ is the \textbf{number of inputs and outputs}, the Leaf-Spine topology is as follows:
\begin{itemize}
    \item Each switch module is bi-directional. 
    \begin{itemize}
        \item \emph{Leaf} has $2k$ bidirectional ports per module;
        \item \emph{Spine} has $k$ bidirectional ports per module.
    \end{itemize}

    \item Each path traverses either 1 or 3 modules.
\end{itemize}
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.3\textwidth]{img/networking-5.pdf}
    \caption{Explanation of Leaf-Spine architecture.}
\end{figure}

\noindent
The \textbf{advantages} are: use of homogeneous equipment, simple routing, the number of hops is the same for any pair of nodes, small blast radius.

\begin{examplebox}
    There are $3072$ servers and $3072$ ports available at 10 Gbit/s. Provides a leaf-spine design.

    There are \textbf{two possible designs}.
    \begin{enumerate}
        \item The first consists of 96 switches with 64 ports and 32 switches with 96 ports.
        \begin{center}
            \includegraphics[width=.4\textwidth]{img/networking-6.pdf}
        \end{center}
        
        \item The second has only 8 switches but they have more ports: 384 ($8 \times 384 = 3072$).
        \begin{center}
            \includegraphics[width=.5\textwidth]{img/networking-7.pdf}
        \end{center}
    \end{enumerate}
\end{examplebox}

\begin{examplebox}
    There are $6144$ servers and $6144$ ports available at 10 Gbit/s. Provides a leaf-spine design.

    There are \textbf{two possible designs}.
    \begin{enumerate}
        \item The first consists of 192 switches with 64 ports and 32 switches with 192 ports.
        \begin{center}
            \includegraphics[width=.4\textwidth]{img/networking-8.pdf}
        \end{center}
        
        \item The second has only 16 switches but they have more ports: 384 ($16 \times 384 = 6144$).
        \begin{center}
            \includegraphics[width=.5\textwidth]{img/networking-9.pdf}
        \end{center}
    \end{enumerate}
\end{examplebox}

\noindent
The \textbf{Pod-based model}, also called \definition{Fat Tree Network}, is another network architecture used to \textbf{increase the scaling} feature respecting the leaf-spine.

\highspace
It transforms each group of spine-leaf into a \definition{PoD (Point of Delivery)}\footnote{A Point Of Delivery is a module or group of network, compute, storage and application components that work together to deliver a network service. The PoD is a repeatable pattern and its components increase the modularity, scalability and manageability of data.} and adds a super spine layer.

\highspace
It is a \textbf{highly scalable} and \textbf{cost-effective} DCN architecture designed to \textbf{maximise bisection bandwidth}. It can be built using standard Gigabit Ethernet switches with the same number of ports.

\highspace
It is composed by a \textbf{\emph{leaf}} of $2k^{2}$ bidirectional ports:
\begin{itemize}
    \item $k^{2}$ ports to the servers;
    \item $k^{2}$ ports to the data center network.
\end{itemize}
In general, let $k^{2}P$ servers: there are $2kP$ switches with $2k$ ports and $k^{2}$ switches with $P$ ports. Using the Fat-Tree model, \textbf{the $\mathbf{P}$ value is $\mathbf{2k}$, so for $\mathbf{2k^{3}}$ servers, there are $\mathbf{5k^{2}}$ switches with $\mathbf{2k}$ ports} ($k^{2} + 2k \cdot 2k$).

\highspace
At the \textbf{edge layer}, there are $2k$ pods (groups of servers), each with $k^{2}$ servers.
\begin{itemize}
    \item Each edge switch is directly connected to $k$ servers in a pod and $k$ aggregation switches. 
    \item A Fat-Tree network with $2k$-port commodity switches can accomodate $2k^{3}$ servers in total.
    \item $k^{2}$ core switches with $2k$-port each, each one connected to $2k$ pods.
    \item Each aggregation switch is connected to $k$ core switches.
\end{itemize}

\newpage

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/networking-11.png}
    \caption{Fat-Tree Network, with $k=2$, 4 pods, 16 servers, 20 switches.}
\end{figure}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.4\textwidth]{img/networking-10.pdf}
    \caption{Explanation of Fat-Tree Network.}
\end{figure}

\newpage

\paragraph{Server-centric and hybrid architectures}

\definition{CamCube} is a \textbf{server-centric architecture} typically proposed for building container-sized data centres.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Advantages}}
\end{flushleft}
It can \textbf{reduce implementation and maintenance costs} by using only servers to build the Data Center Network. It also exploits network locality to \textbf{increase communication efficiency}.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{thumbs-down} \textbf{Disadvantages}}
\end{flushleft}
It \textbf{requires servers with Multiple Network Interface cards} to build a 3D tours network, \textbf{long paths} and \textbf{high routing complexity}.

\highspace
The hybrid architectures are \textbf{DCell}, \textbf{BCube} and \textbf{MDCube}.

\highspace
A \definition{DCell} is a \textbf{scalable and cost-effective hybrid architecture} that uses switches and servers for packet forwarding. It is a recursive architecture and uses a basic building block called $DCell_{0}$ to construct larger DCells.

\highspace
$DCell_{k}$ ($k > 0$) denotes a level-$k$ DCell \textbf{constructed by combining} $n+1$ \textbf{servers} in $DCell_{0}$. A $DCell_{0}$ has $n$ ($n < 8$) \textbf{servers directly connected by a commodity switch}.

\highspace
\emph{Disadvantages}: \textbf{long communication paths}, many required Network Interface Cards and \textbf{increased cabling costs}.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.6\textwidth]{img/networking-12.png}
    \caption{DCell hybrid architecture.}
\end{figure}

\newpage

\noindent
\definition{BCube} is a \textbf{hybrid and cost-effective architecture} that scales through recursion. It provides \textbf{high bisection bandwidth} and \textbf{graceful throughput degradation}.

\highspace
It uses BCube as a building block, consisting of $n$ servers connected to an $n$-port switch.

\highspace
A $BCube_{k}$ ($k>0$) is constructed with $n$ $BCube_{k-1}s$ and $n^{k}$ $n$-port switches. In a $BCube_{k}$ there are $n^{\left(k+1\right)}$ $k+1$-port servers and $k+1$ layers of switches.

\highspace
\emph{Disadvantages}: \textbf{limited scalability} and \textbf{high cabling costs} (NICs reason).

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/networking-13.png}
    \caption{BCube hybrid architecture.}
\end{figure}

\newpage

\noindent
\definition{MDCube} is designed to \textbf{reduce the number of cables used to connect containers}. 
\begin{itemize}
    \item Each container has an ID which is mapped to a multidimensional tuple.
    \item Each container is connected to a neighbouring container with a different tuple in one dimension.
    \item There are two types of connections: Intra-container links and high-speed inter-container links.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/networking-14.png}
    \caption{MDCube hybrid architecture.}
\end{figure}