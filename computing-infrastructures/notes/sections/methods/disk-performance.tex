\subsection{Disk performance}\label{subsection: Disk performance}

\subsubsection{HDD}\label{subsubsection: HDD performance}

We can calculate some performance metrics related to the types of delay of HDD (page \pageref{four types of hdd delay}).
\begin{itemize}
    \item \definition{Full Rotation Delay} $R$ is:
    \begin{equation}
        R = \dfrac{1}{\texttt{DiskRPM}}
    \end{equation}
    And in seconds:
    \begin{equation}
        R_{\text{sec}} = 60 \times R
    \end{equation}
    From the $R_{\text{sec}}$ we can also calculate the \definition{total rotation average}:
    \begin{equation}
        T_{\text{rotation AVG}} = \dfrac{R_{\text{sec}}}{2}
    \end{equation}
    It is half of a full rotation, because on average, the sector will be halfway around the platter from the current head position.

    \item \label{def: Seek Time} \definition{Seek Time}. The time to seek from one track to another depends on the distance moved. In real systems, this relation isn't perfectly linear, but it's often approximated as (\definition{seek average}):
    \begin{equation}
        T_{\text{seek AVG}} = \dfrac{T_{\text{seek MAX}}}{3}
    \end{equation}
    Where $T_{\text{seek MAX}}$ is the \textbf{time for the longest possible seek} (from \emph{outermost to innermost} track) and the division by 3 assumes a \textbf{uniform random distribution of seeks} across the disk.

    \item \definition{Transfer time}. It is the \textbf{time that data is either read from or written to the surface}. It \textbf{includes} the time the \textbf{head needs to pass on the sectors} and the \textbf{I/O transfer}:
    \begin{equation}
        T_{\text{transfer}} = \dfrac{\text{R/W of a sector}}{\text{Data transfer rate}}
    \end{equation}
    The \textbf{total time to complete a disk I/O operation} is called the $T_{\text{I/O}}$ \definition{Service Time}:
    \begin{equation}\label{eq: Service Time}
        T_{\text{I/O}} = T_{\text{seek}} + T_{\text{rotation}} + T_{\text{transfer}} + T_{\text{overhead}}
    \end{equation}
    If the disk is shared among processes, we must also consider \textbf{queueing time}. And the \definition{Response Time} is:
    \begin{equation}
        T_{\text{response}} = T_{\text{queue}} + T_{\text{I/O}}
    \end{equation}
    Where $T_{\text{queue}}$ depends on queue length, disk utilization rate, variance in request time, arrival rate of I/O requests.
    \newpage
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.7\textwidth]{img/performance-hdd-1.pdf}
        \caption{Service and response time.}
    \end{figure}
\end{itemize}

\begin{exercisebox}[: Mean Service Time of an I/O operation]\label{exercise: mean service time of an I/O operation}
    The data of the exercise are:
    \begin{itemize}
        \item Read/Write of a sector of $512$ bytes ($0.5$ KB)
        \item Data \underline{transfer} rate: $50$ MB/sec
        \item \underline{Rotation} speed: $10000$ RPM (Round Per Minute)
        \item Mean \underline{seek} time: $6$ ms
        \item Overhead \underline{Controller}: $0.2$ ms
    \end{itemize}
    The goal is to calculate the average I/O service time. To calculate the \emph{service time} $T_{\text{I/O}}$, we need the following information:
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] $T_{\text{seek}}$, which we already have, and it is $6$ ms.
        \item[\textcolor{Red2}{\faIcon{times}}] $T_{\text{rotation}}$
        \item[\textcolor{Red2}{\faIcon{times}}] $T_{\text{transfer}}$
        \item[\textcolor{Green3}{\faIcon{check}}] $T_{\text{overhead}}$, which we already have, and it is $0.2$ ms.
    \end{itemize}
    We also know the rotation and transfer information, but we want to know the \emph{mean} service time. Then we calculate the total rotation average $T_{\text{rotation AVG}}$:
    \begin{equation*}
        \begin{array}{rcl}
            R &=& \dfrac{1}{\texttt{DiskRPM}} = \dfrac{1}{10000} = 0.0001 \\ [1em]
            R_{\text{sec}} &=& 60 \cdot R = 60 \cdot 0.0001 = 0.006 \text{ seconds} \\ [1em]
            T_{\text{rotation AVG}} &=& \dfrac{R_{\text{sec}}}{2} = \dfrac{0.006}{2} = 0.003 \text{ seconds} = 3 \text{ ms}
        \end{array}
    \end{equation*}
    Finally, the transfer time is easy to calculate because we have the R/W of a sector and the data transfer rate. First we do a conversions from megabytes to kilobytes:
    \begin{equation*}
        \begin{array}{rcl}
            \text{Data transfer rate: }&& 50 \text{ MB/sec} \\ [1em] 
            &=& 50 \cdot 1024 \text{ KB/sec} \\ [1em] 
            &=& 51200 \text{ KB/sec} \\ [1em]
            T_{\text{transfer}} &=& \dfrac{0.5 \text{ KB/sec}}{51200 \text{ KB/sec}} \\ [1em]
            &=& 0.000009765625 \text{ sec } \cdot 1000 \\ [1em]
            &=& 0.009765625 \text{ ms } \approx 0.01 \text{ ms}
        \end{array}
    \end{equation*}
    The exercise can be completed by calculating the mean I/O service time required:
    \begin{equation*}
        \begin{array}{rcl}
            T_{\text{I/O}} &=& T_{\text{seek}} + T_{\text{rotation}} + T_{\text{transfer}} + T_{\text{overhead}} \\ [.8em]
            T_{\text{I/O}} &=& 6 + 3 + 0.01 + 0.2 = 9.21 \text{ ms}
        \end{array}
    \end{equation*}
\end{exercisebox}

\noindent
The I/O service time computed in the previous exercise (9.21 ms) assumes a \textbf{worst-case scenario}. This is very useful for understanding disk behavior, but it \textbf{doesn't always reflect what happens in real workloads}. It assumes:
\begin{enumerate}
    \item \textbf{Every time} we read a small file or sector,
    \item The \textbf{read must seek} to a new track,
    \item And then \textbf{wait for rotation} to bring the right sector under the head
\end{enumerate}
This is worst-case behavior, and happens when \textbf{files are very small}, so each one is just one block (e.g., 512 bytes); or disk is \textbf{heavily externally fragmented}, so even larger files are broken into scattered blocks. In such cases, \textbf{every access pays both seek and rotational delay}, making the access time slow and constant.

\highspace
We can introduce a new metric that comes from the idea of measuring locality: \textbf{how often we can avoid seek and rotation delays}. We define \definition{Data Locality $DL$} as:
\begin{equation}
    DL = \% \text{ of blocks that can be accessed without new seek or rotation}
\end{equation}
If \textbf{locality is high}, then most of our \textbf{data} is laid out in a nice, \textbf{sequential way}, therefore performance improves significantly. So, \textbf{locality determines whether our performance is close to best-case or worst-case}.

\highspace
Thanks to the Data Locality, it is possible to calculate the \definition{Average Service Time} by modifying the terms of \emph{seek} and \emph{rotation} of the Service Time equation (page \pageref{eq: Service Time}):
\begin{equation}\label{eq: Input Output average time}
    T_{\text{I/O AVG}} = \left(1 - DL\right) \cdot \left(T_{\text{seek}} + T_{\text{rotation}}\right) + T_{\text{transfer}} + T_{\text{controller}}
\end{equation}


\begin{exercisebox}[: Data Locality]
    The data of the exercise are:
    \begin{itemize}
        \item Read/Write of a sector of $512$ bytes ($0.5$ KB)
        \item \underline{Data Locality}: $DL = 75\%$
        \item Data \underline{transfer} rate: $50$ MB/sec
        \item \underline{Rotation} speed: $10000$ RPM (Round Per Minute)
        \item Mean \underline{seek} time: $6$ ms
        \item Overhead \underline{Controller}: $0.2$ ms
    \end{itemize}
    Since the Data Locality is $75\%$, only $25\%$ of the operations are affected by the DL:
    \begin{equation*}
        \left(1-DL\right) = \left(1 - 0.75\right) = 0.25
    \end{equation*}
    See the exercise on page \pageref{exercise: mean service time of an I/O operation} to understand the values of $T_{\text{seek}}$, $T_{\text{rotation}}$, $T_{\text{transfer}}$ and $T_{\text{overhead}}$:
    \begin{itemize}
        \item $T_{\text{seek}} = 6$
        \item $T_{\text{rotation}} = 3$
        \item $T_{\text{transfer}} = 0.01$
        \item $T_{\text{overhead}} = 0.2$
    \end{itemize}
    Finally the average time for read/write a sector of $0.5$ KB with a DL of $75\%$ is:
    \begin{equation*}
        \begin{array}{rcl}
            T_{\text{I/O AVG}} &=& 0.25 \cdot \left(6 + 3\right) + 0.01 + 0.2 \\ [1em]
            &=& 0.25 \cdot 9 + 0.21 \\ [1em]
            &=& 2.46 \text{ ms}
        \end{array}
    \end{equation*}
\end{exercisebox}

\newpage

\begin{exercisebox}[: Influence of \dquotes{Not Optimal} Data Allocation]
    The data of the exercise are:
    \begin{itemize}
        \item $10$ blocks of $1/10$ MB for each block ($10$ blocks of $1/10$ MB \dquotes{not well} distributed on disk)
        \item $T_{\text{seek}} = 6$ ms
        \item $T_{\text{rotation AVG}} = 3$ ms
        \item Data transfer rate: $50$ MB/sec
    \end{itemize}
    In the exercise you were asked to calculate the time taken to transfer a 1 MB file with 100\% and 0\% data locality:
    \begin{itemize}
        \item Data Locality equals to 100\%:
        \begin{itemize}
            \item An initial seek ($6$ ms)
            \item A total rotation average ($3$ ms)
            \item Now it's possible to do the 1MB global transfer directly because there are no blocks to seek or rotation latency:
            \begin{equation*}
                \text{1 MB of 50 MB} = \dfrac{1}{50} = 0.02 \text{ seconds } \cdot 1000 = 20 \text{ ms}
            \end{equation*}
            \item The total time is:
            \begin{equation*}
                T = 6 + 3 + 20 = 29 \text{ ms}
            \end{equation*}
        \end{itemize}

        \newpage
        \item Data Locality equals to 0\%:
        \begin{itemize}
            \item An initial seek ($6$ ms)
            \item A total rotation average ($3$ ms)
            \item In this case, it's not possible to do a global transfer directly, because each block is affected by the seek or rotation latency. Then we have to transfer block by block and calculate the delay:
            \begin{equation*}
                \text{1 MB of 10 MB} = \dfrac{1}{10} = 0.1 \text{ seconds } \cdot 1000 = 100 \text{ ms}
            \end{equation*}
            \item The total time is:
            \begin{equation*}
                T = \left(6 + 3 + 2\right) \cdot 10 = 110 \text{ ms}
            \end{equation*}
            Where $10$ is the number of blocks.
        \end{itemize}
    \end{itemize}
    Note: the controller times is not considered.
\end{exercisebox}

\newpage

\subsubsection{RAID}

We can calculate some performance metrics related to the RAID technology (page \pageref{paragraph: RAID}).
\begin{itemize}
    \item \definition{MTTF (Mean Time To Failure)} is the \textbf{expected time before a disk fails}. It is usually provided by the manufacturer, based on statistical modeling (often assuming exponential failure distribution).
    
    To calculate the MTTF for a Disk Array without redundancy, we must make the following assumptions:
    \begin{itemize}
        \item Constant Failure Rate;
        \item Exponentially distributed time to failure;
        \item Independent disk failures.
    \end{itemize}
    Then the formula is:
    \begin{equation}
        \texttt{MTTF}_{\text{array}} = \dfrac{\texttt{MTTF}_{\text{single disk}}}{\texttt{Number of Disks}}
    \end{equation}
    This is because the array fails if \textbf{any one} of its disks fails; so the \textbf{failure rate of the system increases linearly with the number of disks}.

    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{RAID 0 and MTTF}}. RAID 0 offers \textbf{no redundancy}, so it behaves like a pure array, and the \definitionWithSpecificIndex{$\texttt{MTTF}_{\text{RAID 0}}$}{RAID 0 - MTTF}{}:
    \begin{equation}
        \texttt{MTTF}_{\text{RAID 0}} = \texttt{MTTF}_{\text{array}} = \dfrac{\texttt{MTTF}_{\text{single disk}}}{\texttt{Number of Disks}}
    \end{equation}
    this is why RAID 0 is \textbf{high-risk} despite its performance: one disk failure means total data loss.

    \begin{examplebox}[: MTTF]
        If:
        \begin{itemize}
            \item Each disk has an MTTF of $1,000,000$ hours.
            \item We have an array of 100 disks.
        \end{itemize}
        Then:
        \begin{equation*}
            \texttt{MTTF}_{\text{array}} = \dfrac{1,000,000}{100} = 10,000 \text{ hours}
        \end{equation*}
        Without fault tolerance, our system is $100\times$ more likely to fail than a single disk.
    \end{examplebox}

    With RAID 5 or 6, the system can still work if one disk fails. However, \textbf{if another disk fails during the rebuild} process, \textbf{data is lost} in RAID 5 or \textbf{further degraded} in RAID 6. Therefore, the probability of data loss is approximately equal to the probability of a second failure occurring within the mean time to recovery (MTTR) window.
    \begin{equation}\label{eq: Mean Time To Failure of a RAID}
        \texttt{MTTF}_{\text{RAID}} = \texttt{MTTF}_{\text{array}} \cdot \left( \frac{1}{\text{\% of additional critical failures during MTTR}} \right)
    \end{equation}
    The first part $\texttt{MTTF}_{\text{array}}$ captures the \textbf{rate of first failures}. The second part \textbf{increases the effective \texttt{MTTF} depending} on \textbf{how} likely we are to \textbf{survive the rebuild window} (i.e., how unlikely a second failure is during \texttt{MTTR}). In other words, the \textbf{probability term} models the likelihood of \textbf{another failure occurring during a vulnerable period when the system is degraded} (e.g., during the process of rebuilding a RAID 5 array with one failed disk).

    \item The \textbf{Mean Time To Failure of each RAID level} (except the zero) is:
    \begin{itemize}
        \item \definitionWithSpecificIndex{\texttt{\underline{RAID 1}}}{MTTF of \texttt{RAID 1} $\texttt{MTTF}_{\text{RAID 1}}$}{} (page \hqpageref{RAID 1}) employs \textbf{mirroring}: every disk has an exact\break replica. This means each piece of data is stored \textbf{twice}, on two separate drives. The system can tolerate \textbf{one drive failure per mirrored pair} without data loss. In theory:
        \begin{itemize}
            \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Best case}}: up to $\frac{N}{2}$ drives can fail without affecting data (if each failed drive is from a different mirror).
            \item[\textcolor{Red2}{\faIcon{times}}] \textcolor{Red2}{\textbf{Worst case}}: system fails if both disks in any mirror pair fail before the array is rebuilt.
        \end{itemize}
        The formula used is:
        \begin{equation}
            \texttt{MTTF}_{\texttt{RAID 1}} = \texttt{MTTF}_{\text{array}} \cdot \left(
                \dfrac{1}{\text{\% of 2nd critical failure during MTTR}}
            \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $\texttt{MTTF}_{\text{array}} = \dfrac{\texttt{MTTF}_{\text{single disk}}}{\texttt{Number of Disks}}$
            \item $\text{MTTR}$: Mean Time To Repair.
            \item $\text{\% of 2nd critical failure during MTTR}$:
            \begin{equation}\label{eq: RAID 1 - probability of 2nd critical failure during MTTR}
                \text{\% of 2nd critical failure during MTTR} = \frac{1}{\texttt{MTTF}_{\text{disk}}} \cdot \texttt{MTTR}
            \end{equation}
            Where:
            \begin{itemize}
                \item $\dfrac{1}{\texttt{MTTF}_{\text{single disk}}}$ is the \textbf{failure rate} of a single disk.
                
                \item $\texttt{MTTR}$ is the the \textbf{duration in which the system is in a degraded state}, i.e., vulnerable to a second failure.
            \end{itemize}
        \end{itemize}
        This probability models the \textbf{chance that the mirror of a failed disk also fails during the rebuild window}. The extended version:\index{RAID 1 - MTTF}
        \begin{equation}\label{eq: RAID 1 - MTTF}
            \texttt{MTTF}_{\texttt{RAID 1}} = \dfrac{\texttt{MTTF}_{\text{disk}}^{2}}{N \cdot \texttt{MTTR}}
        \end{equation}

        \item \definitionWithSpecificIndex{\texttt{\underline{RAID 0 + 1}}}{MTTF of \texttt{RAID 01} $\texttt{MTTF}_{\text{RAID 0 + 1}}$}{} (page \hqpageref{RAID 0 + 1}) means \textbf{first striping} (RAID 0), then \textbf{mirroring} (RAID 1). The disks are divided into \textbf{stripe groups}, and then each group is mirrored. If any disk in a stripe group fails, \textbf{that whole group is lost}. The array can survive \textbf{a single group failure}, but not a second.
        \begin{equation}
            \texttt{MTTF}_{\texttt{RAID 0+1}} = \texttt{MTTF}_{\text{array}} \cdot \left(
                \dfrac{1}{\text{\% of 2nd critical failure during MTTR}}
            \right)
        \end{equation}

        \newpage

        Where:
        \begin{itemize}
            \item $\texttt{MTTF}_{\text{array}} = \dfrac{\texttt{MTTF}_{\text{single disk}}}{\texttt{Number of Disks}}$
            \item $\texttt{MTTR}$ is the mean time to repair (the vulnerable window).
            \item Despite the RAID 1, the \textbf{probability of a second critical failure during the mean time to repair} (MTTR) is:
            \begin{equation}
                \text{\% of 2nd critical failure during MTTR} = \left(\dfrac{G}{\texttt{MTTF}_{\text{disk}}}\right) \cdot \texttt{MTTR}
            \end{equation}
            Where $G$ is the number of \textbf{disks in each stripe group}, and $\dfrac{G}{\texttt{MTTF}_{\text{disk}}}$ is the \textbf{combined failure rate} of the $G$ disks in the remaining group. Multiplying this by the MTTR gives the probability that the second group will fail during the rebuild.
            
            Once a \textbf{disk} in one stripe group \textbf{fails}, the \textbf{entire group becomes inoperative}. To avoid total data loss, the \textbf{mirrored stripe group must not fail before rebuild completes}. 
        \end{itemize}
        The extended version:\index{RAID $0+1$ - MTTF}
        \begin{equation}\label{eq: RAID 01 - MTTF}
            \texttt{MTTF}_{\texttt{RAID 01}} = \dfrac{\texttt{MTTF}_{\text{disk}}^{2}}{N \cdot G \cdot \texttt{MTTR}} \quad G = \text{disks per stripe group}
        \end{equation}

        
        \item \definitionWithSpecificIndex{\texttt{\underline{RAID 1 + 0}}}{MTTF of \texttt{RAID 10} $\texttt{MTTF}_{\text{RAID 1 + 0}}$}{} (page \hqpageref{RAID 1 + 0}) combines \textbf{mirroring first} (RAID 1) then \textbf{striping} (RAID 0). Disks are organized into mirrored pairs. Then data is striped across the pairs. A disk can fail in any mirrored pair, the system remains operational. To cause data loss, \textbf{both disks in a mirrored pair must fail}. Thus, it \textbf{tolerates multiple disk failures} as long as no pair is entirely lost.
        
        \begin{equation}
            \texttt{MTTF}_{\texttt{RAID 1+0}} = \texttt{MTTF}_{\text{array}} \cdot \left(
                \dfrac{1}{\text{\% of 2nd critical failure during MTTR}}
            \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $\texttt{MTTF}_{\text{array}} = \dfrac{\texttt{MTTF}_{\text{single disk}}}{\texttt{Number of Disks}}$
            \item $\texttt{MTTR}$ is the mean time to repair (the vulnerable window).
            \item The probability of a second critical failure during the Mean Time To Repair (MTTR) is the same as for RAID 1 (see formula \ref{eq: RAID 1 - probability of 2nd critical failure during MTTR}):
            \begin{equation}
                \text{\% of 2nd critical failure during MTTR} = \dfrac{1}{\texttt{MTTF}_{\text{disk}}} \cdot \texttt{MTTR}
            \end{equation}
            The system is only at risk if the \textbf{exact mirror} of the failed disk also fails during the rebuild. So, unlike \texttt{RAID 0+1}, the failure risk localized, not dependent on an entire group.
        \end{itemize}
        The extended version:\index{RAID $1+0$ - MTTF}
        \begin{equation}\label{eq: RAID 10 - MTTF}
            \texttt{MTTF}_{\texttt{RAID 10}} = \texttt{MTTF}_{\texttt{RAID 1}} = \dfrac{\texttt{MTTF}_{\text{disk}}^{2}}{N \cdot \texttt{MTTR}}
        \end{equation}


        \item \definitionWithSpecificIndex{\texttt{\underline{RAID 4}}}{MTTF of \texttt{RAID 4} $\texttt{MTTF}_{\text{RAID 4}}$}{} (page \hqpageref{RAID 4}) is a block-level striping with a \textbf{dedicated parity disk}. All parity information is stored on \textbf{one disk}, creating a bottleneck on writes.
        
        \definitionWithSpecificIndex{\texttt{\underline{RAID 5}}}{MTTF of \texttt{RAID 5} $\texttt{MTTF}_{\text{RAID 5}}$}{} (page \hqpageref{RAID 5}) is a block-level striping with \textbf{distributed parity}. Parity is \textbf{spread across all disks}, improving write performance and eliminating the RAID 4 bottleneck.

        Both RAID 4 and RAID 5 can \textbf{tolerate a single disk failure}. Data is reconstructed using parity. Failure occurs only if \textbf{a second disk} fails \textbf{before} the first one is rebuilt.
        
        \begin{equation}
            \texttt{MTTF}_{\texttt{RAID 4, 5}} = \texttt{MTTF}_{\text{array}} \cdot \left(
                \dfrac{1}{\text{\% of 2nd failure during MTTR}}
            \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $\texttt{MTTF}_{\text{array}} = \dfrac{\texttt{MTTF}_{\text{single disk}}}{\texttt{Number of Disks}}$
            \item $\texttt{MTTR}$ is the mean time to repair (the vulnerable window).
            \item The probability of a second critical failure during the Mean Time To Repair (MTTR) is:
            \begin{equation}
                \text{\% of 2nd failure during MTTR} = \dfrac{N - 1}{\texttt{MTTF}_{\text{disk}}} \cdot \texttt{MTTR}
            \end{equation}
            Where $N$ is the number of disks in the array. The \textbf{first failure} does not cause data loss (covered by parity). The \textbf{second failure} is only dangerous \textbf{if it occurs during rebuild}. The probability considers all $N - 1$ remaining disks and their individual failure rates.
        \end{itemize}
        They have the same formula because they both rely on \textbf{single-parity protection}. The \textbf{main difference} between them is their \textbf{performance} and \textbf{load balancing capabilities}, not their reliability! The extended version:\index{RAID 4 - MTTF}\index{RAID 5 - MTTF}
        \begin{equation}\label{eq: RAID 4 - 5 - MTTF}
            \texttt{MTTF}_{\texttt{RAID 4}} = \texttt{MTTF}_{\texttt{RAID 5}} = \dfrac{\texttt{MTTF}_{\text{disk}}^{2}}{N \left(N-1\right) \cdot \texttt{MTTR}}
        \end{equation}


        \item \definitionWithSpecificIndex{\texttt{\underline{RAID 6}}}{MTTF of \texttt{RAID 6} $\texttt{MTTF}_{\text{RAID 6}}$}{} (page \hqpageref{RAID 6}) is an extension of RAID 5 with \textbf{double parity}. It uses two independent parity blocks per stripe (commonly $P$ and $Q$). These allow the array to tolerate \textbf{two simultaneous disk failures}. The system only fails if \textbf{a third disk fails} during the time in which the two failed disks are being rebuilt.

        \begin{equation}
            \texttt{MTTF}_{\texttt{RAID 6}} = \texttt{MTTF}_{\text{array}} \cdot \dfrac{1}{
                \text{\% 2nd failure} \cdot \text{\% 3rd failure}
            }
        \end{equation}
        Where:
        \begin{itemize}
            \item $\texttt{MTTF}_{\text{array}} = \dfrac{\texttt{MTTF}_{\text{single disk}}}{\texttt{Number of Disks}}$
            \item $\texttt{MTTR}$ is the mean time to repair (the vulnerable window).
            \item The probability of the second failure during MTTR:
            \begin{equation}
                \text{\% 2nd failure} = \dfrac{N-1}{\texttt{MTTF}_{\text{single disk}}} \cdot \texttt{MTTR}
            \end{equation}
            It is the risk of a second failure while the first is being repaired.
            \item The probability of the third failure during MTTR:
            \begin{equation}
                \text{\% 3nd failure} = \dfrac{N-2}{\texttt{MTTF}_{\text{single disk}}} \cdot \dfrac{\texttt{MTTR}}{2}
            \end{equation}
            It is the risk of a third failure during the overlapping time when \textbf{both first and second disks} are being rebuilt. The \texttt{MTTR} is divided by two because it \textbf{assumes} that both \textbf{disks are under recovery simultaneously} (\textbf{average time}).
        \end{itemize}
        Unlike RAID 5, RAID 6 can still recover after a second disk fails. But during this dual-failure state, the system becomes vulnerable to a \textbf{third-failure}. That third failure is what RAID 6 models as catastrophic. The extended version:\index{RAID 6 - MTTF}
        \begin{equation}\label{eq: RAID 6 - MTTF}
            \texttt{MTTF}_{\texttt{RAID 6}} = \dfrac{2}{N} \cdot \dfrac{\texttt{MTTF}_{\text{disk}}^{3}}{\left(N-1\right) \cdot \left(N-2\right) \cdot \texttt{MTTR}^{2}}
        \end{equation}
    \end{itemize}
\end{itemize}

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        \textbf{Metric}                                                                                                                                                         & \textbf{Tolerates} \\
        \midrule
        $\texttt{MTTF}_{\texttt{RAID 0}} = \dfrac{\texttt{MTTF}_{\text{single disk}}}{N}$                                                                                        & 0 disk failures \\ \\
        $\texttt{MTTF}_{\texttt{RAID 1+0}} = \dfrac{\left(\texttt{MTTF}_{\text{single disk}}\right)^{2}}{\left(N \times \texttt{MTTR}\right)}$                                   & Multiple failures (1 per mirror) \\ \\
        $\texttt{MTTF}_{\texttt{RAID 0+1}} = \dfrac{\left(\texttt{MTTF}_{\text{single disk}}\right)^{2}}{\left(N \times G \times \texttt{MTTR}\right)}$                          & 1 full group \\ \\
        $\texttt{MTTF}_{\texttt{RAID 4}} = \dfrac{\left(\texttt{MTTF}_{\text{single disk}}\right)^{2}}{\left(N \times N-1 \times \texttt{MTTR}\right)}$                          & 1 disk \\ \\
        $\texttt{MTTF}_{\texttt{RAID 5}} = \dfrac{\left(\texttt{MTTF}_{\text{single disk}}\right)^{2}}{\left(N \times N-1 \times \texttt{MTTR}\right)}$                          & 1 disk \\ \\
        $\texttt{MTTF}_{\texttt{RAID 6}} = \dfrac{2 \times \left(\texttt{MTTF}_{\text{single disk}}\right)^{3}}{\left(N \times N-1 \times N-2 \times \texttt{MTTR}^{2}\right)}$  & 2 disks \\
        \bottomrule
    \end{tabular}
    \caption{MTTF summary RAID levels.}
\end{table}

\begin{itemize}
    \item \texttt{RAID 0}: No redundancy. Failure of any disk leads to total data loss.
    \item \texttt{RAID 1+0}: High reliability; localized failure risk; faster rebuilds.
    \item \texttt{RAID 0+1}: Less reliable than \texttt{1+0}; entire group fails if any disk in it fails.
    \item \texttt{RAID 4/5}: Identical \texttt{MTTF}; \texttt{RAID 5} distributes parity to avoid bottlenecks of \texttt{RAID 4}.
    \item \texttt{RAID 6}: Extremely high reliability; more complex failure modeling.
\end{itemize}
Last remarks:
\begin{itemize}
    \item The increase in \textbf{exponential of} $\texttt{MTTF}_{\text{disk}}$ reflects higher redundancy\break (\texttt{RAID 6} uses $\texttt{MTTF}^{3}$).
    \item More complex RAID levels (\texttt{1+0} or \texttt{6}) handle multiple simultaneous failures much more gracefully.
    \item \textbf{Mean Time To Repair (MTTR)} plays a critical role: the long the rebuild takes, the more likely a second or third failure occurs.
\end{itemize}