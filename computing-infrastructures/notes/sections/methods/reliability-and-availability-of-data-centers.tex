\section{Methods}

\subsection{Reliability and availability of data centers}

\subsubsection{Introduction}\label{subsubsection: introduction - dependability}

\definition{Dependability} is a \textbf{comprehensive measure} of how much we can \textbf{trust a system} to perform its expected function correctly and consistently over time.

\highspace
Dependability is not a single property but a \textbf{composite of multiple interrelated concerns}, some more related to system correctness (reliability, availability), others more to protection (safety, security). In large-scale systems like datacenters, \textbf{dependability is not optimal}, it is integral to Service Level Agreement (SLAs)\footnote{%
    A \definition{Service Level Agreement (SLA)} is a contract between a service provider and a customer that defines the expected level of service. It typically includes metrics like availability, performance, and response times, along with remedies if the provider fails to meet those standards.
}, customer trust, and legal compliance (e.g., data protection laws)\footnote{%
    \textbf{Legal compliance} in data protection laws ensures that organizations handle personal data responsibly and in accordance with regulations. In the EU, the \emph{General Data Protection Regulation (GDPR)} is the primary framework, setting strict rules on data collection, processing, and storage. Globally, different countries have their own laws, such as the \emph{California Consumer Privacy Act (CCPA)} in the U.S., and similar regulations emerging worldwide.
}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Core Attributes of Dependability}}
\end{flushleft}
\begin{table}[!htp]
    \centering
    \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}
        \begin{tabular}{@{} l p{13em} p{17em} @{}}
            \toprule
            \textbf{Attribute} & \textbf{Meaning} & \textbf{Analogy / Data Center View} \\
            \midrule
            \textbf{Reliability}        & Continuity of correct service         & A web server not crashing or corrupting data for long periods. \\ [.5em]
            \textbf{Availability}       & Readiness for correct service         & Can the system serve requests \emph{right now}? \\ [.5em]
            \textbf{Maintainability}    & Ability for easy maintenance          & Hot-swappable disks, modular servers. \\ [.5em]
            \textbf{Safety}             & Absence of catastrophic consequences  & E.g., fail-safes in a power system to avoid fire/electrical hazard. \\ [.5em]
            \textbf{Security}           & Confidentiality and integrity of data & Access control, data encryption, intrusion prevention. \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\end{table}

\begin{examplebox}[: Cloud Service Provider]
    Image Google Cloud or AWS:
    \begin{itemize}
        \item \textbf{Reliability}: Our VMs shouldn't crash without reason.
        \item \textbf{Availability}: We must access our cloud database anytime.
        \item \textbf{Maintainability}: They update the hypervisor with zero downtime.
        \item \textbf{Safety}: If a cooling system fails, automatic shutdown prevents hardware fire.
        \item \textbf{Security}: Data encryption at rest and in transit, isolation across tenants.
    \end{itemize}
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Do we really need to care about Dependability?}}
\end{flushleft}
First, we'll explore intuition. Even when a system is \emph{well-designed}, passes \emph{functional verification}, meets \emph{performance and energy constraints}, it can still \textbf{fail}! Why? Because real-world conditions and \textbf{non-functional factors} can break even perfectly specified systems.

\highspace
So dependability is \underline{necessary} because \hl{\textbf{failures are inevitable}}. They stem from \textbf{multiple layers of system stack}, including:

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | l @{}}
        \toprule
        \textbf{Source} & \textbf{Example} \\
        \midrule
        \textbf{Hardware aging}             & Transistor degradation, wear-out mechanisms.  \\ [.5em]
        \textbf{Manufacturing defects}      & Process variation, cosmic rays.               \\ [.5em]
        \textbf{Design flaws}               & Logic errors, incorrect specifications.       \\ [.5em]
        \textbf{Software bugs}              & Memory leaks, race conditions, kernel panics. \\ [.5em]
        \textbf{System misconfiguration}    & Wrong firewall rule, VM setting.              \\ [.5em]
        \textbf{External disturbances}      & Electromagnetic interference, overheating.    \\ [.5em]
        \textbf{Human errors}               & Mistakes by admins, developers.               \\ [.5em]
        \textbf{Malicious attacks}          & DoS, ransomware, data exfiltration.           \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
Even a tiny misbehavior in one layer can cascade through the entire system.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Consequences of Undependability}}
\end{flushleft}
Let's categorize \textbf{why lack of dependability is dangerous}, especially in data centers:
\begin{enumerate}
    \item \important{High Economic Cost}: Downtime may cause lost transactions, SLA violations, brand damage.
    \item \important{Information Loss}: Permanent deletion or corruption of critical data (e.g., backups overwritten, logs lost).
    \item \important{Cascade Failures}: One component failure affects dependent services (e.g., cloud outages).
    \item \important{User Distrust}: If users don't trust the service, they abandon it.
    \item \important{Security \& Safety Hazards}: leaked credentials, incorrect data processing and in physical systems, risk to human life (think healthcare or industrial systems).
\end{enumerate}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{When to Think About Dependability?}}
\end{flushleft}
Short answer: \textbf{Always}. Dependability is \textbf{not an optional}, it must be \textbf{considered} throughout the \textbf{entire system lifecycle}: \emph{design-time}, \emph{deployment}, \emph{run-time}, and even \emph{decommissioning}. We think about dependability:
\begin{itemize}
    \item At \important{Design-Time}, the main goal is to \textbf{prevent as many failures as possible before the system is deployed}. This is when we:
    \begin{itemize}
        \item \textbf{Analyze}: Model the system and predict how it behaves under faults.
        \item \textbf{Measure}: Evaluate expected MTTF, availability, error rates.
        \item \textbf{Optimize}: Adjust design to improve weak points (e.g., redundancy, replication).
        \item \textbf{Verify}: Perform fault-injection, static analysis, formal verification.
    \end{itemize}
    \item At \important{Run-Time}, no matter how good our design is, \hl{failures will still happen} in the field. So \textbf{we must}:
    \begin{itemize}
        \item \hl{Detect}: \textbf{Monitor system} state to spot abnormal behavior.
        \item \hl{Diagnose}: \textbf{Understand} root \textbf{causes} (e.g., logs, telemetry).
        \item \hl{React}: \textbf{Automatically correct} or \textbf{isolate} the problem.
    \end{itemize}
    For example rebooting a crashed VM, replacing a failed disk in a RAID array or routing traffic away from an unhealthy load balancer.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{balance-scale} \textbf{Why Distinguish Development-Time vs Operation-Time Failures?}}
\end{flushleft}
These two types of failures have completely different roots, so we must use different strategies to handle them. 
\begin{itemize}
    \item \definition{Development-Time Failures}. These are \textbf{errors that are introduced during system creation}, in the design, coding or configuration phase.

    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{Examples}}
        \begin{itemize}
            \item A bug in the file system code that causes data corruption.
            \item A wrong assumption in system specifications.
            \item An incorrect server configuration (e.g., firewall blocking valid ports).
            \item A logic error in redundancy management (e.g., mirrored disk not syncing).
        \end{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Solutions}}
        \begin{itemize}
            \item These \textbf{should be avoided} before deployment.
            \item They're addressed using:
            \begin{itemize}
                \item \textbf{Testing} (unit, integration, fault-injection)
                \item \textbf{Verification} (formal methods, code reviews)
                \item \textbf{Validation} (does it meet real-world expectations?)
            \end{itemize}
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{exclamation-triangle}}] \hl{If a system fails} due to a development-time error, it means there was something \textbf{wrong in how it was built}.
    \end{itemize}
    

    \newpage


    \item \definition{Operation-Time Failures}. These are \textbf{failures that happen while the system is running}, even if the system was perfectly designed.
    
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{tools}}] \textcolor{Green3}{\textbf{Examples}}
        \begin{itemize}
            \item A hard disk physically fails after 3 years.
            \item A memory bit flips due to cosmic radiation (soft error).
            \item The power supply overheats due to dust.
            \item A data center operator accidentally disconnects a cable.
            \item A hacker exploits an unknown vulnerability.
        \end{itemize}
        \item[\textcolor{Green3}{\faIcon{check}}] \textcolor{Green3}{\textbf{Solutions}}
        \begin{itemize}
            \item These \textbf{cannot be entirely avoided}, because hardware ages, users make mistakes, environments are unpredictable.
            \item They \textbf{must be handled at runtime} with:
            \begin{itemize}
                \item \textbf{Monitoring}
                \item \textbf{Redundancy / Fault tolerance}
                \item \textbf{Error correction \& self-repair}
            \end{itemize}
        \end{itemize}
        \item[\textcolor{Red2}{\faIcon{exclamation-triangle}}] \hl{If a system fails} at runtime, it's not necessarily badly designed, \textbf{reality is imperfect}, and we need to plan for that.
    \end{itemize}
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Where to Apply Dependability?}}
\end{flushleft}
Originally, dependability was essential only in a few extreme environments, systems where a failure could endanger lives, waste millions or billions, be unacceptable by definition (space, nuclear, aviation). The \hl{types of Critical Systems}:
\begin{itemize}
    \item \definition{Mission-Critical Systems}. The \textbf{mission} is \textbf{not completed if a failure occurs}. Some examples are satellites (loss $=$ mission failure), unmanned vehicles (e.g., drones, crash $=$ loss of platform and data). \textbf{Dependability ensures the system fulfills its intended goal}.

    \item \definition{Safety-Critical Systems}. If a failure occurs, human life is at risk. Some examples are aircraft control (system crash $=$ airplane crash), nuclear plant controls (meltdown). Here, the \textbf{cost of failure is catastrophic}.
\end{itemize}
\textbf{Dependability today is no longer limited} to space and nuclear systems. Now it's \textbf{central to computing infrastructures}, including data centers, cloud platforms, edge devices, etc. Why? Because a failure today in these systems can disrupt global services (e.g., Gmail outage), leak personal data, etc.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How to Provide Dependability?}}
\end{flushleft}
Provide Dependability means designing our system (hardware and software) in a way that it \textbf{keeps working}, even when \textbf{things go wrong}, or at least, it fails in a \textbf{safe and recoverable way}. There are \textbf{two main strategies} to achieve this:
\begin{enumerate}
    \item \definition{Failure Avoidance} (try not to fail). This approach is like saying: ``\emph{let me build the system so well that it doesn't break}''. For example, use very reliable hardware, run a lot of testing before deploying our software, do quality control in factories to catch early hardware faults, and use formally verified algorithms in critical parts.

    This approach is \textbf{good}, \textbf{but expensive}, and we \textbf{can't predict everything}.

    \begin{examplebox}[: Failure Avoidance Analogy]
        Like buying a car and checking \emph{everything} before leaving the factory: brakes, oil, tires, engine. So it's unlikely to break.
    \end{examplebox}


    \item \definition{Failure Tolerance} (prepare for when it does fail). This approach accepts the fact that something \textbf{will} go wrong. So let us prepare the system to \textbf{detect}, \textbf{contain}, and \textbf{recover} from the problem.

    For example, use RAID for disks, use ECC memory (it can correct small memory errors automatically), monitor servers, and in the cloud, if one data center goes down, reroute to another.

    This is \textbf{essential in data centers}, because with thousands of machines, failures are guaranteed.

    \begin{examplebox}[: Failure Tolerance Analogy]
        Like having a spare tire in our car, a seatbelt, and road assistance. We're ready when things go wrong.
    \end{examplebox}
\end{enumerate}
\textcolor{Green3}{\faIcon{question-circle} \textbf{Where Can We Add Dependability?}} We can ``\emph{add dependability}'' at \textbf{different layers}:
\begin{itemize}
    \item \important{Hardware Level}. Make components themselves very reliable.
    \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
        \item Use robust materials.
        \item Use parts that are known to last a long time.
        \item[\textcolor{Red2}{\faIcon{times}}] Expensive, and maybe slow (old, stable tech).
    \end{itemize}

    \item \important{Architecture/System Design Level}. Make the \textbf{structure of the system} fault-tolerant.
    \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
        \item Duplicate components (e.g., 2 power supplies, RAID).
        \item Create \textbf{clusters}: if one server fails, others can take over
        \item[\textcolor{Red2}{\faIcon{times}}] More machines, more space, higher cost.
    \end{itemize}

    \item \important{Software Level}. Add dependability using \textbf{code}.
    \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
        \item Detect problems: logs, alerts, health checks.
        \item Retry or restart automatically.
        \item Recover from failures (e.g., checkpoint/restart in HPC).
        \item Cheaper and flexible.
        \item[\textcolor{Red2}{\faIcon{times}}] Needs smart design and might slow down performance.
    \end{itemize}
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{There ain't no such thing as a free lunch (TANSTAAFL)}}
\end{flushleft}
It is impossible to get something for nothing. We always \textbf{pay} for dependability:

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l l @{}}
        \toprule
        We Pay In... & Why? \\
        \midrule
        \textbf{Money}          & Redundant servers, better hardware. \\ [.3em]
        \textbf{Time}           & Development of testing and recovery logic. \\ [.3em]
        \textbf{Performance}    & Extra steps for safety and monitoring. \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\subsubsection{Reliability and Availability}

Dependability contains the properties of reliability and availability (see page \pageref{subsubsection: introduction - dependability}).

\highspace
\begin{definitionbox}[: Reliability]\index{Reliability}
    The \textbf{ability} of a system or component \textbf{to perform its required functions under stated conditions for a specified period of time}.
\end{definitionbox}

\highspace
We can also calculate the \textbf{probability} that the \textbf{system will operate correctly} in a specified operating environment \underline{until} time $t$:
\begin{equation}\label{eq: reliability probability}
    R\left(t\right) = P\left(\text{not failed during } \left[0,t\right]\right)
\end{equation}
(assuming it was operating at time $t=0$). Note that the time $t$ is essential because it is often \textbf{used to characterize systems in which even small periods of incorrect behaviour are unacceptable} (e.g. impossibility to repair). For example, if a system needs to work for slots of ten hours at a time, then ten hours is the reliability target.

\highspace
As a consequence, the \definition{unreliability $Q\left(t\right)$} can be calculated as follows:
\begin{equation}
    Q\left(t\right) = 1 - R\left(t\right)
\end{equation}
The reliability probability is a \textbf{non-increasing function} ranging from $1$ to $0$ over $\left.\left[0, +\infty\right.\right)$.
\begin{equation}
    \begin{array}{rcl}
        R\left(0\right) &=& 1 \\ [.5em]
        \lim\limits_{t \rightarrow +\infty} R\left(t\right) &=& 0 \\ [.5em]
        f\left(x\right) &=& -\dfrac{\mathrm{d}R\left(t\right)}{\mathrm{d}t}
    \end{array}
\end{equation}
We can observe that the probability of the reliability at the time zero is equal to one because we assume it was operating at time zero. Furthermore, the reliability probability function goes to zero when the time goes to infinity.

\highspace
\begin{definitionbox}[: Availability]\index{Availability}
    The degree to which a system or component is \textbf{operational and accessible when required for use}. It can be calculated as follows:
    \begin{equation}
        \text{Availability } = \dfrac{\text{Uptime}}{\left(\text{Uptime } + \text{ Downtime}\right)}
    \end{equation}
\end{definitionbox}

\highspace
The \textbf{main difference} between reliability and availability is that \textbf{reliability does not break down}, and \textbf{availability works when needed}, even if it breaks down.

\highspace
Finally, we calculate the \textbf{probability that the system will be operational at time} $t$ as follows:
\begin{equation}
    A\left(t\right) = P\left(\text{not failed at time } t\right)
\end{equation}
It is ready for service and admits the possibility of brief outages. Finally, of course, the \definition{unavailability} is:
\begin{equation}
    \text{unavailability } = 1-A\left(t\right)
\end{equation}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{What is the relationship between reliability and availability?}}
\end{flushleft}
The \textbf{relationship with the reliability} is that:
\begin{itemize}
    \item When the \textbf{system is not repairable}, the availability and reliability are the same:
    \begin{equation}
        A\left(t\right) = R\left(t\right)
    \end{equation}

    \item In general, the \textbf{reparable systems} have
    \begin{equation}
        A\left(t\right) \ge R\left(t\right)
    \end{equation}
\end{itemize}
However, the relationship is more robust because if a \textbf{system is unavailable}, \textbf{it does not deliver the specified system services}. However, it is possible to have \textbf{systems with low reliability that must be available}. Then, the system failures can be repaired quickly and do not damage data, so the low reliability may not be a problem. The opposite is generally more complex.

\newpage

\begin{flushleft}
    \textcolor{Red2}{\faIcon{chart-bar} \textbf{Metrics}}
\end{flushleft}
Some metrics exist for reliability and availability.
\begin{itemize}
    \item The \definition{Mean Time To Failure (MTTF)} is the mean time before any failure will occur. Moreover, it is calculated as the \textbf{integral of the reliability probability} (eq. \ref{eq: reliability probability}, page \pageref{eq: reliability probability}):
    \begin{equation}
        \texttt{MTTF} = \displaystyle\int_{0}^{\infty} R\left(t\right) \:\mathrm{d}t
    \end{equation}
    
    \item The \definition{Mean Time Between Failures (MTBF)} is the \textbf{mean time between two failures}. It is the relationship between the total operating time and the number of failures.
    \begin{equation}
        \texttt{MTBF} = \dfrac{\text{total operating time}}{\text{number of failures}}
    \end{equation}
    %\newpage
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=\textwidth]{img/reliability-and-availability-1.png}
    \end{figure}
    
    \item The \definition{Failures In Time (FIT)} is \textbf{another way of reporting \texttt{MTBF}}. It is the number of \textbf{expected failures per one billion hours} ($10^9$) of operation for a device. Then, the \texttt{MTBF} in hours is:
    \begin{equation}
        \texttt{MTBF} = \dfrac{10^{9}}{\texttt{FIT}}
    \end{equation}
    
    \item The \definition{Failure Rate $\lambda$} is the relationship between the number of failures and the total operating time:
    \begin{equation}\label{eq: failure rate lambda}
        \text{Failure Rate }\lambda = \dfrac{\text{number of failures}}{\text{total operating time}}
    \end{equation}
    If we observe closely, it equals $MTBF^{-1}$, then:
    \begin{equation}
        \texttt{MTBF} = \dfrac{1}{\lambda}
    \end{equation}
\end{itemize}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How to compute reliability? The Empirical Evaluation}}
\end{flushleft}
In general, Empirical Evaluation is an evaluation method in which results are derived from observation or experiment rather than theory.

\highspace
Regarding reliability, let us consider:
\begin{itemize}
    \item $n_{0}$ independent and statistically identical elements deployed at time $t = 0$ in identical conditions $n\left(0\right) = n_{0}$;

    \item At time $t$, the $n\left(t\right)$ elements do not fail.

    \item Furthermore, $t_1$, $t_2$, $\dots$, $t_{n_{0}}$ are the times of failure of the $n_0$ elements. Note that the times to failure are independent occurrences of the random quantity $T$.
\end{itemize}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/reliability-and-availability-2.png}
\end{figure}

\noindent
The function:
\begin{equation}
    \dfrac{n\left(t\right)}{n_{0}}
\end{equation}
Is the \definition{empirical function of reliability} that, as $n_{0} \rightarrow \infty$, \textbf{converges to the value}: 
\begin{equation}
    \dfrac{n\left(t\right)}{n_0} \rightarrow R\left(t\right)
\end{equation}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Ok, but what do we do with the reliability probability?}}
\end{flushleft}
Well, the exploitation of the reliability probability information is \textbf{used to compute}, for a complex system, its reliability in time and the \textbf{expected lifetime}. Note that the computation of the overall reliability starts from the component one.

\newpage

\begin{flushleft}
    \textcolor{Red2}{\faIcon{bookmark} \textbf{Reliability terminology}}
\end{flushleft}
The \textbf{Constant Failure rate of the reliability} is:
\begin{equation}
    \begin{array}{rcl}
        R\left(t\right) &=& e^{-\lambda \: t} \\ [.5em]
        \texttt{MTTF} &=& \displaystyle\int_{0}^{\infty} R\left(t\right) \:\mathrm{d}t = \dfrac{1}{\lambda}
    \end{array}
\end{equation}
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.5\textwidth]{img/reliability-and-availability-4.png}
\end{figure}

\noindent
Then, to refer to it, we use the correct terminology.
\begin{itemize}
    \item The \underline{\textbf{Fault}} is a \textbf{defect within the system}.
    \item The \underline{\textbf{Error}} is a \textbf{deviation from the required operation of the system} or subsystem.
    \item \underline{\textbf{Failure}} is when the \textbf{system fails to perform its required function}.
\end{itemize}
\begin{figure}[!htp]
    \centering
    \includegraphics[width=.8\textwidth]{img/reliability-and-availability-3.png}
\end{figure}

\begin{examplebox}
    A flying drone with an automatic radar-guided landing system. An example of:
    \begin{itemize}
        \item Fault: the electromagnetic disturbances interfere with a radar measurement.
        \item Error: the radar-guided landing system calculates a wrong trajectory.
        \item Failure: the drone crashes to the ground.
    \end{itemize}
\end{examplebox}

\begin{examplebox}[: not always the fault-error-failure chain closes]
    A tele-surgery system. An example of:
    \begin{itemize}
        \item Fault: the radioactive ions change some memory cells' value (bitflip).
        \item Error: some frames of the video stream are corrupted.
        \item Failure: the surgeon kills the patient.
    \end{itemize}
    However, not always the fault-error-failure chain closes:
    \begin{itemize}
        \item Fault: the radioactive ions make some memory cells change value (bitflip), but the corrupted memory does not involve the video stream.
        \item Error: no frames are corrupted.
        \item Failure: the surgeon carries out the procedure.
    \end{itemize}
    As we can see, there is no activated fault! With the same logic, a flying drone with automatic radar-guided landing:
    \begin{itemize}
        \item Fault: electromagnetic disturbances interfere with a radar measurement.
        \item Error: the radar-guided landing system calculates a wrong trajectory, but then, based on subsequent correct radar measurements, it can recover the right trajectory.
        \item Failure: the drone safely lands.
    \end{itemize}
    Here, there is no propagated (or absorbed error).
\end{examplebox}

\newpage

\subsubsection{Reliability Block Diagrams}

The \definition{Reliability Block Diagram (RBD)}\footnote{The RBD argument was already treated in the \href{https://github.com/PoliMI-HPC-E-notes-projects-AndreVale69/HPC-E-PoliMI-university-notes/tree/main/software-engineering-for-hpc}{Software Engineering for HPC notes}.} is an \textbf{inductive model in which a system is divided into blocks representing distinct elements}, such as components or subsystems. \textbf{Each element in the RBD has its reliability} (previously calculated or modelled). All blocks are then combined to model all the possible success paths.

\highspace
The diagram follows strict rules. To represent:
\begin{itemize}
    \item \definition{Components in series}: the \textbf{system failure} is \textbf{determined by the failure of the \emph{first} component}.
    \begin{equation}
        R_{s}\left(t\right) = \prod_{i=1}^{n} R_{i}\left(t\right)
    \end{equation}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.8\textwidth]{img/RBD-1.pdf}
    \end{figure}

    For example, in the previous illustration, reliability is calculated as:
    \begin{equation*}
        R_{s}\left(t\right) = R_{C1}\left(t\right) \times R_{C2}\left(t\right)
    \end{equation*}

    In general, if the system $S$ consists of \textbf{components with a reliability with an exponential distribution} (the only case considered in this course), the reliability can be calculated as:
    \begin{equation}
        R_{s}\left(t\right) = e^{-\lambda_{s} \: t}
    \end{equation}
    Where $t$ is the time and $\lambda_{s}$ is the \definition{Failure in time}:
    \begin{equation}
        \lambda_{s} = \displaystyle\sum_{i=1}^{n}\lambda_{i}
    \end{equation}
    Note that the $\lambda_{i}$ value is explained on page \pageref{eq: failure rate lambda} (eq. \ref{eq: failure rate lambda}). The Mean Time To Failure of a system is $S$:
    \begin{equation}
        \texttt{MTTF}_{s} = \dfrac{1}{\lambda_{s}} = \dfrac{1}{\displaystyle\sum_{i=1}^{n}\lambda_{i}} = \dfrac{1}{
            \displaystyle\sum_{i=1}^{n} \dfrac{1}{\texttt{MTTF}_{i}}
        }
    \end{equation}

    If \textbf{all components are identical}:
    \begin{equation}
        R_{s}\left(t\right) = e^{- n \lambda t} = \exp\left(- \dfrac{n t}{\texttt{MTTF}_{1}}\right)
    \end{equation}
    \begin{equation}
        \texttt{MTTF}_{s} = \dfrac{\texttt{MTTF}_{1}}{n}
    \end{equation}

    Finally, the \textbf{availability} is:
    \begin{equation}
        A_{s} = \prod_{i=1}^{n} \dfrac{\texttt{MTTF}_{i}}{\texttt{MTTF}_{i} + \texttt{MTTR}_{i}}
    \end{equation}
    Where \texttt{MTTR} is the \definition{Mean Time To Repair (MTTR)}.

    If \textbf{all components are identical}:
    \begin{equation}
        A_{s}\left(t\right) = A_{1}\left(t\right)^{n}
    \end{equation}
    \begin{equation}
        A = \left(\dfrac{
            \texttt{MTTF}_{1}
        }{
            \texttt{MTTF}_{1} + \texttt{MTTR}_{1}
        }\right)^{n}
    \end{equation}


    \item \definition{Components in parallel}: the \textbf{system fails when the \emph{last} component fails}.
    \begin{equation}
        R_{s}\left(t\right) = 1 - \prod_{i=1}^{n} \left(1 - R_{i}\left(t\right)\right)
    \end{equation}
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.8\textwidth]{img/RBD-2.pdf}
    \end{figure}

    For example, in the previous illustration, reliability is calculated as:
    \begin{equation*}
        R_{s}\left(t\right) = 1 - \left[\left(1-R_{C1}\left(t\right)\right) \times \left(1-R_{C2}\left(t\right)\right)\right]
    \end{equation*}

    Consider a system $P$ composed of $n$ components, the \textbf{reliability} is:
    \begin{equation}
        R_{p}\left(t\right) = 1 - \prod_{i=1}^{n} \left(1 - R_{i}\left(t\right)\right)
    \end{equation}
    And the \textbf{availability} is:
    \begin{equation}
        \begin{array}{rcl}
            A_{p}\left(t\right) &=& 1 - \displaystyle\prod_{i=1}^{n} \left(1 - A_{i}\left(t\right)\right) \\ [1em]
            &=& 1 - \displaystyle\prod_{i=1}^{n} \dfrac{\texttt{MTTR}_{i}}{\texttt{MTTF}_{i} + \texttt{MTTR}_{i}}
        \end{array}
    \end{equation}
\end{itemize}
The difference between these two representations is that if a \textbf{component in the series is unhealthy}, \textbf{the whole system is unhealthy}. Instead, in the \textbf{parallel architecture}, the \textbf{system can work properly if a component is unhealthy}.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{bookmark} \textbf{A quick recap}}
\end{flushleft}
\begin{itemize}
    \item \underline{\textbf{Series}}.
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.6\textwidth]{img/RBD-1.pdf}
    \end{figure}

    Reliability:
    \begin{equation*}
        R_{s} = \displaystyle\prod_{i}^{n} R_{i} \Longrightarrow R_{s} = R_{C1} \cdot R_{C2}
    \end{equation*}
    

    \item \underline{\textbf{Parallel}}.
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.6\textwidth]{img/RBD-2.pdf}
    \end{figure}

    Reliability:
    \begin{equation*}
        R_{s} = 1 - \displaystyle\prod_{i}^{n} \left(1 - R_{i}\right) \Longrightarrow R_{s} = 1 - \left[\left(1 - R_{C1}\right) \cdot \left(1 - R_{C2}\right)\right]
    \end{equation*}


    \item \underline{\textbf{Series-Parallel}} (component redundancy).
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=\textwidth]{img/RBD-3.pdf}
    \end{figure}

    Reliability:
    \begin{equation*}
        R_{s} = \left\{1 - \left[\left(1 - R_{C1}\right) \cdot \left(1 - R_{C2}\right)\right]\right\} \cdot \left\{1 - \left[\left(1 - R_{C3}\right) \cdot \left(1 - R_{C4}\right)\right]\right\}
    \end{equation*}


    \item \underline{\textbf{Parallel-Series}} (system redundancy).
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=\textwidth]{img/RBD-4.pdf}
    \end{figure}

    Reliability:
    \begin{equation*}
        R_{s} = 1 - \left[\left(1 - R_{C1} \cdot R_{C3}\right) \cdot \left(1 - R_{C2} \cdot R_{C4}\right)\right]
    \end{equation*}
\end{itemize}

\newpage

\begin{examplebox}[: calculate the reliability of the system]
    \begin{flushleft}
        \textcolor{Green3}{\faIcon{question-circle} \textbf{Question}}
    \end{flushleft}
    What is the Reliability of the entire system knowing the reliability of each component?
    \begin{center}
        \includegraphics[width=\textwidth]{img/RBD-5.pdf}
    \end{center}
    \begin{multicols}{3}
        \begin{itemize}
            \item $R_{C1} = 0.95$
            \item $R_{C2} = 0.97$
            \item $R_{C3} = 0.99$
            \item $R_{C4} = 0.99$
            \item $R_{C5} = 0.92$
            \item $R_{C6} = 0.92$
        \end{itemize}
    \end{multicols}
    \begin{flushleft}
        \textcolor{Green3}{\faIcon{check} \textbf{Solution}}
    \end{flushleft}
    \begin{enumerate}
        \item Consider components $C1$ and $C2$. The reliability, which we will call $R_{G}$, is then calculated as a \emph{series}:
        \begin{equation*}
            R_{G} = R_{C1} \cdot R_{C2} = 0.95 \cdot 0.97 = 0.9215
        \end{equation*}

        \item Consider components $C3$ and $C4$. The reliability, which we will call $R_{H}$, is then calculated as a \emph{parallel}:
        \begin{equation*}
            \begin{array}{rcl}
                R_{H} &=& 1 - \left[\left(1-R_{C3}\right) \cdot \left(1-R_{C4}\right)\right] \\ [.5em]
                &=& 1 - \left[\left(1 - 0.99\right) \cdot \left(1 - 0.99\right)\right] \\ [.5em]
                &=& 1 - 0.0001 \\ [.5em]
                &=& 0.9999
            \end{array}
        \end{equation*}

        \item Consider components $C5$ and $C6$. The reliability, which we will call $R_{I}$, is then calculated as in the previous step:
        \begin{equation*}
            \begin{array}{rcl}
                R_{I} &=& 1 - \left[\left(1-R_{C5}\right) \cdot \left(1-R_{C6}\right)\right] \\ [.5em]
                &=& 1 - \left[\left(1 - 0.92\right) \cdot \left(1 - 0.92\right)\right] \\ [.5em]
                &=& 1 - 0.0064 \\ [.5em]
                &=& 0.9936
            \end{array}
        \end{equation*}

        \item Finally, we calculate the reliability of the system by multiplying each calculated component reliability:
        \begin{equation*}
            \begin{array}{rcl}
                R_{s} &=& R_{G} \cdot R_{H} \cdot R_{I} \\ [.5em]
                &=& 0.9215 \cdot 0.9999 \cdot 0.9936 \\ [.5em]
                &=& 0.91551083976 \approx 0.9155
            \end{array}
        \end{equation*}
    \end{enumerate}
\end{examplebox}

\begin{examplebox}[: calculate reliability without numbers]
    \begin{flushleft}
        \textcolor{Green3}{\faIcon{question-circle} \textbf{Question}}
    \end{flushleft}
    The system consists of 2 control blocks and 3 voice channels. The system is up when at least 1 control channel and at least 1 voice channel are up.
    \begin{center}
        \includegraphics[width=\textwidth]{img/RBD-6.pdf}
    \end{center}
    \begin{flushleft}
        \textcolor{Green3}{\faIcon{check} \textbf{Solution}}
    \end{flushleft}
    Reliability can be calculated in parallel, as it takes almost a component to work properly. Each control channel has reliability $R_{c}$ and each voice channel has reliability $R_{v}$:
    \begin{equation*}
        R = \left[1 - \left(1 - R_{c}\right)^{2}\right] \cdot \left[1 - \left(1 - R_{v}\right)^{3}\right]
    \end{equation*}
\end{examplebox}

\newpage

\paragraph{R out of N redundancy (RooN)}

An \definition{RooN ($r$ out of $n$)} redundancy system \textbf{contains} both the \textbf{series system model and the parallel system model} as special cases. The system has $n$ components that operate or fail independently of one another and as long as at least $r$ of these components (any $r$) survive, the system survives.\cite{nist8184Model}

\highspace
\textbf{System failure occurs when} the $\left(n - r + 1\right)$-th component failure occurs.\cite{nist8184Model}

\highspace
But note an interesting observation:\cite{nist8184Model}
\begin{itemize}
    \item When $r=n$, the $r$ out of $n$ model reduces to the \textbf{series} model.
    \item When $r=1$, the $r$ out of $n$ model becomes the \textbf{parallel} model.
\end{itemize}

\highspace
In simple terms, \texttt{RooN} is a system made up of $n$ identical replicas, where at least $r$ replicas have to work well for the whole system to work well.

\highspace
The reliability formula for the RooN system is:
\begin{equation}
	R_{s}\left(t\right) = RV \displaystyle\sum_{i=r}^{n} R_{c}^{i} \left(1 - R_{c}\right)^{n-i} \: \dfrac{n!}{i!\left(n-i\right)!}
\end{equation}
The last part of the formula can be replaced by the binomial coefficient:
\begin{equation*}
	\dfrac{n!}{i!\left(n-i\right)!} = \binom{n}{i}
\end{equation*}
The components are:
\begin{multicols}{2}
	\begin{itemize}
		\item $R_{s}$: System Reliability
		
		\item $R_{c}$: Component Reliability
		
		\item $R_{v}$: Voter Reliability
		
		\item $n$: number of components
		
		\item $r$: minimum number of components which must survive
	\end{itemize}
\end{multicols}
\begin{figure}[!htp]
	\centering
	\includegraphics[width=.6\textwidth]{img/roon-1.pdf}
	\caption{General structure of RooN system.}
\end{figure}

\newpage

\paragraph{Triple Modular Redundancy (TMR)}

\definition{Triple Modular Redundancy (TMR)} is a fault-tolerant form of N-modular redundancy, in which \textbf{three systems perform a process and the result is processed by a majority-voting system to produce a single output}. If any \textbf{one of the three systems fails}, the \textbf{other two systems can correct and mask the fault}.

\highspace
The system works properly if 2 out of 3 components work properly \textbf{\underline{and}} the voter works properly.

\highspace
The \definition{TMR Reliability $R_{TMR}$} is:
\begin{equation}
	R_{TMR} = R_{v}\left(3 \cdot R_{m}^{2} - 2 \cdot R_{m}^{3}\right)
\end{equation}
And the \definition{TMR MTTF $MTTF_{TMR}$} is:
\begin{equation}
	\texttt{MTTF}_{TMR} = \dfrac{5}{6} \cdot MTTF_{\text{simplex}}
\end{equation}

\begin{flushleft}
	\textcolor{Green3}{\faIcon{question-circle} \textbf{TMR: good or bad?}}
\end{flushleft}
TMR systems can \textbf{tolerate} both \textbf{transient}\footnote{In electrical engineering, a \definition{transient fault} is defined as an error condition that vanishes after the power is disconnected and restored.} \textbf{and permanent faults}\footnote{In electrical engineering, a persistent or \definition{permanent faults} are a type of fault that is present regardless of the disconnection of the power supply.}. It also has \textbf{higher reliability} (for shorter missions).

\highspace
The \textbf{TMR reliability} can be the \textbf{same as the series systems} if:
\begin{equation}
	R_{TMR}\left(t\right) = R_{c}\left(t\right) \Longrightarrow 3e^{-2 \lambda_{m} t} - 2e^{-3 \lambda_{m} t} = e^{-\lambda_{m} t}
\end{equation}
The time $t$ is:
\begin{equation}
	t = \dfrac{\ln\left(2\right)}{\lambda_{m}} \approx 0.7 \: \texttt{MMTF}_{c}
\end{equation}
Note that $R_{TMR}\left(t\right) > R_{c}\left(t\right)$ when mission time is less than 70\% of $\texttt{MTTF}_{c}$.

\newpage

\paragraph{Standby redundancy}

\definition{Standby redundancy} is a system consisting of two parallel replicas:
\begin{itemize}
	\item The \emph{primary} replica, which \textbf{operates all the time}.
	\item The \emph{redundant} replica (generally disabled) is \textbf{activated when the primary replica fails}.
\end{itemize}
\begin{figure}[!htp]
	\centering
	\includegraphics[width=.6\textwidth]{img/standby-redundancy-1.pdf}
\end{figure}

\noindent
\textbf{To be operational}, the standby system requires two mechanisms:
\begin{enumerate}
	\item A mechanism to \textbf{determine whether or not the primary replica is functioning properly} (on-line self check);
	\item A dynamic switching mechanism to \textbf{deactivate the primary replica and activate the redundant replica}.
\end{enumerate}

\begin{table}[!htp]
	\centering
	\begin{tabular}{@{} p{13em} | p{18em} @{}}
		\toprule
		\textbf{Standby Parallel Model} & \textbf{System Reliability} \\
		\midrule
		Equal failure rates, perfect switching & $R_{s} = e^{-\lambda t}\left(1 + \lambda t\right)$ \\
		Unequal failure rates, perfect switching & $R_{s} = e^{-\lambda_{1} t} + \lambda_{1} \dfrac{\left(e^{-\lambda_{1} t} - e^{-\lambda_{2} t}\right)}{\lambda_{2}-\lambda_{1}}$ \\
		Equal failure rates, imperfect switching & $R_{s} = e^{-\lambda t} \left(1 + R_{\text{switch}} \lambda t\right)$ \\
		Unequal failure rates, imperfect switching & $R_{s} = e^{-\lambda_{1} t} + R_{\text{switch}}\lambda_{1} \dfrac{\left(e^{-\lambda_{1} t} - e^{-\lambda_{2} t}\right)}{\lambda_{2} - \lambda_{1}}$ \\
		\bottomrule
	\end{tabular}
	\caption{Standby redundancy - Quick Formulas.}
\end{table}

\noindent
In the previous table we have:
\begin{itemize}
	\item $R_{s}$: System Reliability
	\item $\lambda$: Failure Rate
	\item $t$: Operating Time
	\item $R_{\text{switch}}$: Switching Reliability
\end{itemize}