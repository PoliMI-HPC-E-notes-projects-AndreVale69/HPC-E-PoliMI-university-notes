@unpublished{network-computing-polimi,
	title  = {Network {C}omputing},
	author = {Antichi Gianni},
	note   = {Slides from the HPC-E master's degree course on Politecnico di Milano},
	year   = {2024}
}
@article{10.1145/2829988.2787496,
	author = {Guo, Chuanxiong and Yuan, Lihua and Xiang, Dong and Dang, Yingnong and Huang, Ray and Maltz, Dave and Liu, Zhaoyi and Wang, Vin and Pang, Bin and Chen, Hua and Lin, Zhi-Wei and Kurien, Varugis},
	title = {Pingmesh: A Large-Scale System for Data Center Network Latency Measurement and Analysis},
	year = {2015},
	issue_date = {October 2015},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {45},
	number = {4},
	issn = {0146-4833},
	url = {https://doi.org/10.1145/2829988.2787496},
	doi = {10.1145/2829988.2787496},
	abstract = {Can we get network latency between any two servers at any time in large-scale data center networks? The collected latency data can then be used to address a series of challenges: telling if an application perceived latency issue is caused by the network or not, defining and tracking network service level agreement (SLA), and automatic network troubleshooting. We have developed the Pingmesh system for large-scale data center network latency measurement and analysis to answer the above question affirmatively. Pingmesh has been running in Microsoft data centers for more than four years, and it collects tens of terabytes of latency data per day. Pingmesh is widely used by not only network software developers and engineers, but also application and service developers and operators.},
	journal = {SIGCOMM Comput. Commun. Rev.},
	month = aug,
	pages = {139–152},
	numpages = {14},
	keywords = {silent packet drops, network troubleshooting, data center networking}
}
@inproceedings{10.1145/3098822.3098849,
	author = {Zhuo, Danyang and Ghobadi, Monia and Mahajan, Ratul and F\"{o}rster, Klaus-Tycho and Krishnamurthy, Arvind and Anderson, Thomas},
	title = {Understanding and Mitigating Packet Corruption in Data Center Networks},
	year = {2017},
	isbn = {9781450346535},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3098822.3098849},
	doi = {10.1145/3098822.3098849},
	abstract = {We take a comprehensive look at packet corruption in data center networks, which leads to packet losses and application performance degradation. By studying 350K links across 15 production data centers, we find that the extent of corruption losses is significant and that its characteristics differ markedly from congestion losses. Corruption impacts fewer links than congestion, but imposes a heavier loss rate; and unlike congestion, corruption rate on a link is stable over time and is not correlated with its utilization.Based on these observations, we developed CorrOpt, a system to mitigate corruption. To minimize corruption losses, it intelligently selects which corrupting links can be safely disabled, while ensuring that each top-of-rack switch has a minimum number of paths to reach other switches. CorrOpt also recommends specific actions (e.g., replace cables, clean connectors) to repair disabled links, based on our analysis of common symptoms of different root causes of corruption. Our recommendation engine has been deployed in over seventy data centers of a large cloud provider. Our analysis shows that, compared to current state of the art, CorrOpt can reduce corruption losses by three to six orders of magnitude and improve repair accuracy by 60\%.},
	booktitle = {Proceedings of the Conference of the ACM Special Interest Group on Data Communication},
	pages = {362–375},
	numpages = {14},
	keywords = {CorrOpt, Data Center Networks, Fault Mitigation, Optics, Packet Corruption},
	location = {Los Angeles, CA, USA},
	series = {SIGCOMM '17}
}
@inproceedings{10.1145/3387514.3406214,
	author = {Zhou, Yu and Sun, Chen and Liu, Hongqiang Harry and Miao, Rui and Bai, Shi and Li, Bo and Zheng, Zhilong and Zhu, Lingjun and Shen, Zhen and Xi, Yongqing and Zhang, Pengcheng and Cai, Dennis and Zhang, Ming and Xu, Mingwei},
	title = {Flow Event Telemetry on Programmable Data Plane},
	year = {2020},
	isbn = {9781450379557},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3387514.3406214},
	doi = {10.1145/3387514.3406214},
	abstract = {Network performance anomalies (NPAs), e.g. long-tailed latency, bandwidth decline, etc., are increasingly crucial to cloud providers as applications are getting more sensitive to performance. The fundamental difficulty to quickly mitigate NPAs lies in the limitations of state-of-the-art network monitoring solutions --- coarse-grained counters, active probing, or packet telemetry either cannot provide enough insights on flows or incur too much overhead. This paper presents NetSeer, a flow event telemetry (FET) monitor which aims to discover and record all performance-critical data plane events, e.g. packet drops, congestion, path change, and packet pause. NetSeer is efficiently realized on the programmable data plane. It has a high coverage on flow events including inter-switch packet drop/corruption which is critical but also challenging to retrieve the original flow information, with novel intra- and inter-switch event detection algorithms running on data plane; NetSeer also achieves high scalability and accuracy with innovative designs of event aggregation, information compression, and message batching that mainly run on data plane, using switch CPU as complement. NetSeer has been implemented on commodity programmable switches and NICs. With real case studies and extensive experiments, we show NetSeer can reduce NPA mitigation time by 61\%-99\% with only 0.01\% overhead of monitoring traffic.},
	booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
	pages = {76–89},
	numpages = {14},
	keywords = {Flow event telemetry, monitoring, programmable data plane},
	location = {Virtual Event, USA},
	series = {SIGCOMM '20}
}
@inproceedings {276948,
	author = {Weiwu Pang and Sourav Panda and Jehangir Amjad and Christophe Diot and Ramesh Govindan},
	title = {{CloudCluster}: Unearthing the Functional Structure of a Cloud Service},
	booktitle = {19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
	year = {2022},
	isbn = {978-1-939133-27-4},
	address = {Renton, WA},
	pages = {1213--1230},
	url = {https://www.usenix.org/conference/nsdi22/presentation/pang},
	publisher = {USENIX Association},
	month = apr
}
@article{10.1145/2829988.2787483,
	author = {Zhu, Yibo and Kang, Nanxi and Cao, Jiaxin and Greenberg, Albert and Lu, Guohan and Mahajan, Ratul and Maltz, Dave and Yuan, Lihua and Zhang, Ming and Zhao, Ben Y. and Zheng, Haitao},
	title = {Packet-Level Telemetry in Large Datacenter Networks},
	year = {2015},
	issue_date = {October 2015},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {45},
	number = {4},
	issn = {0146-4833},
	url = {https://doi.org/10.1145/2829988.2787483},
	doi = {10.1145/2829988.2787483},
	abstract = {Debugging faults in complex networks often requires capturing and analyzing traffic at the packet level. In this task, datacenter networks (DCNs) present unique challenges with their scale, traffic volume, and diversity of faults. To troubleshoot faults in a timely manner, DCN administrators must a) identify affected packets inside large volume of traffic; b) track them across multiple network components; c) analyze traffic traces for fault patterns; and d) test or confirm potential causes. To our knowledge, no tool today can achieve both the specificity and scale required for this task.We present Everflow, a packet-level network telemetry system for large DCNs. Everflow traces specific packets by implementing a powerful packet filter on top of "match and mirror" functionality of commodity switches. It shuffles captured packets to multiple analysis servers using load balancers built on switch ASICs, and it sends "guided probes" to test or confirm potential faults. We present experiments that demonstrate Everflow's scalability, and share experiences of troubleshooting network faults gathered from running it for over 6 months in Microsoft's DCNs.},
	journal = {SIGCOMM Comput. Commun. Rev.},
	month = aug,
	pages = {479–491},
	numpages = {13},
	keywords = {datacenter network, failure detection, probe}
}
@inproceedings{greenberg2015packet-level,
	author = {Greenberg, Albert and Maltz, Dave and Lu, Guohan and Cao, Jiaxin and Mahajan, Ratul and Zhu, Yibo},
	title = {Packet-Level Telemetry in Large Datacenter Networks},
	booktitle = {SIGCOMM'15},
	year = {2015},
	month = {August},
	abstract = {Debugging faults in complex networks often requires capturing and analyzing traffic at the packet level. In this task, datacenter networks (DCNs) present unique challenges with their scale, traffic volume, and diversity of faults. To troubleshoot faults in a timely manner, DCN administrators must a) identify affected packets inside large volume of traffic; b) track them across multiple network components; c) analyze traffic traces for fault patterns; and d) test or confirm potential causes. To our knowledge, no tool today can achieve both the specificity and scale required for this task. We present Everflow, a packet-level network telemetry system for large DCNs. Everflow traces specific packets by implementing a powerful packet filter on top of “match and mirror” functionality of commodity switches. It shuffles captured packets to multiple analysis servers using load balancers built on switch ASICs, and it sends “guided probes” to test or confirm potential faults. We present experiments that demonstrate Everflow’s scalability, and share experiences of troubleshooting network faults gathered from running it for over 6 months in Microsoft’s DCNs.},
	url = {https://www.microsoft.com/en-us/research/publication/packet-level-telemetry-in-large-datacenter-networks/},
	edition = {SIGCOMM'15},
}
@inproceedings{10.5555/2930611.2930632,
	author = {Li, Yuliang and Miao, Rui and Kim, Changhoon and Yu, Minlan},
	title = {FlowRadar: a better NetFlow for data centers},
	year = {2016},
	isbn = {9781931971294},
	publisher = {USENIX Association},
	address = {USA},
	abstract = {NetFlow has been a widely used monitoring tool with a variety of applications. NetFlow maintains an active working set of flows in a hash table that supports flow insertion, collision resolution, and flow removing. This is hard to implement in merchant silicon at data center switches, which has limited per-packet processing time. Therefore, many NetFlow implementations and other monitoring solutions have to sample or select a subset of packets to monitor. In this paper, we observe the need to monitor all the flows without sampling in short time scales. Thus, we design FlowRadar, a new way to maintain flows and their counters that scales to a large number of flows with small memory and bandwidth overhead. The key idea of FlowRadar is to encode perflow counters with a small memory and constant insertion time at switches, and then to leverage the computing power at the remote collector to perform network-wide decoding and analysis of the flow counters. Our evaluation shows that the memory usage of FlowRadar is close to traditional NetFlow with perfect hashing. With FlowRadar, operators can get better views into their networks as demonstrated by two new monitoring applications we build on top of FlowRadar.},
	booktitle = {Proceedings of the 13th Usenix Conference on Networked Systems Design and Implementation},
	pages = {311–324},
	numpages = {14},
	location = {Santa Clara, CA},
	series = {NSDI'16}
}
@inproceedings{al2010hedera,
  title={Hedera: dynamic flow scheduling for data center networks.},
  author={Al-Fares, Mohammad and Radhakrishnan, Sivasankar and Raghavan, Barath and Huang, Nelson and Vahdat, Amin and others},
  booktitle={Nsdi},
  volume={10},
  number={8},
  pages={89--92},
  year={2010},
  organization={San Jose, USA}
}
@inproceedings{katta2016hula,
  title={Hula: Scalable load balancing using programmable data planes},
  author={Katta, Naga and Hira, Mukesh and Kim, Changhoon and Sivaraman, Anirudh and Rexford, Jennifer},
  booktitle={Proceedings of the Symposium on SDN Research},
  pages={1--12},
  year={2016}
}
@inproceedings{eisenbud2016maglev,
  title={Maglev: A fast and reliable software network load balancer.},
  author={Eisenbud, Daniel E and Yi, Cheng and Contavalli, Carlo and Smith, Cody and Kononov, Roman and Mann-Hielscher, Eric and Cilingiroglu, Ardas and Cheyney, Bin and Shang, Wentao and Hosein, Jinnah Dylan},
  booktitle={Nsdi},
  volume={16},
  pages={523--535},
  year={2016}
}
@inproceedings{barbette2020high,
  title={A $\{$High-Speed$\}$$\{$Load-Balancer$\}$ Design with Guaranteed $\{$Per-Connection-Consistency$\}$},
  author={Barbette, Tom and Tang, Chen and Yao, Haoran and Kosti{\'c}, Dejan and Maguire Jr, Gerald Q and Papadimitratos, Panagiotis and Chiesa, Marco},
  booktitle={17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)},
  pages={667--683},
  year={2020}
}
@article{patel2013ananta,
  title={Ananta: Cloud scale load balancing},
  author={Patel, Parveen and Bansal, Deepak and Yuan, Lihua and Murthy, Ashwin and Greenberg, Albert and Maltz, David A and Kern, Randy and Kumar, Hemant and Zikos, Marios and Wu, Hongyu and others},
  journal={ACM SIGCOMM Computer Communication Review},
  volume={43},
  number={4},
  pages={207--218},
  year={2013},
  publisher={ACM New York, NY, USA}
}
@inproceedings{araujo2018balancing,
  title={Balancing on the edge: Transport affinity without network state},
  author={Ara{\'u}jo, Jo{\~a}o Taveira and Saino, Lorenzo and Buytenhek, Lennert and Landa, Raul},
  booktitle={15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)},
  pages={111--124},
  year={2018}
}