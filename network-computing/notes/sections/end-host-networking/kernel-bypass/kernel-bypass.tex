\subsection{Kernel Bypass}

The kernel bypass is motivated by one central idea: the traditional kernel networking stack becomes the bottleneck at high packet rates.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Kernel Overhead}}
\end{flushleft}
In the traditional Linux networking stack, the data path from the NIC to the application is as follows:
\begin{equation*}
    \text{NIC} \rightarrow \text{Driver} \rightarrow \text{Kernel} \rightarrow \text{Socket} \rightarrow \text{Application}
\end{equation*}
Each packet undergoes driver processing, interrupt handling, and a protocol stack (e.g., TCP/IP) before reaching the socket layer. Finally, the data is copied to the user-space buffer. Each of \textbf{these steps consumes CPU cycles}. At 10 Gbps, this may be manageable; however, at 100 Gbps, the CPU becomes overwhelmed. This leads to \textbf{packet drops and increased latency}. For example, with a minimum packet size of 64 bytes, the packet rate can reach 148.8 million packets per second. This is far beyond what a single CPU core can handle. At these rates, the per-packet overhead must be extremely small; even tens of CPU cycles matter. The classic kernel networking stack, designed for generality and correctness, was not originally designed for this scale.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Context Switching}}
\end{flushleft}
When an application receives packets via sockets, it usually makes a system call (e.g., \texttt{recv()}) to read data from the kernel. This involves switching (\emph{context switching}) from user space to kernel space, which is costly in terms of CPU cycles. The process is as follows:
\begin{equation*}
    \text{Application} \rightarrow \text{System call} \rightarrow \text{Kernel} \rightarrow \text{Return to user}
\end{equation*}
Each packet reception may involve a transition from user space to kernel space and back again. This process is called a \textbf{context switch}. Context switches are not free. They can take thousands of CPU cycles because the CPU must save registers, change the privilege level, switch stacks, and flush parts of the CPU pipeline. Although it only costs a few hundred nanoseconds, millions of packets per second results in significant overhead. \textbf{Context switches destroy scalability because the CPU spends more time switching contexts than processing packets}. This is especially problematic for high-performance applications, such as web servers, databases, and real-time analytics, which require low latency and high throughput.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{System Call Cost}}
\end{flushleft}
When using the traditional kernel networking stack socket APIs:
\begin{itemize}
    \item \texttt{recv()}: This system call is used to receive data from a socket. It involves copying data from the kernel buffer to the user-space buffer, which incurs overhead. The kernel must also check permissions, manage buffers, and handle any necessary protocol processing before returning data to the application.
    \item \texttt{send()}: This system call is used to send data through a socket. Similar to \texttt{recv()}, it involves copying data from the user-space buffer to the kernel buffer, which also incurs overhead. The kernel must handle protocol processing, manage buffers, and ensure that the data is sent correctly over the network.
    \item \texttt{select()/poll()/epoll()}: These system calls are used for multiplexing I/O operations. They allow an application to monitor multiple file descriptors (sockets) for events such as incoming data or the ability to send data. However, they also involve overhead due to the need to check multiple file descriptors and manage the event loop.
\end{itemize}
In general, each of these system calls requires a \textbf{transition from user space to kernel space}, which is expensive in terms of CPU cycles. \textbf{Even when no packet is available}, the cost of the syscall is incurred because the application must check for events. This can lead to \textbf{significant overhead}, particularly at high packet rates, when the application may make millions of system calls per second. The \hl{kernel bypass approach aims to eliminate this overhead by enabling applications to access the NIC directly and manage their own buffers}. This avoids the need for system calls and context switches.

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Packet Copy Overhead}}
\end{flushleft}
In the traditional kernel networking stack:
\begin{enumerate}
    \item NIC DMA writes packet into kernel memory (kernel buffer).
    \item Kernel copies packet from kernel buffer to user-space buffer (application buffer).
\end{enumerate}
We have two copies of each packet: one in kernel memory and one in user space. This equates to \textbf{at least one memory copy per packet}, which can be costly at high packet rates. For instance, with a 1500-byte packet and a 100 Gbps link, there can be up to 8.3 million packets per second. This means the CPU must perform 8.3 million memory copies per second. This can \textbf{consume a significant amount of CPU cycles} and lead to performance degradation. A \hl{kernel bypass eliminates the kernel-to-user packet copy, allowing the NIC to DMA packets directly into user-space memory}. This reduces memory bandwidth usage and latency.

\highspace
In summary, the kernel networking stack was optimized for fairness (i.e., sharing the CPU among multiple applications), security (i.e., isolating applications from each other), general-purpose workloads (i.e., supporting a wide range of applications), and multi-tenant systems (i.e., allowing multiple applications to run on the same machine). However, high-performance applications, such as load balancers, firewalls, and trading systems, require maximum throughput, lowest latency, and predictable performance. The kernel bypass approach allows these applications to achieve their performance goals by eliminating the overhead associated with the traditional kernel networking stack.

