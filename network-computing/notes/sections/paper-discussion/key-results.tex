\subsection{Key Results}

The paper presents a comprehensive evaluation of the proposed architecture, demonstrating significant improvements in network performance and reliability. The key results include:
\begin{itemize}
    \item \important{Queue Reduction}. They measured \textbf{switch queue occupancy under load}. This is the amount of buffered data waiting in switches. With high network load, partition-aggregate traffic patterns, and a TCP-based transport protocol, the results show that the proposed architecture significantly reduces queue occupancy compared to traditional approaches.
    \begin{itemize}
        \item TCP median queue: 4.35 MB
        \item Fastpass median queue: 18 KB
        \item \hl{$\approx 240\times$ reduction in queue occupancy} (99.6\% reduction in latency)
        \item Throughput penalty is only 1.5\% (i.e., Fastpass achieves 98.5\% of TCP throughput)
    \end{itemize}
    \item \important{Latency Reduction}. They measured \textbf{Round-Trip Time (RTT) under heavy load}.
    \begin{itemize}
        \item TCP: 3.56 ms
        \item Fastpass: 230 $\mu$s
        \item \hl{$\approx 15.5\times$ reduction in latency}
    \end{itemize}
    Reducing queue size directly reduces latency, and for latency-sensitive RPC workloads, this is a significant improvement.
    \item \important{Fairness}. They evaluated \textbf{throughput variability across flows}. They looked at the standard deviation of throughput. The result shows a \hl{$5200\times$ reduction in throughput variance}, indicating that under TCP, some flows dominate the network while others starve, whereas Fastpass achieves much more consistent throughput across flows.
    \item \important{Scalability of Arbiter}. They measured \textbf{how much traffic the arbiter can schedule}. The result shows that with 8 cores, the arbiter has a \hl{scheduling capacity of 2.21 Tbps}, which is enormously higher than the 100 Gbps line rate of the network, indicating that the arbiter is not a bottleneck.
    \item \important{Production Deployment (Facebook)}. They developed Fastpass in Facebook's datacenter. Results from production deployment show that Fastpass has \hl{$2.5\times$ fewer TCP retransmissions}. This is a significant improvement in reliability, as TCP retransmissions indicate packet loss and degraded performance.
\end{itemize}