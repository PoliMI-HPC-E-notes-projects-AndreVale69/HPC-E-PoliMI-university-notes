\subsection{OpenFlow limitations}

OpenFlow, conceived around 2007, introduced centralized control by standardizing how switches expose forwarding behavior to an SDN controller (as we discussed in the previous section, page \pageref{subsection: OpenFlow}). The \textbf{insight} at that time was that \textbf{most switches perform similar tasks} (Ethernet switching, IPv4 routing, VLAN tagging, ACL enforcement) \textbf{all via fixed}, \textbf{predictable behaviors}.

\highspace
OpenFlow capitalized on this fixed-function approach. Controllers could install flow rules into switches, dictating how they process known packet headers. However, a \textcolor{Red2}{\textbf{critical limitation}} emerged: \textbf{we couldn't add new protocols or processing capabilities easily}. Because \textbf{OpenFlow assumes a static data plane}, \textcolor{Red2}{\textbf{hardcoded to process only a predefined set of protocols and headers}}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Expanding OpenFlow: pushing its limits}}
\end{flushleft}
As networking needs evolved, particularly in \textbf{virtualized environments} and \textbf{cloud datacenters}, operators needed more \textbf{specialized packet processing}. For example, \texttt{VXLAN}, used to identify tenants in multi-tenant environments, wasn't supported in early OpenFlow.

\highspace
\textcolor{Green3}{\faIcon{check}} To address this, vendors and the OpenFlow community developed \textbf{new versions} (\texttt{1.1}, \texttt{1.2}, \texttt{1.3}, $\dots$). Each iteration \textbf{added support for more header types}, up to 50 different header types, but \textcolor{Red2}{\textbf{the process was slow and cumbersome}}. Each new feature needed:
\begin{itemize}
    \item New OpenFlow specification extensions.
    \item New ASICs in hardware to support the processing logic.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Hardware Bottlenecks: The ASIC Development Bottleneck}}
\end{flushleft}
Here lies the \textbf{\underline{core problem}}: even with updated protocols, \textbf{switches couldn't adapt until vendors redesigned and shipped new ASICs} (Application-Specific Integrated Circuits).

\highspace
This hardware dependency meant:
\begin{itemize}[label=\textcolor{Red2}{\faIcon{times}}]
    \item New features took \textbf{years to reach production}.
    \item Network owners \textbf{couldn't simply get a software upgrade}.
    \item The result: \textbf{slow innovation} in data plane capabilities.
\end{itemize}

\newpage
\begin{examplebox}[: \texttt{VXLAN}]
    Virtual Extensible LAN (\texttt{VXLAN}) was urgently needed by cloud providers and datacenters to enable multi-tenant network virtualization.
    
    Despite this high demand, hardware vendors took $\approx 4$ years to support \texttt{VXLAN} in switches due to ASIC development cycles and the fixed-function nature of OpenFlow switches.
    
    Even though vendors delayed its release, once \texttt{VXLAN} was available, it became a standard requirement in data centers.

    But attention! In the meantime, network operators used complex software overlays or kludges to simulate \texttt{VXLAN} functionality, increasing network complexity and cost.
\end{examplebox}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{thumbs-down} \textbf{The Cost of Delay: Workarounds and Complexity}}
\end{flushleft}
When vendors take years to deliver a new feature, network engineers often \textbf{develop complex workarounds}, \textbf{increasing network complexity} and \textbf{technical debt}. Even when the vendor releases the official feature:
\begin{itemize}
    \item The workaround may already be deeply integrated.
    \item The official solution may no longer solve the problem.
    \item Worse, it may require a forklift upgrade, replacing hardware at high cost.
\end{itemize}
This inertia locks networks into \textbf{suboptimal solutions} and \textbf{impedes the agility promised by SDN}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{\speedIcon} \textbf{The Missing Ingredient: Programmability at the Data Plane}}
\end{flushleft}
The shift from fixed-function to programmable data planes mirrors other computing domains:

\begin{table}[!htp]
    \centering
    \begin{tabular}{@{} l | l | l @{}}
        \toprule
        \textbf{Domain} & \textbf{Hardware} & \textbf{Compiler/SW Stack} \\
        \midrule
        General Computing & CPU             & Java, C, OS Kernels          \\ [.3em]
        Graphics          & GPU             & OpenCL, CUDA                 \\ [.3em]
        Signal Processing & DSP             & Matlab Compiler              \\ [.3em]
        Machine Learning  & TPU             & TensorFlow Compiler          \\ [.3em]
        \textbf{Networking} & \textbf{PISA Switch} & \textbf{P4 Language, P4 Compiler} \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
Just as CPUs became programmable via compilers, \textbf{networking needs} flexible \textbf{data planes programmable via languages} like P4, running on PISA (Protocol Independent Switch Architecture).

\highspace
\begin{takeawaysbox}[: OpenFlow limitations]
    \begin{itemize}
        \item OpenFlow was a \textbf{revolution in control plane innovation}, but its \textbf{rigid data plane} became a bottleneck. The industry's response, iterative protocol updates and ASIC redesigns, proved \textbf{slow and reactive}.

        \item A true solution lies in programmable data planes, where \textbf{software defines packet processing}, and the network evolves \textbf{as fast as the application demands}.

        \item This transition is \textbf{not trivial}, it requires new hardware, new abstractions, and operator retraining, but it's essential to \textbf{fulfill SDN's promise} of \textbf{rapid, flexible, and scalable networking}.
    \end{itemize}
\end{takeawaysbox}
