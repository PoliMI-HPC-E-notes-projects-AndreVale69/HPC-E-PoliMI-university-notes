\subsubsection{January 23}

\subsubsection*{Exercise 1 - Theory}

\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Consider the following problem: find $\mathbf{x} \in \mathbb{R}^{n}$, such that $A\mathbf{x} = \mathbf{b}$, where $A \in \mathbb{R}^{n \times n}$ and $\mathbf{b} \in \mathbb{R}^{n}$ are given. State under which conditions the mathematical problem is well posed.
    }}

    \textbf{Answer}: page \hqpageref{exam: when a linear system is mathematically well posed}.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Describe the LU factorization and its use to approximately solve the above linear system.
    }}

    \textbf{Answer}: LU factorization is a method of decomposing a given square matrix $A$ into the product of two matrices: a lower triangular matrix $L$ with ones on the diagonal ($L_{ii} = 1$) and an upper triangular matrix $U$.
    \begin{equation*}
        A = LU
    \end{equation*}
    
    Steps to Solve $A\mathbf{x} = \mathbf{b}$ using LU Factorization:
    \begin{enumerate}
        \item Compute the LU factorization of $A$:
        \begin{equation*}
            A = LU
        \end{equation*}

        \item Solve the intermediate system $L\mathbf{y} = \mathbf{b}$ for $\mathbf{y}$: since $L$ is a lower triangular matrix, this step is performed using \textbf{\emph{forward substitution}}. Start with the first equation and solve for $y_{1}$, then proceed to the next equation, incorporating previously computed values.
        \begin{equation*}
            \begin{pmatrix}
                l_{11} & 0      & \cdots & 0      \\
                l_{21} & l_{22} & \cdots & 0      \\
                \vdots & \vdots & \ddots & \vdots \\
                l_{n1} & l_{n2} & \cdots & l_{nn}
            \end{pmatrix}
            \begin{pmatrix}
                y_{1} \\ y_{2} \\ \vdots \\ y_{n}
            \end{pmatrix}
            =
            \begin{pmatrix}
                b_{1} \\ b_{2} \\ \vdots \\ b_{n}
            \end{pmatrix}
        \end{equation*}

        \item Solve the upper triangular system $U\mathbf{y} = \mathbf{x}$: using the solution $\mathbf{y}$ obtained from the forward substitution step, solve for $\mathbf{x}$ using \textbf{\emph{backward substitution}}. Start with the last equation and solve for $x_{n}$, then proceed upwards, incorporating previously computed values.
        \begin{equation*}
            \begin{pmatrix}
                u_{11} & u_{12} & \cdots & u_{1n} \\
                0      & u_{22} & \cdots & u_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                0      & 0      & \cdots & u_{nn}
                \end{pmatrix}
                \begin{pmatrix}
                x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{pmatrix}
                =
                \begin{pmatrix}
                y_{1} \\ y_{2} \\ \vdots \\ y_{n}
            \end{pmatrix}
        \end{equation*}
    \end{enumerate}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        State the necessary and sufficient condition that guarantees existence and uniqueness of the LU factorization. For what classes of matrices the LU factorization exists and is unique?
    }}

    \textbf{Answer}: The \textbf{\emph{necessary and sufficient condition}} for the existence and uniqueness of the LU factorization of a matrix $A$ is that all leading principal minors\footnote{A \definition{Leading Principal Minor} is the determinant of a top-left submatrix of $A$.} of $A$ are non-zero.
    
    Formally, let $A$ be an $n \times n$ matrix. The LU factorization $A = LU$ exists and is unique if and only if the leading principal minors of $A$, i.e., the determinants of the submatrices $A_{1:k, 1:k}$ for $k = 1, 2, \ldots, n$, are non-zero:
    \begin{equation*}
        \exists\left(A = LU\right) \iff \det(A_{1:k, 1:k}) \neq 0 \hspace{2em} \forall k = 1, 2, \dots, n
    \end{equation*}

    \textbf{\emph{Classes of Matrices}}
    \begin{enumerate}
        \item \textbf{\emph{Non-Singular (Invertible) Matrices}}. If $A$ is a non-singular (invertible) matrix and all leading principal minors are non-zero, LU factorization exists and is unique.
        \item \textbf{\emph{Diagonally Dominant Matrices}}. For any matrix $A$ that is strictly diagonally dominant (where the magnitude of each diagonal element is greater than the sum of the magnitudes of the other elements in the same row), the LU factorization exists and is unique without the need for pivoting.
        \item \textbf{\emph{Positive Definite Matrices}}. If $A$ is a symmetric positive definite matrix, LU factorization exists and is unique. This case often leads to Cholesky decomposition, where $L = U^T$.
    \end{enumerate}
    
    \textbf{\emph{Pivoting}}. For matrices that do not meet these conditions (i.e., if some leading principal minors are zero or close to zero), LU factorization can still be performed by using partial pivoting or complete pivoting.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Describe the main pivoting techniques and comment on the computational costs.
    }}

    \textbf{Answer}: The \textbf{\emph{Main pivoting techniques}}
    \begin{enumerate}
        \item Pivoting by rows (Partial Pivoting):
        \begin{itemize}
            \item It involves swapping the current row with another row below it such that the largest absolute value element in the current column is moved to the diagonal position.
            \item The procedure is: for each column $k$ (from the first to the last)
            \begin{enumerate}
                \item Find the row $i$ (where $i>k$) with the largest absolute value in the pivot column.
                \item Swap row $k$ with row $i$ to move the largest pivot element to the diagonal position $a_{kk}$.
                \item Continue with the Gaussian elimination process.
            \end{enumerate}
            \item Its goal is to avoid division by zero (pivot element $\ne 0$) and to improve numerical stability by avoiding division by small numbers.
        \end{itemize}

        \item Complete Pivoting:
        \begin{itemize}
            \item It involves both row and column swaps to ensure the largest absolute value element in the remaining submatrix is moved to the diagonal position.
            \item The procedure is: for each step $k$
            \begin{enumerate}
                \item Identify the largest absolute value element in the remaining submatrix.
                \item Swap the current row $k$ with the row containing the largest element.
                \item Swap the current column $k$ with the column containing the largest element.
            \end{enumerate}
            \item Its goal is to guarantee the most accurate and stable results during matrix factorization. Because of the double swap (row and column), it introduces more overhead despite the partial pivoting.
        \end{itemize}
    \end{enumerate}

    \textbf{\emph{Computational Costs}} The computational cost of pivoting techniques includes the overhead associated with identifying and performing row and column swaps.
    \begin{itemize}
        \item Pivoting by rows (partial pivoting). The maximum element search takes $O\left(n^{2}\right)$, the row swapping is $O\left(n^{2}\right)$, therefore the \textbf{\emph{total cost}} is $O\left(n^{2}\right)$, additional to the $O\left(n^{3}\right)$ cost of LU factorization.

        \item Complete Pivoting. The maximum element search takes $O\left(n^{3}\right)$, the row/column swapping is $O\left(n^{2}\right)$, therefore the \textbf{\emph{total cost}} is $O\left(n^{3}\right)$, comparable to the cost of LU factorization.
    \end{itemize}
    Complete pivoting provides greater numerical stability at the cost of increased computational effort, whereas partial pivoting offers a good balance between stability and efficiency.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 1 - Laboratory}

The exam text provides the following matrix:
\begin{center}
    A: \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-a-mtx}
    \hspace{2em}
    B: \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-b-mtx}
    \hspace{2em}
    C: \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-c-mtx}
\end{center}
The solution code snippet is available here:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-exer1-cpp}
\end{center}
\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Download the sparse matrices \texttt{A.mtx}, \texttt{B.mtx}, and \texttt{C.mtx} and save it. Load the three matrices in a new file \texttt{exer1.cpp}. Define the block matrix $M = \begin{pmatrix}
            A & B^{T} \\ B & C
        \end{pmatrix}$. Report on the \texttt{.txt} file the matrix size and the Euclidean norm of $M$. Is the matrix symmetric?
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <unsupported/Eigen/SparseExtra>

int main() {
    // Load the 3 matrices
    Eigen::SparseMatrix<double> A, B, C;
    loadMarket(A, "A.mtx");
    loadMarket(B, "B.mtx");
    loadMarket(C, "C.mtx");
    const auto B_t = Eigen::SparseMatrix<double>(B.transpose());
    // Create the matrix M
    Eigen::MatrixXd M(A.rows()+B.rows(), A.cols()+B_t.cols());
    // Create the matrix M = [A B.t ; B C]
    M.topLeftCorner(A.rows(), A.cols()) = A;
    M.topRightCorner(A.rows(), B_t.cols()) = B_t;
    M.bottomLeftCorner(B.rows(), A.cols()) = B;
    M.bottomRightCorner(B.rows(), C.cols()) = C;
    // Print size of M
    printf("Size of M: %ldx%ld\n", M.rows(), M.cols());
    // The norm (magnitude) of the matrix M and the M-M.t norm
    printf("Norm of M: %g\n", M.norm());
    printf("Norm of M-M.t is %g ", (M.transpose() - M).norm());
    // To see if the matrix is symmetric
    // we can check if the difference between M and M.t is zero.
    // Instead of checking if the difference is zero,
    // we can check if the norm of the difference is zero.
    printf("and the matrix M is indeed %s symmetric.\n",
            (M.transpose() - M).norm() < 1e-10 ? "" : "not");
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Size of M: 278x278
Norm of M: 459.79
Norm of M-M.t is 132.073 and the matrix M is indeed not symmetric.\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Define an \texttt{Eigen} vector $\mathbf{b} = \left(1, 1, \dots, 1\right)^{T}$ with size equal to the number of rows of $M$. Solve the linear system $M\mathbf{x} = \mathbf{b}$ using the LU decomposition method available in the \texttt{Eigen} library. Report on the \texttt{.txt} file the norm of the obtained absolute residual.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Define vector b
const Eigen::VectorXd b = Eigen::VectorXd::Ones(M.rows());
// Solve the system Mx = b using the LU decomposition
Eigen::SparseLU<Eigen::SparseMatrix<double>> solver;
solver.compute(M.sparseView());
if (solver.info() != Eigen::Success) {
    printf("Factorization failed\n");
    return 1;
}
const Eigen::VectorXd x = solver.solve(b);
// Print the norm of the absolute residual
// We compute the absolute residual by subtracting the product of M and x from b.
// In fact, the residual is the difference between the right-hand side
// and the left-hand side of the equation.
// If the residual is (approximately) zero, the solution is correct.
printf("Norm of the absolute residual: %g\n", (M*x - b).norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of the absolute residual: 1.62043e-13\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Using again the LU decomposition provided by \texttt{Eigen}, compute an approximation of the Schur complement of $M$ with respect to the $A$ block defined as the matrix $S = C- B A^{-1}B^{T}$. Report on the \texttt{.txt} file the matrix size and the Euclidean norm of $S$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
/**
* The Schur complement is a concept in linear algebra that
* describes the result of eliminating a set of variables from a system of linear equations.
*
* However, from the definition of the Schur complement,
* we can see that it is not necessary to compute the inverse of A.
* We can use the LU decomposition of A to calculate its inverse.
* Therefore, we can directly compute the Schur complement using its definition.
*
* This is the same thing we do in the real world on the paper.
*/
// Compute the inverse of A
Eigen::SparseLU<Eigen::SparseMatrix<double>> solver_A;
solver_A.compute(A);
if (solver_A.info() != Eigen::Success) {
    printf("Factorization failed\n");
    return 1;
}
const Eigen::SparseMatrix<double> A_inv = solver_A.solve(
    Eigen::MatrixXd::Identity(A.rows(), A.cols()).sparseView()
);
// Compute the Schur complement
const Eigen::SparseMatrix<double> S = C - B * A_inv * B_t;
// Print the norm of the Schur complement
printf("Norm of the Schur complement: %g\n", S.norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of the Schur complement: 9.7299\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Solve the linear system:
    }
    \begin{equation*}
        M\mathbf{x} = \begin{pmatrix}
            A & B^{T} \\ B & C
        \end{pmatrix}
        \begin{pmatrix}
            \mathbf{x}_{1} \\ \mathbf{x}_{2}
        \end{pmatrix}
        =
        \begin{bmatrix}
            \mathbf{b}_{1} \\ \mathbf{b}_{2}
        \end{bmatrix}
        =
        \mathbf{b}
    \end{equation*}
    \textbf{by exploiting the Schur complement $S$. First use the LU factorization method to approximate the solution of $S\mathbf{x}_{2} = \mathbf{b}_{2} - BA^{-1}\mathbf{b}_{1}$. Then compute the approximate solution of $A\mathbf{x}_{1} = \mathbf{b}_{1} - B^{T}\mathbf{x}_{2}$. Report the norm of the absolute residual $\mathbf{r} = \mathbf{b} - M * \left[\overline{\mathbf{x}}_{1}, \overline{\mathbf{x}}_{2}\right]^{T}$, where $\overline{\mathbf{x}}_{1}$ and $\overline{\mathbf{x}}_{2}$ are the computed approximations of $\mathbf{x}_{1}$ and $\mathbf{x}_{2}$ respectively.}
    }

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Solve in two steps exploiting the Schur complement
Eigen::SparseLU<Eigen::SparseMatrix<double>> solve_lu;
// Compute the Schur complement
solve_lu.compute(S);
if (solve_lu.info() != Eigen::Success) {
    printf("Factorization failed\n");
    return 1;
}
const Eigen::VectorXd x2 = solve_lu.solve(b.tail(S.rows()) - B*solver_A.solve(b.head(A.rows())));
const Eigen::VectorXd x1 = solver_A.solve(b.head(A.rows()) - B_t*x2);
Eigen::VectorXd x12(M.rows());
x12 << x1, x2;
// Print the norm of the absolute residual
printf("Solution with Schur complement\n");
printf("Norm of the absolute residual: %g\n", (b - M*x12).norm());
printf("Comparison solutions: %g\n", (x - x12).norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Solution with Schur complement
Norm of the absolute residual: 1.29778e-13
Comparison solutions: 1.13896e-13\end{lstlisting}
\end{enumerate}