\subsubsection{January 23}

\subsubsection*{Exercise 1 - Theory}

\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Consider the following problem: find $\mathbf{x} \in \mathbb{R}^{n}$, such that $A\mathbf{x} = \mathbf{b}$, where $A \in \mathbb{R}^{n \times n}$ and $\mathbf{b} \in \mathbb{R}^{n}$ are given. State under which conditions the mathematical problem is well posed.
    }}

    \textbf{Answer}: page \hqpageref{exam: when a linear system is mathematically well posed}.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Describe the LU factorization and its use to approximately solve the above linear system.
    }}

    \textbf{Answer}: LU factorization is a method of decomposing a given square matrix $A$ into the product of two matrices: a lower triangular matrix $L$ with ones on the diagonal ($L_{ii} = 1$) and an upper triangular matrix $U$.
    \begin{equation*}
        A = LU
    \end{equation*}
    
    Steps to Solve $A\mathbf{x} = \mathbf{b}$ using LU Factorization:
    \begin{enumerate}
        \item Compute the LU factorization of $A$:
        \begin{equation*}
            A = LU
        \end{equation*}

        \item Solve the intermediate system $L\mathbf{y} = \mathbf{b}$ for $\mathbf{y}$: since $L$ is a lower triangular matrix, this step is performed using \textbf{\emph{forward substitution}}. Start with the first equation and solve for $y_{1}$, then proceed to the next equation, incorporating previously computed values.
        \begin{equation*}
            \begin{pmatrix}
                l_{11} & 0      & \cdots & 0      \\
                l_{21} & l_{22} & \cdots & 0      \\
                \vdots & \vdots & \ddots & \vdots \\
                l_{n1} & l_{n2} & \cdots & l_{nn}
            \end{pmatrix}
            \begin{pmatrix}
                y_{1} \\ y_{2} \\ \vdots \\ y_{n}
            \end{pmatrix}
            =
            \begin{pmatrix}
                b_{1} \\ b_{2} \\ \vdots \\ b_{n}
            \end{pmatrix}
        \end{equation*}

        \item Solve the upper triangular system $U\mathbf{y} = \mathbf{x}$: using the solution $\mathbf{y}$ obtained from the forward substitution step, solve for $\mathbf{x}$ using \textbf{\emph{backward substitution}}. Start with the last equation and solve for $x_{n}$, then proceed upwards, incorporating previously computed values.
        \begin{equation*}
            \begin{pmatrix}
                u_{11} & u_{12} & \cdots & u_{1n} \\
                0      & u_{22} & \cdots & u_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                0      & 0      & \cdots & u_{nn}
                \end{pmatrix}
                \begin{pmatrix}
                x_{1} \\ x_{2} \\ \vdots \\ x_{n}
                \end{pmatrix}
                =
                \begin{pmatrix}
                y_{1} \\ y_{2} \\ \vdots \\ y_{n}
            \end{pmatrix}
        \end{equation*}
    \end{enumerate}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        State the necessary and sufficient condition that guarantees existence and uniqueness of the LU factorization. For what classes of matrices the LU factorization exists and is unique?
    }}

    \textbf{Answer}: The \textbf{\emph{necessary and sufficient condition}} for the existence and uniqueness of the LU factorization of a matrix $A$ is that all leading principal minors\footnote{A \definition{Leading Principal Minor} is the determinant of a top-left submatrix of $A$.} of $A$ are non-zero.
    
    Formally, let $A$ be an $n \times n$ matrix. The LU factorization $A = LU$ exists and is unique if and only if the leading principal minors of $A$, i.e., the determinants of the submatrices $A_{1:k, 1:k}$ for $k = 1, 2, \ddots, n$, are non-zero:
    \begin{equation*}
        \exists\left(A = LU\right) \iff \det(A_{1:k, 1:k}) \neq 0 \hspace{2em} \forall k = 1, 2, \dots, n
    \end{equation*}

    \textbf{\emph{Classes of Matrices}}
    \begin{enumerate}
        \item \textbf{\emph{Non-Singular (Invertible) Matrices}}. If $A$ is a non-singular (invertible) matrix and all leading principal minors are non-zero, LU factorization exists and is unique.
        \item \textbf{\emph{Diagonally Dominant Matrices}}. For any matrix $A$ that is strictly diagonally dominant (where the magnitude of each diagonal element is greater than the sum of the magnitudes of the other elements in the same row), the LU factorization exists and is unique without the need for pivoting.
        \item \textbf{\emph{Positive Definite Matrices}}. If $A$ is a symmetric positive definite matrix, LU factorization exists and is unique. This case often leads to Cholesky decomposition, where $L = U^T$.
    \end{enumerate}
    
    \textbf{\emph{Pivoting}}. For matrices that do not meet these conditions (i.e., if some leading principal minors are zero or close to zero), LU factorization can still be performed by using partial pivoting or complete pivoting.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Describe the main pivoting techniques and comment on the computational costs.
    }}

    \textbf{Answer}: The \textbf{\emph{Main pivoting techniques}}
    \begin{enumerate}
        \item Pivoting by rows (Partial Pivoting):
        \begin{itemize}
            \item It involves swapping the current row with another row below it such that the largest absolute value element in the current column is moved to the diagonal position.
            \item The procedure is: for each column $k$ (from the first to the last)
            \begin{enumerate}
                \item Find the row $i$ (where $i>k$) with the largest absolute value in the pivot column.
                \item Swap row $k$ with row $i$ to move the largest pivot element to the diagonal position $a_{kk}$.
                \item Continue with the Gaussian elimination process.
            \end{enumerate}
            \item Its goal is to avoid division by zero (pivot element $\ne 0$) and to improve numerical stability by avoiding division by small numbers.
        \end{itemize}

        \item Complete Pivoting:
        \begin{itemize}
            \item It involves both row and column swaps to ensure the largest absolute value element in the remaining submatrix is moved to the diagonal position.
            \item The procedure is: for each step $k$
            \begin{enumerate}
                \item Identify the largest absolute value element in the remaining submatrix.
                \item Swap the current row $k$ with the row containing the largest element.
                \item Swap the current column $k$ with the column containing the largest element.
            \end{enumerate}
            \item Its goal is to guarantee the most accurate and stable results during matrix factorization. Because of the double swap (row and column), it introduces more overhead despite the partial pivoting.
        \end{itemize}
    \end{enumerate}

    \textbf{\emph{Computational Costs}} The computational cost of pivoting techniques includes the overhead associated with identifying and performing row and column swaps.
    \begin{itemize}
        \item Pivoting by rows (partial pivoting). The maximum element search takes $O\left(n^{2}\right)$, the row swapping is $O\left(n^{2}\right)$, therefore the \textbf{\emph{total cost}} is $O\left(n^{2}\right)$, additional to the $O\left(n^{3}\right)$ cost of LU factorization.

        \item Complete Pivoting. The maximum element search takes $O\left(n^{3}\right)$, the row/column swapping is $O\left(n^{2}\right)$, therefore the \textbf{\emph{total cost}} is $O\left(n^{3}\right)$, comparable to the cost of LU factorization.
    \end{itemize}
    Complete pivoting provides greater numerical stability at the cost of increased computational effort, whereas partial pivoting offers a good balance between stability and efficiency.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 1 - Laboratory}

The exam text provides the following matrix:
\begin{center}
    A: \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-a-mtx}
    \hspace{2em}
    B: \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-b-mtx}
    \hspace{2em}
    C: \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-c-mtx}
\end{center}
The solution code snippet is available here:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-exer1-cpp}
\end{center}
\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Download the sparse matrices \texttt{A.mtx}, \texttt{B.mtx}, and \texttt{C.mtx} and save it. Load the three matrices in a new file \texttt{exer1.cpp}. Define the block matrix $M = \begin{pmatrix}
            A & B^{T} \\ B & C
        \end{pmatrix}$. Report on the \texttt{.txt} file the matrix size and the Euclidean norm of $M$. Is the matrix symmetric?
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <unsupported/Eigen/SparseExtra>

int main() {
    // Load the 3 matrices
    Eigen::SparseMatrix<double> A, B, C;
    loadMarket(A, "A.mtx");
    loadMarket(B, "B.mtx");
    loadMarket(C, "C.mtx");
    const auto B_t = Eigen::SparseMatrix<double>(B.transpose());
    // Create the matrix M
    Eigen::MatrixXd M(A.rows()+B.rows(), A.cols()+B_t.cols());
    // Create the matrix M = [A B.t ; B C]
    M.topLeftCorner(A.rows(), A.cols()) = A;
    M.topRightCorner(A.rows(), B_t.cols()) = B_t;
    M.bottomLeftCorner(B.rows(), A.cols()) = B;
    M.bottomRightCorner(B.rows(), C.cols()) = C;
    // Print size of M
    printf("Size of M: %ldx%ld\n", M.rows(), M.cols());
    // The norm (magnitude) of the matrix M and the M-M.t norm
    printf("Norm of M: %g\n", M.norm());
    printf("Norm of M-M.t is %g ", (M.transpose() - M).norm());
    // To see if the matrix is symmetric
    // we can check if the difference between M and M.t is zero.
    // Instead of checking if the difference is zero,
    // we can check if the norm of the difference is zero.
    printf("and the matrix M is indeed %s symmetric.\n",
            (M.transpose() - M).norm() < 1e-10 ? "" : "not");
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Size of M: 278x278
Norm of M: 459.79
Norm of M-M.t is 132.073 and the matrix M is indeed not symmetric.\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Define an \texttt{Eigen} vector $\mathbf{b} = \left(1, 1, \dots, 1\right)^{T}$ with size equal to the number of rows of $M$. Solve the linear system $M\mathbf{x} = \mathbf{b}$ using the LU decomposition method available in the \texttt{Eigen} library. Report on the \texttt{.txt} file the norm of the obtained absolute residual.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Define vector b
const Eigen::VectorXd b = Eigen::VectorXd::Ones(M.rows());
// Solve the system Mx = b using the LU decomposition
Eigen::SparseLU<Eigen::SparseMatrix<double>> solver;
solver.compute(M.sparseView());
if (solver.info() != Eigen::Success) {
    printf("Factorization failed\n");
    return 1;
}
const Eigen::VectorXd x = solver.solve(b);
// Print the norm of the absolute residual
// We compute the absolute residual by subtracting the product of M and x from b.
// In fact, the residual is the difference between the right-hand side
// and the left-hand side of the equation.
// If the residual is (approximately) zero, the solution is correct.
printf("Norm of the absolute residual: %g\n", (M*x - b).norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of the absolute residual: 1.62043e-13\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Using again the LU decomposition provided by \texttt{Eigen}, compute an approximation of the Schur complement of $M$ with respect to the $A$ block defined as the matrix $S = C- B A^{-1}B^{T}$. Report on the \texttt{.txt} file the matrix size and the Euclidean norm of $S$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
/**
* The Schur complement is a concept in linear algebra that
* describes the result of eliminating a set of variables from a system of linear equations.
*
* However, from the definition of the Schur complement,
* we can see that it is not necessary to compute the inverse of A.
* We can use the LU decomposition of A to calculate its inverse.
* Therefore, we can directly compute the Schur complement using its definition.
*
* This is the same thing we do in the real world on the paper.
*/
// Compute the inverse of A
Eigen::SparseLU<Eigen::SparseMatrix<double>> solver_A;
solver_A.compute(A);
if (solver_A.info() != Eigen::Success) {
    printf("Factorization failed\n");
    return 1;
}
const Eigen::SparseMatrix<double> A_inv = solver_A.solve(
    Eigen::MatrixXd::Identity(A.rows(), A.cols()).sparseView()
);
// Compute the Schur complement
const Eigen::SparseMatrix<double> S = C - B * A_inv * B_t;
// Print the norm of the Schur complement
printf("Norm of the Schur complement: %g\n", S.norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of the Schur complement: 9.7299\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Solve the linear system:
    }
    \begin{equation*}
        M\mathbf{x} = \begin{pmatrix}
            A & B^{T} \\ B & C
        \end{pmatrix}
        \begin{pmatrix}
            \mathbf{x}_{1} \\ \mathbf{x}_{2}
        \end{pmatrix}
        =
        \begin{bmatrix}
            \mathbf{b}_{1} \\ \mathbf{b}_{2}
        \end{bmatrix}
        =
        \mathbf{b}
    \end{equation*}
    \textbf{by exploiting the Schur complement $S$. First use the LU factorization method to approximate the solution of $S\mathbf{x}_{2} = \mathbf{b}_{2} - BA^{-1}\mathbf{b}_{1}$. Then compute the approximate solution of $A\mathbf{x}_{1} = \mathbf{b}_{1} - B^{T}\mathbf{x}_{2}$. Report the norm of the absolute residual $\mathbf{r} = \mathbf{b} - M * \left[\overline{\mathbf{x}}_{1}, \overline{\mathbf{x}}_{2}\right]^{T}$, where $\overline{\mathbf{x}}_{1}$ and $\overline{\mathbf{x}}_{2}$ are the computed approximations of $\mathbf{x}_{1}$ and $\mathbf{x}_{2}$ respectively.}
    }

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Solve in two steps exploiting the Schur complement
Eigen::SparseLU<Eigen::SparseMatrix<double>> solve_lu;
// Compute the Schur complement
solve_lu.compute(S);
if (solve_lu.info() != Eigen::Success) {
    printf("Factorization failed\n");
    return 1;
}
const Eigen::VectorXd x2 = solve_lu.solve(b.tail(S.rows()) - B*solver_A.solve(b.head(A.rows())));
const Eigen::VectorXd x1 = solver_A.solve(b.head(A.rows()) - B_t*x2);
Eigen::VectorXd x12(M.rows());
x12 << x1, x2;
// Print the norm of the absolute residual
printf("Solution with Schur complement\n");
printf("Norm of the absolute residual: %g\n", (b - M*x12).norm());
printf("Comparison solutions: %g\n", (x - x12).norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Solution with Schur complement
Norm of the absolute residual: 1.29778e-13
Comparison solutions: 1.13896e-13\end{lstlisting}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 2 - Theory}

\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Consider the following eigenvalue problem: $A\mathbf{x} = \lambda\mathbf{x}$, where $A \in \mathbb{R}^{n \times n}$ is given. Describe the inverse power method for the numerical approximation of the smallest in modulus eigenvalue of $A$. Introduce the notation and discuss the applicability conditions.
    }}

    \textbf{Answer}: The inverse power method is an iterative technique used to find the smallest eigenvalues of a matrix.

    The inverse power method leverages the fact that applying the power method to the inverse of a matrix $A$ will converge to the eigenvalue of $A^{-1}$ with the largest magnitude. Since the eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of $A$, the inverse power method converges to the eigenvalue of $A$ with the smallest absolute value.

    \textbf{\emph{Notation}}
    \begin{itemize}
        \item $A$: An $n \times n$ real matrix.
        \item $\mathbf{x}$: An eigenvector corresponding to the eigenvalue $\lambda$.
        \item $\lambda$: The eigenvalue associated with the eigenvector $\mathbf{x}$.
        \item $\mathbf{x}^{(k)}$: The approximation of the eigenvector at iteration $k$.
        \item $\lambda^{(k)}$: The approximation of the eigenvalue at iteration $k$.
    \end{itemize}

    \textbf{\emph{Applicability Conditions}}
    \begin{enumerate}
        \item \textbf{\emph{Invertibility}}: The matrix $A$ must be invertible, i.e., it should have a non-zero determinant.
        \item \textbf{\emph{Distinct Smallest Eigenvalue}}: The smallest eigenvalue in magnitude should be distinct and well-separated from the other eigenvalues. This ensures rapid convergence.
        \item \textbf{\emph{Initial Vector}}: The initial vector $\mathbf{x}^{(0)}$ should have a non-zero component in the direction of the eigenvector corresponding to the smallest eigenvalue. Random vectors typically satisfy this condition. Also, its Euclidean norm should be one.
        \item \textbf{\emph{Numerical Stability}}: Solving the linear system $A \mathbf{y}^{(k+1)} = \mathbf{x}^{(k)}$ at each iteration can introduce numerical instability, especially if $A$ is ill-conditioned. Preconditioning or regularization techniques might be needed.
    \end{enumerate}

    \begin{tipbox}
        How can we remember the \dquotes{Applicability Conditions}?
        \begin{enumerate}
            \item \textbf{\emph{Invertibility}}. If the matrix $A$ is not invertible, then the matrix $A^{-1}$ doesn't exist. This means that the step in the algorithm $A\mathbf{y}^{\left(k+1\right)} = \mathbf{x}^{\left(k\right)}$ cannot be solved if the matrix is not invertible!
            \item \textbf{\emph{Distinct Smallest Eigenvalue}}. Having a distinct smallest eigenvalue ensures that the inverse power method can effectively and accurately converge to the correct smallest eigenvalue. It guarantees that the iterative process is dominated by the single smallest eigenvalue, leading to reliable and stable convergence.
            \item \textbf{\emph{Initial Vector}}. In each iteration of the inverse power method, the vector is multiplied by $A^{-1}$. This process amplifies the components of the vector in the direction of the eigenvector corresponding to the smallest eigenvalue. If the initial vector $\mathbf{x}^{0}$ has a zero component in this direction, the iterative process will not capture or amplify this component, leading to incorrect convergence.
            \item \textbf{\emph{Numerical Stability}}. The first operation of the algorithm at each step can introduce some instability, so preconditioning and pivoting techniques are used to massage the matrices during factorization.
        \end{enumerate}
    \end{tipbox}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{Write the (pseudo) algorithm.}}
    
    \textbf{Answer}:
    \begin{enumerate}
        \item Choose an initial non-zero vector $\mathbf{x}^{(0)}$, such that its norm is one $\left\| \mathbf{x}^{\left(0\right)} \right\| = 1$. Often, a random vector or a vector of all ones is chosen.

        \item For $k \ge 0$ until convergence:
        \begin{enumerate}
            \item Solve the linear system:
            \begin{equation*}
                A \mathbf{y}^{(k+1)} = \mathbf{x}^{(k)}
            \end{equation*}
            \item Normalize the resulting vector to prevent it from growing too large:
            \begin{equation*}
                \mathbf{x}^{(k+1)} = \frac{\mathbf{y}^{(k+1)}}{\left\|\mathbf{y}^{(k+1)}\right\|}
            \end{equation*}
            \item Approximate the eigenvalue:
            \begin{equation*}
                \lambda^{(k+1)} = \left[\mathbf{x}^{\left(k+1\right)}\right]^{H} A \mathbf{x}^{\left(k+1\right)}
            \end{equation*}
        \end{enumerate}

        \item Repeat until we meet a specific stopping criteria, for example:
        \begin{equation*}
            \left|\lambda^{(k+1)} - \lambda^{(k)}\right| < \varepsilon
        \end{equation*}
    \end{enumerate}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{State the main theoretical result.}}
    
    \textbf{Answer}: 
    \begin{itemize}
        \item \textbf{\emph{Convergence}}. The inverse power method converges to the eigenvalue of $A$ with the smallest absolute value. This is because applying the power method to $A^{-1}$ targets the largest eigenvalue of $A^{-1}$, which corresponds to the smallest eigenvalue of $A$.
        
        \item \textbf{\emph{Rate of Convergence}}. The rate of convergence of the inverse power method depends on the ratio of the second smallest eigenvalue to the smallest eigenvalue.
        
        If $\lambda_{1}$ is the smallest eigenvalue and $\lambda_{2}$ is the second smallest eigenvalue in magnitude, the error in the $k$-th iteration decreases proportionally to:
        \begin{equation*}
            \left( \frac{\lambda_{1}}{\lambda_{2}} \right)^{k}
        \end{equation*}
        A larger gap between the smallest and the second smallest eigenvalues results in faster convergence.

        Faster convergence when:
        \begin{equation*}
            \dfrac{\left|\lambda_{1}\right|}{\left|\lambda_{2}\right|} \ll 1
        \end{equation*}

        \item \textbf{\emph{Computational Cost}}. For the dense matrices it takes $n^{3}$ flops, while for the sparse matrices it takes only $n \cdot m$ flops (where $n$ is the number of rows in the square matrix, and $m$ is the number of non-zero elements).
    \end{itemize}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 2 - Laboratory}

The solution code snippet is available here:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/a1fed38529d02eaa59bd0025dc8ded68#file-exer2-cpp}
\end{center}
\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Let $A$ be a $100 \times 100$ tetradiagonal matrix defined such that:
    }
    \begin{equation*}
        \begin{pmatrix}
            8 & -4 & -1 & 0 & 0 & \cdots & 0 \\
            -2 & 8 & -4 & -1 & 0 & \cdots & 0 \\
            0 & -2 & 8 & -4 & -1 & \ddots & \vdots \\
            0 & 0 & \ddots & \ddots & \ddots & \ddots & 0 \\
            0 & \ddots & \ddots & \ddots & \ddots & \ddots & -1 \\
            \vdots & \vdots & \vdots & 0 & -2 & 8 & -4 \\
            0 & 0 & \cdots & 0 & 0 & -2 & 8
        \end{pmatrix}
    \end{equation*}
    \textbf{In a new file called \texttt{exer2.cpp}, define the matrix $A$ in the sparse format. Report $\left\| A \right\|$ on the \texttt{.txt} file, where $\left\| \cdot \right\|$ denotes the Euclidean norm.}}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <unsupported/Eigen/SparseExtra>

int main() {
    // Create matrix A 100x100
    constexpr int rows = 100;
    constexpr int cols = 100;
    Eigen::SparseMatrix<double> A(rows, cols);
    // Insert on the matrix A:
    // * 8 on the main diagonal
    // * -2 on the first subdiagonal
    // * -4 on the first superdiagonal
    // * -1 on the second superdiagonal
    //  8 -4 -1
    // -2  8 -4 -1
    //    -2  8 -4 -1
    //       -2  8 -4 -1
    for (int row = 0; row < rows; ++row) {
        A.insert(row, row) = 8;
        if (row > 0) A.insert(row, row-1) = -2;
        if (row < rows-1) A.insert(row, row+1) = -4;
        if (row < rows-2) A.insert(row, row+2) = -1;
    }
    // Make matrix A compressed
    A.makeCompressed();
    printf("Euclidean norm of A: %g\n", A.norm());
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Euclidean norm of A: 92.0761\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Solve the eigenvalue problem $A\mathbf{x} = \lambda\mathbf{x}$ using the proper solver provided by \texttt{Eigen}. Report on the \texttt{.txt} fle the computed smallest and largest (in modulus) eigenvalues of $A$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <unsupported/Eigen/SparseExtra>
#include <Eigen/Dense>

int main() {
    // ...
    // Check if the matrix is symmetric to use the correct eigenvalue solver
    const Eigen::SparseMatrix<double> A_transpose = A.transpose();
    const Eigen::SparseMatrix<double> B = A_transpose - A;
    const bool is_symmetric = B.norm() < 1e-10;
    printf("Norm of A.t - A: %g\n", B.norm());
    printf("The matrix A is %s\n", is_symmetric ? "symmetric" : "not symmetric");
    // Copy the original matrix
    const auto A_sparse_copy = Eigen::SparseMatrix(A);
    // Found the largest/smallest eigenvalue of A
    if (is_symmetric) {
        const Eigen::SelfAdjointEigenSolver<Eigen::SparseMatrix<double>> eigen_solver(A_sparse_copy);
        if (eigen_solver.info() != Eigen::Success) {
            printf("Eigen solver failed\n");
            return 1;
        }
        printf("Largest eigenvalue of A: %g\n", eigen_solver.eigenvalues()[rows-1]);
        printf("Smallest eigenvalue of A: %g\n", eigen_solver.eigenvalues()[0]);
    } else {
        const Eigen::EigenSolver<Eigen::MatrixXd> eigen_solver(A_sparse_copy.toDense());
        if (eigen_solver.info() != Eigen::Success) {
            printf("Eigen solver failed\n");
            return 1;
        }
        // Dense matrix, 1x100, of complex numbers
        const Eigen::Matrix<std::complex<double>, -1, 1>& eigenvalues = eigen_solver.eigenvalues();
        printf("Largest eigenvalue of A: %g\n", eigenvalues[rows-1].real());
        printf("Smallest eigenvalue of A: %g\n", eigenvalues[0].real());
    }
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of A.t - A: 31.4325
The matrix A is not symmetric
Largest eigenvalue of A: 11.1048
Smallest eigenvalue of A: 1.91332\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Using the \texttt{unsupported/Eigen/SparseExtra} module, export matrix $A$ in the matrix market format (save as \texttt{Aex2.mtx}) and move it to the folder \texttt{lis-2.0.34/test}. Using the proper iterative solver available in the LIS library compute the largest eigenvalue $\lambda_{\max}$ of $A$ up to a tolerance of $10^{-7}$. Report on the \texttt{.txt} file the computed eigenvalue and the number of iterations required to achieve the prescribed tolerance.
    }}

    \textbf{Answer}: We export the matrix:
    \begin{lstlisting}[language=C++]
// Save the matrix A to a file
saveMarket(A, "Aex2.mtx");
printf("Matrix A saved to Aex2.mtx\n");\end{lstlisting}
    We \href{https://www.ssisc.org/lis/dl/lis-2.1.6.zip}{download} and unzip into the lis folder. Then we move the matrix to the test folder. After have compiled \texttt{eigen1.c}, we run the command:
    \begin{lstlisting}[mathescape=false]
$ ./eigen1 Aex2.mtx eigvec.txt hist.txt -e pi -etol 1.e-7 -emaxiter 50000

number of processes = 1
matrix size = 100 x 100 (396 nonzero entries)

initial vector x      : all components set to 1
precision             : double
eigensolver           : Power
convergence condition : ||lx-(B^-1)Ax||_2 <= 1.0e-07 * ||lx||_2
matrix storage format : CSR
shift                 : 0.000000e+00
eigensolver status    : normal end

Power: mode number          = 0
Power: eigenvalue           = 1.299901e+01
Power: number of iterations = 39800
Power: elapsed time         = 3.054283e-02 sec.
Power:   preconditioner     = 0.000000e+00 sec.
Power:     matrix creation  = 0.000000e+00 sec.
Power:   linear solver      = 0.000000e+00 sec.
Power: relative residual    = 9.997934e-08\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Find a shift $\mu \in \mathbb{R}$ yielding an acceleration of the previous eigensolver. Report $\mu$ and the number of iterations required to achieve a tolerance of $10^{-7}$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[mathescape=false]
$ ./eigen1 Aex2.mtx eigvec.txt hist.txt -e pi -etol 1.e-7 -emaxiter 50000 -shift 7.0

number of processes = 1
matrix size = 100 x 100 (396 nonzero entries)

initial vector x      : all components set to 1
precision             : double
eigensolver           : Power
convergence condition : ||lx-(B^-1)Ax||_2 <= 1.0e-07 * ||lx||_2
matrix storage format : CSR
shift                 : 7.000000e+00
eigensolver status    : normal end

Power: mode number          = 0
Power: eigenvalue           = 1.299901e+01
Power: number of iterations = 19920
Power: elapsed time         = 1.569868e-02 sec.
Power:   preconditioner     = 0.000000e+00 sec.
Power:     matrix creation  = 0.000000e+00 sec.
Power:   linear solver      = 0.000000e+00 sec.
Power: relative residual    = 9.997483e-08\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Using the proper iterative solver available in the LIS library compute the smallest eigenvalue $\lambda_{\min}$ of $A$ up to a tolerance of $10^{-7}$. Report the computed eigenvalue and the number of iterations required to achieve the prescribed tolerance.
    }}
    
    \textbf{Answer}:
    \begin{lstlisting}[mathescape=false]
$ ./eigen1 Aex2.mtx eigvec.txt hist.txt -e ii -etol 1.e-7 -emaxiter 50000

number of processes = 1
matrix size = 100 x 100 (396 nonzero entries)

initial vector x      : all components set to 1
precision             : double
eigensolver           : Inverse
convergence condition : ||lx-(B^-1)Ax||_2 <= 1.0e-07 * ||lx||_2
matrix storage format : CSR
shift                 : 0.000000e+00
linear solver         : BiCG
preconditioner        : none
eigensolver status    : normal end

Inverse: mode number          = 0
Inverse: eigenvalue           = 1.913297e+00
Inverse: number of iterations = 1348
Inverse: elapsed time         = 2.169116e-02 sec.
Inverse:   preconditioner     = 1.313231e-03 sec.
Inverse:     matrix creation  = 2.978600e-05 sec.
Inverse:   linear solver      = 1.501240e-02 sec.
Inverse: relative residual    = 9.946932e-08\end{lstlisting}
\end{enumerate}