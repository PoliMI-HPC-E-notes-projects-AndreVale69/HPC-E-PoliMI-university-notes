\subsection{Year 2024}

\subsubsection{July 03}

\subsubsection*{Exercise 1 - Theory}

\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Consider the following problem: find $\mathbf{x} \in \mathbb{R}^{n}$, $A\mathbf{x} = \mathbf{b}$, where $A \in \mathbb{R}^{n \times n}$ and $\mathbf{b} \in \mathbb{R}^{n}$ are given. State under which conditions the mathematical problem is well posed.%
    }}

    \textbf{Answer}: To determine when the problem of finding $\mathbf{x} \in \mathbb{R}^{n}$ such that $A\mathbf{x} = \mathbf{b}$ (where $A \in \mathbb{R}^{n \times n}$ and $\mathbf{b} \in \mathbb{R}^n$ are given) is well-posed, we need to consider the following conditions:
    \begin{enumerate}
        \item \emph{\textbf{Existence}}: There must be \textbf{at least one solution} to the equation $A\mathbf{x} = \mathbf{b}$. For this to happen, the vector $\mathbf{b}$ should be in the column space of $A$, which means $A$ \textbf{must have full rank}, i.e., $\mathrm{rank}\left(A\right) = n$.
        
        \item \emph{\textbf{Uniqueness}}: There must be at most one solution. For the solution to be unique, the matrix $A$ must be invertible. This is equivalent to saying that $A$ has \textbf{full rank and its determinant is non-zero} ($\det(A) \neq 0$).
        
        \item \emph{\textbf{Stability}}: The solution should change continuously with changes in $\mathbf{b}$. In other words, the problem should be well-conditioned. The \textbf{condition number of $A$ should be reasonably small} to ensure that small changes in $\mathbf{b}$ do not cause large changes in the solution $\mathbf{x}$.
    \end{enumerate}
    Thus, the mathematical problem is well-posed if:
    \begin{itemize}
        \item The matrix $A$ is \textbf{square};
        \item The matrix $A$ is \textbf{invertible};
        \item The matrix $A$ has a \textbf{reasonably low condition number}, ensuring the existence, uniqueness, and stability of the solution. 
    \end{itemize}
    \begin{tipbox}
        To remember when a mathematical problem is well posed, it can be helpful to remember that:
        \begin{itemize}
            \item A solution must exist, otherwise we have a problem not well posed, simple.
            \item Ok, if the solution exists, how many? We mean, at least one solution should exist, right? So the existence of the solution has to be guaranteed. This can be achieved by taking advantage of the full rank of the matrix. But at this point the student's question should be: \dquotes{\emph{Why the full rank?}}. Well, the answer is simple: invertible. If the matrix is invertible, we have the guarantee that there exists an inverse matrix such that $A A^{-1} = I$, where $I$ is the identity matrix. This also tells us that the equation $A\mathbf{x} = \mathbf{b}$ has a unique solution given by $\mathbf{x} = A^{-1}\mathbf{b}$.
            \item Ok, but a system with $n$ solutions is not much information. Yes, it is interesting to know that we can find almost one solution when we try to solve the system, but it should be very helpful to know that where we get the solution, it is unique. Again, \dquotes{\emph{why do we need to compute the determinant?}}. A partial answer comes from the previous point, if the matrix is invertible, we have the mathematical guarantee that the determinant is not zero, therefore the inverse matrix exists and is uniqueness. Finally, an interesting implication can be observed:
            \begin{equation*}
                \mathrm{rank}\left(A\right) = n \: \Rightarrow \: \text{invertible} \: \Rightarrow \det\left(A\right) \ne 0
            \end{equation*}
            \item Finally, the mathematical problem is well-posed if the condition number is small, so that the small changes in $\mathbf{b}$ do not cause large changes in the solution $\mathbf{x}$.
        \end{itemize}
    \end{tipbox}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage


    \item \textcolor{Green3}{\textbf{%
        Describe the Cholesky factorization and how it is used to approximately solve the above linear system.%
    }}

    \textbf{Answer}: Cholesky factorization is a numerical method for decomposing a symmetric, positive-definite (SPD) matrix $A$ into a product of a lower triangular matrix $L$ and its transpose $L^{T}$. Specifically, if $A$ is symmetric and positive-definite, then there exists a unique lower triangular matrix $L$ such that:
    \begin{equation*}
        A = LL^{T}
    \end{equation*}
    We also provide the algorithm, but it might not be necessary. However, the algorithm is:
    \begin{itemize}
        \item Start with an empty lower triangular matrix $L$.
        \item For $i = 1$ to $n$ (where $n$ is the dimension of the matrix $A$):
        \begin{itemize}
            \item Compute the diagonal element:
            \begin{equation*}
                L_{ii} = \sqrt{A_{ii} - \sum_{k=1}^{i-1} L_{ik}^2}
            \end{equation*}
            \item For $ j = i+1 $ to $ n $:
            \begin{itemize}
                \item Compute the off-diagonal elements:
                \begin{equation*}
                    L_{ji} = \frac{1}{L_{ii}} \left( A_{ji} - \sum_{k=1}^{i-1} L_{jk} L_{ik} \right)
                \end{equation*}
            \end{itemize}
        \end{itemize}
    \end{itemize}

    About the previous and classical linear system $A\mathbf{x} = \mathbf{b}$, we can solve it using the Cholesky factorization:
    \begin{enumerate}
        \item Decompose $A$ as $LL^T$ using Cholesky factorization.
        \item Since $L$ is lower triangular, use \textbf{forward substitution}. Solve the triangular system $L\mathbf{y} = \mathbf{b}$ for $\mathbf{y}$.
        \begin{equation*}
            y_{i} = \dfrac{b_{i} - \displaystyle\sum_{j=1}^{i-1} L_{ij} y_{j}}{L_{ii}}
        \end{equation*}
        \item Since $L^{T}$ is upper triangular, use \textbf{backward substitution}. Solve the triangular system $L^{T}\mathbf{x} = \mathbf{y}$ for $\mathbf{x}$.
        \begin{equation*}
            x_{i} = \dfrac{y_{i} - \displaystyle\sum_{j=i+1}^{n} L_{ji} x_{j}}{L_{ii}}
        \end{equation*}
    \end{enumerate}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        State under which conditions the Cholesky factorization can be use.%
    }}

    \textbf{Answer}: Cholesky factorization can be used under the following conditions:
    \begin{enumerate}
        \item \textbf{\emph{Symmetric Matrix}}: The matrix $A$ must be symmetric, meaning $A = A^T$. This ensures that the matrix is equal to its transpose and guarantees two important properties:
        \begin{itemize}
            \item Symmetric matrices are required because Cholesky factorization is designed to work with positive-definite matrices. A symmetric matrix that is also positive-definite ensures that all eigenvalues are positive, making the square root operation (used in the factorization) valid and yielding real numbers. This property is crucial for the numerical stability and accuracy of the factorization.
            \item The symmetry of the matrix guarantees that the Cholesky factorization, if it exists, is unique. This uniqueness means that for a given symmetric, positive-definite matrix $A$, there is exactly one lower triangular matrix $L$ such that $A = LL^{T}$.
        \end{itemize}

        \item \textbf{\emph{Positive-Definite Matrix}}: The matrix $A$ must be positive definite. This means that for any non-zero vector $\mathbf{y} \in \mathbb{R}^n$, $\mathbf{y}^T A \mathbf{y} > 0$. Positive definiteness ensures that all eigenvalues of $A$ are positive.
    \end{enumerate}
    When these two conditions are met, the Cholesky factorization decomposes $A$ into a product of a lower triangular matrix $L$ and its transpose $L^T$ (i.e., $A = LL^T$).

    If these conditions are not met, Cholesky factorization is not applicable, and other factorization methods like LU decomposition might be used instead.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Report the main theoretical result and comment on the main diﬀerences with respect to the LU factorization.%
    }}

    \textbf{Answer}: The main differences between Cholesky factorization and LU factorization are
    \begin{itemize}
        \item \textbf{\emph{Matrix Requirements}}.
        \begin{itemize}
            \item \textbf{Cholesky}: Requires the matrix $A$ to be symmetric and positive-definite (SPD). This means all eigenvalues of $A$ must be positive.
            \item \textbf{LU}: Can be applied to any square matrix $A$ (even not SPD). However, partial pivoting may be necessary for numerical stability, but it is not a requirement!
        \end{itemize}

        \item \textbf{\emph{Factorization Form}}.
        \begin{itemize}
            \item \textbf{Cholesky}: Decomposes $A$ into $LL^{T}$, where $L$ is a lower triangular matrix and $A^{T}$ is an upper triangular matrix.
            \item \textbf{LU}: Decomposes $A$ into $LU$, where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix.
        \end{itemize}

        \item \textbf{\emph{Complexity and Efficiency}}.
        \begin{itemize}
            \item \textbf{Cholesky}: More efficient for SPD matrices as it exploits symmetry, leading to approximately half the computational effort compared to LU factorization. Specifically, Cholesky factorization has a time complexity of $O\left(\frac{n^{3}}{3}\right)$.
            \item \textbf{LU}: More general and can handle more matrix types, but typically requires more computational effort, with a time complexity of $O\left(2 \cdot \frac{n^{3}}{3}\right)$ when performed with partial pivoting.
        \end{itemize}

        \item \textbf{\emph{Stability}}.
        \begin{itemize}
            \item \textbf{Cholesky}: Numerically stable for SPD matrices. It avoids the potential pitfalls\footnote{%
                In the context of numerical linear algebra, \definitionWithSpecificIndex{pitfalls}{Pitfalls} refer to potential problems or challenges that can arise during the factorization process. These pitfalls can lead to numerical instability or inaccuracies in the computed solutions. Specifically, for LU factorization, pitfalls include issues like division by small numbers, round-off errors, ill-conditioned matrices, and zero or near-zero pivot elements. Pivoting strategies are often employed to mitigate these pitfalls and improve the robustness of the factorization.
            } of large pivot elements that can arise in LU factorization.
            \item \textbf{LU}: May require partial or full pivoting to ensure numerical stability, particularly for matrices that are not well-conditioned.
        \end{itemize}
    \end{itemize}
    In summary, Cholesky factorization is a specialized, efficient method for symmetric, positive-definite (SPD) matrices, while LU factorization is a more general approach applicable to a broader range of matrices, but potentially less efficient and stable without pivoting.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 1 - Laboratory}

The exam text provides the following matrix:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-aex1-mtx}
\end{center}
The solution code snippet is available here:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-exer1-cpp}
\end{center}
\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Download the sparse matrix \texttt{Aex1.mtx} from the Exam folder and save it. Load the matrix in a new ﬁle \texttt{exer1.cpp}. Report on the sheet the matrix size and the Euclidean norm of \texttt{Aex1.mtx}. Is the matrix symmetric?%
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <Eigen/Sparse>
#include <unsupported/Eigen/SparseExtra>

int main() {
    // Load the matrix
    Eigen::SparseMatrix<double> A;
    loadMarket(A, "Aex1.mtx");

    // Matrix size
    printf("Matrix size: %ldX%ld\n", A.rows(), A.cols());
    // The Euclidean norm is calculated by subtracting the
    // transpose of A from A;
    // This is done to check also if the matrix is symmetric
    // since A = A.t if it is symmetric;
    // However it can be calculated directly by A.norm()
    // if we want to check the magnitude of the matrix
    const Eigen::SparseMatrix<double> A_transpose = Eigen::SparseMatrix<double>(A.transpose());
    const Eigen::SparseMatrix<double> B = A_transpose - A;
    printf("The Euclidean norm of A-A.t: %g\n", B.norm());
    printf("The Euclidean norm of A: %g\n", A.norm());
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Matrix size: 256X256
The Euclidean norm of A-A.t: 5.24291e-15
The Euclidean norm of A: 117.201\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage


    \item \textcolor{Green3}{\textbf{%
        Define an \texttt{Eigen} vector $\mathbf{x}^{*} = \left(1, 0, 1, 0, \dots, 0\right)^{T}$ with size equal to the number of columns of \texttt{Aex1.mtx}. Compute $\mathbf{b} = Ax^{*}$ and report on the sheet $\left|\left|\mathbf{b}\right|\right|$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Create a row vector x star: 1,0,1,0,...,0
// Use ColMajor to characterize the vector as a column vector
// (since the exercise asks for a transpose)
Eigen::SparseVector<double, Eigen::ColMajor> x_star(A.cols());
for (int i = 0; i < A.cols() / 2; i++) {
    // start with 0 and put 1.0 every 2 steps
    x_star.insert(2 * i) = 1.0;
}
// Compute b = Ax*
const Eigen::VectorXd b = A * x_star;
printf("Norm of b: %g\n", b.norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of b: 51.2307\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Solve the linear system $A\mathbf{x} = \mathbf{b}$ using the Cholesky decomposition method available in the Eigen library. Report on the sheet the norm of the absolute error.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Create the Cholesky decomposition solver
Eigen::SimplicialLDLT<Eigen::SparseMatrix<double>> choleskySolver;
// Try to compute the Cholesky decomposition
choleskySolver.compute(A);
// Check if the decomposition was successful
if (choleskySolver.info() != Eigen::Success) {
    printf("Cannot factorize the matrix\n");
    return 0;
}
// The decomposition was successful, solve the system
Eigen::VectorXd x = choleskySolver.solve(b);
// Calculate the absolute error
printf("Solved the system Ax = b using Cholesky decomposition\n");
printf("The norm of the absolute error is: %g\n", (x - x_star).norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Solved the system Ax = b using Cholesky decomposition
The norm of the absolute error is: 4.43266e-14\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage


    \item \textcolor{Green3}{\textbf{%
        Solve the previous linear system using the Gradient method implemented in the \texttt{grad.hpp} template combined with the diagonal preconditioner provided by Eigen. Set a tolerance of $10^{-7}$ and take $\mathbf{x}_{0} = \left(0,0,\dots,0\right)^{T}$ as initial guess. Report on the sheet the iteration counts and the norm of the absolute error at the last iteration.
    }}

    The \texttt{grad.hpp} file can be found here:
    \begin{center}
        \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-grad-hpp}
    \end{center}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include "headers/grad.hpp"

int main() {
    /// ...
    // Set the tolerance, the maximum iterations (avoid infinite loop)
    double tol = 1.e-7; // 10^-7
    int max_iter = 5000;
    // Create an initial guess of zeros
    Eigen::VectorXd x_gradient = Eigen::VectorXd::Zero(A.cols());
    // Create the diagonal preconditioner
    Eigen::DiagonalPreconditioner<double> diagonalPreconditioner(A);
    // Solve the system using the Gradient method
    int result = LinearAlgebra::GRAD(A, x_gradient, b, diagonalPreconditioner, max_iter, tol);
    // The number of iterations performed is overwritten
    // by the function on the maxIterations variable
    printf("The Gradient method has converged? %s", result ? "No\n" : "Yes\n");
    printf("The number of iterations performed: %d\n", max_iter);
    // The solution is stored in x_gradient
    printf("The absolute error is: %g\n", (x_gradient - x_star).norm());
    // Finally, just for curiosity, print the tolerance achieved
    printf("The tolerance achieved is: %g\n", tol);
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
The Gradient method has converged? Yes
The number of iterations performed: 4788
The absolute error is: 0.000235242
The tolerance achieved is: 9.96454e-08\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Repeat the previous point using the Conjugate Gradient method implemented in the \texttt{cg.hpp} template combined with the diagonal preconditioner provided by Eigen. Set the solution obtained at the previous point as initial guess. Report the iteration counts (required to achieve a precision of $10^{-14}$) and the norm of the absolute error at the last iteration.
    }}

    The \texttt{cg.hpp} file can be found here:
    \begin{center}
        \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-cg-hpp}
    \end{center}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include "headers/cg.hpp"

int main() {
    // Set the tolerance,
    // the maximum iterations (avoid infinite loop)
    double tol_CG = 1.e-14; // 10^-14
    int max_iter_CG = 100;
    // Create the diagonal preconditioner
    Eigen::DiagonalPreconditioner<double> diagonalPreconditioner_CG(A);
    // Solve the system using the Conjugate Gradient method
    // We set the previous solution as the initial guess
    int result_CG = LinearAlgebra::CG(A, x_gradient, b, diagonalPreconditioner_CG, max_iter_CG, tol_CG);
    // The number of iterations performed is overwritten
    // by the function on the maxIterations variable
    printf("The Conjugate Gradient method has converged? %s", result_CG ? "No\n" : "Yes\n");
    printf("The number of iterations performed: %d\n", max_iter_CG);
    // The solution is stored in x_CG
    printf("The absolute error is: %g\n", (x_gradient - x_star).norm());
    // Finally, just for curiosity, print the tolerance achieved
    printf("The tolerance achieved is: %g\n", tol_CG);
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
The Conjugate Gradient method has converged? Yes
The number of iterations performed: 7
The absolute error is: 2.8326e-13
The tolerance achieved is: 8.94393e-15\end{lstlisting}
\end{enumerate}