\subsection{Year 2024}

\subsubsection{July 03}

\subsubsection*{Exercise 1 - Theory}

\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \hqlabel{exam: when a linear system is mathematically well posed}{
        \textcolor{Green3}{\textbf{%
            Consider the following problem: find $\mathbf{x} \in \mathbb{R}^{n}$, $A\mathbf{x} = \mathbf{b}$, where $A \in \mathbb{R}^{n \times n}$ and $\mathbf{b} \in \mathbb{R}^{n}$ are given. State under which conditions the mathematical problem is well posed.%
        }}
    }

    \textbf{Answer}: To determine when the problem of finding $\mathbf{x} \in \mathbb{R}^{n}$ such that $A\mathbf{x} = \mathbf{b}$ (where $A \in \mathbb{R}^{n \times n}$ and $\mathbf{b} \in \mathbb{R}^n$ are given) is well-posed, we need to consider the following conditions:
    \begin{enumerate}
        \item \emph{\textbf{Existence}}: There must be \textbf{at least one solution} to the equation $A\mathbf{x} = \mathbf{b}$. For this to happen, the vector $\mathbf{b}$ should be in the column space of $A$, which means $A$ \textbf{must have full rank}, i.e., $\mathrm{rank}\left(A\right) = n$.
        
        \item \emph{\textbf{Uniqueness}}: There must be at most one solution. For the solution to be unique, the matrix $A$ must be invertible. This is equivalent to saying that $A$ has \textbf{full rank and its determinant is non-zero} ($\det(A) \neq 0$).
        
        \item \emph{\textbf{Stability}}: The solution should change continuously with changes in $\mathbf{b}$. In other words, the problem should be well-conditioned. The \textbf{condition number of $A$ should be reasonably small} to ensure that small changes in $\mathbf{b}$ do not cause large changes in the solution $\mathbf{x}$.
    \end{enumerate}
    Thus, the mathematical problem is well-posed if:
    \begin{itemize}
        \item The matrix $A$ is \textbf{square};
        \item The matrix $A$ is \textbf{invertible};
        \item The matrix $A$ has a \textbf{reasonably low condition number}, ensuring the existence, uniqueness, and stability of the solution. 
    \end{itemize}
    \begin{tipbox}
        To remember when a mathematical problem is well posed, it can be helpful to remember that:
        \begin{itemize}
            \item A solution must exist, otherwise we have a problem not well posed, simple.
            \item Ok, if the solution exists, how many? We mean, at least one solution should exist, right? So the existence of the solution has to be guaranteed. This can be achieved by taking advantage of the full rank of the matrix. But at this point the student's question should be: \dquotes{\emph{Why the full rank?}}. Well, the answer is simple: invertible. If the matrix is invertible, we have the guarantee that there exists an inverse matrix such that $A A^{-1} = I$, where $I$ is the identity matrix. This also tells us that the equation $A\mathbf{x} = \mathbf{b}$ has a unique solution given by $\mathbf{x} = A^{-1}\mathbf{b}$.
            \item Ok, but a system with $n$ solutions is not much information. Yes, it is interesting to know that we can find almost one solution when we try to solve the system, but it should be very helpful to know that where we get the solution, it is unique. Again, \dquotes{\emph{why do we need to compute the determinant?}}. A partial answer comes from the previous point, if the matrix is invertible, we have the mathematical guarantee that the determinant is not zero, therefore the inverse matrix exists and is uniqueness. Finally, an interesting implication can be observed:
            \begin{equation*}
                \mathrm{rank}\left(A\right) = n \: \Rightarrow \: \text{invertible} \: \Rightarrow \det\left(A\right) \ne 0
            \end{equation*}
            \item Finally, the mathematical problem is well-posed if the condition number is small, so that the small changes in $\mathbf{b}$ do not cause large changes in the solution $\mathbf{x}$.
        \end{itemize}
    \end{tipbox}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage


    \item \textcolor{Green3}{\textbf{%
        Describe the Cholesky factorization and how it is used to approximately solve the above linear system.%
    }}

    \textbf{Answer}: Cholesky factorization is a numerical method for decomposing a symmetric, positive-definite (SPD) matrix $A$ into a product of a lower triangular matrix $L$ and its transpose $L^{T}$. Specifically, if $A$ is symmetric and positive-definite, then there exists a unique lower triangular matrix $L$ such that:
    \begin{equation*}
        A = LL^{T}
    \end{equation*}
    We also provide the algorithm, but it might not be necessary. However, the algorithm is:
    \begin{itemize}
        \item Start with an empty lower triangular matrix $L$.
        \item For $i = 1$ to $n$ (where $n$ is the dimension of the matrix $A$):
        \begin{itemize}
            \item Compute the diagonal element:
            \begin{equation*}
                L_{ii} = \sqrt{A_{ii} - \sum_{k=1}^{i-1} L_{ik}^2}
            \end{equation*}
            \item For $ j = i+1 $ to $ n $:
            \begin{itemize}
                \item Compute the off-diagonal elements:
                \begin{equation*}
                    L_{ji} = \frac{1}{L_{ii}} \left( A_{ji} - \sum_{k=1}^{i-1} L_{jk} L_{ik} \right)
                \end{equation*}
            \end{itemize}
        \end{itemize}
    \end{itemize}

    About the previous and classical linear system $A\mathbf{x} = \mathbf{b}$, we can solve it using the Cholesky factorization:
    \begin{enumerate}
        \item Decompose $A$ as $LL^{T}$ using Cholesky factorization.
        \item Since $L$ is lower triangular, use \textbf{forward substitution}. Solve the triangular system $L\mathbf{y} = \mathbf{b}$ for $\mathbf{y}$.
        \begin{equation*}
            y_{i} = \dfrac{b_{i} - \displaystyle\sum_{j=1}^{i-1} L_{ij} y_{j}}{L_{ii}}
        \end{equation*}
        \item Since $L^{T}$ is upper triangular, use \textbf{backward substitution}. Solve the triangular system $L^{T}\mathbf{x} = \mathbf{y}$ for $\mathbf{x}$.
        \begin{equation*}
            x_{i} = \dfrac{y_{i} - \displaystyle\sum_{j=i+1}^{n} L_{ji} x_{j}}{L_{ii}}
        \end{equation*}
    \end{enumerate}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        State under which conditions the Cholesky factorization can be use.%
    }}

    \textbf{Answer}: Cholesky factorization can be used under the following conditions:
    \begin{enumerate}
        \item \textbf{\emph{Symmetric Matrix}}: The matrix $A$ must be symmetric, meaning $A = A^{T}$. This ensures that the matrix is equal to its transpose and guarantees two important properties:
        \begin{itemize}
            \item Symmetric matrices are required because Cholesky factorization is designed to work with positive-definite matrices. A symmetric matrix that is also positive-definite ensures that all eigenvalues are positive, making the square root operation (used in the factorization) valid and yielding real numbers. This property is crucial for the numerical stability and accuracy of the factorization.
            \item The symmetry of the matrix guarantees that the Cholesky factorization, if it exists, is unique. This uniqueness means that for a given symmetric, positive-definite matrix $A$, there is exactly one lower triangular matrix $L$ such that $A = LL^{T}$.
        \end{itemize}

        \item \textbf{\emph{Positive-Definite Matrix}}: The matrix $A$ must be positive definite. This means that for any non-zero vector $\mathbf{y} \in \mathbb{R}^n$, $\mathbf{y}^{T} A \mathbf{y} > 0$. Positive definiteness ensures that all eigenvalues of $A$ are positive.
    \end{enumerate}
    When these two conditions are met, the Cholesky factorization decomposes $A$ into a product of a lower triangular matrix $L$ and its transpose $L^{T}$ (i.e., $A = LL^{T}$).

    If these conditions are not met, Cholesky factorization is not applicable, and other factorization methods like LU decomposition might be used instead.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Report the main theoretical result and comment on the main diﬀerences with respect to the LU factorization.%
    }}

    \textbf{Answer}: The main differences between Cholesky factorization and LU factorization are
    \begin{itemize}
        \item \textbf{\emph{Matrix Requirements}}.
        \begin{itemize}
            \item \textbf{Cholesky}: Requires the matrix $A$ to be symmetric and positive-definite (SPD). This means all eigenvalues of $A$ must be positive.
            \item \textbf{LU}: Can be applied to any square matrix $A$ (even not SPD). However, partial pivoting may be necessary for numerical stability, but it is not a requirement!
        \end{itemize}

        \item \textbf{\emph{Factorization Form}}.
        \begin{itemize}
            \item \textbf{Cholesky}: Decomposes $A$ into $LL^{T}$, where $L$ is a lower triangular matrix and $A^{T}$ is an upper triangular matrix.
            \item \textbf{LU}: Decomposes $A$ into $LU$, where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix.
        \end{itemize}

        \item \textbf{\emph{Complexity and Efficiency}}.
        \begin{itemize}
            \item \textbf{Cholesky}: More efficient for SPD matrices as it exploits symmetry, leading to approximately half the computational effort compared to LU factorization. Specifically, Cholesky factorization has a time complexity of $O\left(\frac{n^{3}}{3}\right)$.
            \item \textbf{LU}: More general and can handle more matrix types, but typically requires more computational effort, with a time complexity of $O\left(2 \cdot \frac{n^{3}}{3}\right)$ when performed with partial pivoting.
        \end{itemize}

        \item \textbf{\emph{Stability}}.
        \begin{itemize}
            \item \textbf{Cholesky}: Numerically stable for SPD matrices. It avoids the potential pitfalls\footnote{%
                In the context of numerical linear algebra, \definitionWithSpecificIndex{pitfalls}{Pitfalls} refer to potential problems or challenges that can arise during the factorization process. These pitfalls can lead to numerical instability or inaccuracies in the computed solutions. Specifically, for LU factorization, pitfalls include issues like division by small numbers, round-off errors, ill-conditioned matrices, and zero or near-zero pivot elements. Pivoting strategies are often employed to mitigate these pitfalls and improve the robustness of the factorization.
            } of large pivot elements that can arise in LU factorization.
            \item \textbf{LU}: May require partial or full pivoting to ensure numerical stability, particularly for matrices that are not well-conditioned.
        \end{itemize}
    \end{itemize}
    In summary, Cholesky factorization is a specialized, efficient method for symmetric, positive-definite (SPD) matrices, while LU factorization is a more general approach applicable to a broader range of matrices, but potentially less efficient and stable without pivoting.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 1 - Laboratory}

The exam text provides the following matrix:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-aex1-mtx}
\end{center}
The solution code snippet is available here:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-exer1-cpp}
\end{center}
\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Download the sparse matrix \texttt{Aex1.mtx} from the Exam folder and save it. Load the matrix in a new ﬁle \texttt{exer1.cpp}. Report on the sheet the matrix size and the Euclidean norm of \texttt{Aex1.mtx}. Is the matrix symmetric?%
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <Eigen/Sparse>
#include <unsupported/Eigen/SparseExtra>

int main() {
    // Load the matrix
    Eigen::SparseMatrix<double> A;
    loadMarket(A, "Aex1.mtx");

    // Matrix size
    printf("Matrix size: %ldX%ld\n", A.rows(), A.cols());
    // The Euclidean norm is calculated by subtracting the
    // transpose of A from A;
    // This is done to check also if the matrix is symmetric
    // since A = A.t if it is symmetric;
    // However it can be calculated directly by A.norm()
    // if we want to check the magnitude of the matrix
    const Eigen::SparseMatrix<double> A_transpose = Eigen::SparseMatrix<double>(A.transpose());
    const Eigen::SparseMatrix<double> B = A_transpose - A;
    printf("The Euclidean norm of A-A.t: %g\n", B.norm());
    printf("The Euclidean norm of A: %g\n", A.norm());
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Matrix size: 256X256
The Euclidean norm of A-A.t: 5.24291e-15
The Euclidean norm of A: 117.201\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage


    \item \textcolor{Green3}{\textbf{%
        Define an \texttt{Eigen} vector $\mathbf{x}^{*} = \left(1, 0, 1, 0, \dots, 0\right)^{T}$ with size equal to the number of columns of \texttt{Aex1.mtx}. Compute $\mathbf{b} = Ax^{*}$ and report on the sheet $\left|\left|\mathbf{b}\right|\right|$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Create a row vector x star: 1,0,1,0,...,0
// Use ColMajor to characterize the vector as a column vector
// (since the exercise asks for a transpose)
Eigen::SparseVector<double, Eigen::ColMajor> x_star(A.cols());
for (int i = 0; i < A.cols() / 2; i++) {
    // start with 0 and put 1.0 every 2 steps
    x_star.insert(2 * i) = 1.0;
}
// Compute b = Ax*
const Eigen::VectorXd b = A * x_star;
printf("Norm of b: %g\n", b.norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of b: 51.2307\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Solve the linear system $A\mathbf{x} = \mathbf{b}$ using the Cholesky decomposition method available in the Eigen library. Report on the sheet the norm of the absolute error.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Create the Cholesky decomposition solver
Eigen::SimplicialLDLT<Eigen::SparseMatrix<double>> choleskySolver;
// Try to compute the Cholesky decomposition
choleskySolver.compute(A);
// Check if the decomposition was successful
if (choleskySolver.info() != Eigen::Success) {
    printf("Cannot factorize the matrix\n");
    return 0;
}
// The decomposition was successful, solve the system
Eigen::VectorXd x = choleskySolver.solve(b);
// Calculate the absolute error
printf("Solved the system Ax = b using Cholesky decomposition\n");
printf("The norm of the absolute error is: %g\n", (x - x_star).norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Solved the system Ax = b using Cholesky decomposition
The norm of the absolute error is: 4.43266e-14\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage


    \item \textcolor{Green3}{\textbf{%
        Solve the previous linear system using the Gradient method implemented in the \texttt{grad.hpp} template combined with the diagonal preconditioner provided by Eigen. Set a tolerance of $10^{-7}$ and take $\mathbf{x}_{0} = \left(0,0,\dots,0\right)^{T}$ as initial guess. Report on the sheet the iteration counts and the norm of the absolute error at the last iteration.
    }}

    The \texttt{grad.hpp} file can be found here:
    \begin{center}
        \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-grad-hpp}
    \end{center}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include "headers/grad.hpp"

int main() {
    /// ...
    // Set the tolerance, the maximum iterations (avoid infinite loop)
    double tol = 1.e-7; // 10^-7
    int max_iter = 5000;
    // Create an initial guess of zeros
    Eigen::VectorXd x_gradient = Eigen::VectorXd::Zero(A.cols());
    // Create the diagonal preconditioner
    Eigen::DiagonalPreconditioner<double> diagonalPreconditioner(A);
    // Solve the system using the Gradient method
    int result = LinearAlgebra::GRAD(A, x_gradient, b, diagonalPreconditioner, max_iter, tol);
    // The number of iterations performed is overwritten
    // by the function on the maxIterations variable
    printf("The Gradient method has converged? %s", result ? "No\n" : "Yes\n");
    printf("The number of iterations performed: %d\n", max_iter);
    // The solution is stored in x_gradient
    printf("The absolute error is: %g\n", (x_gradient - x_star).norm());
    // Finally, just for curiosity, print the tolerance achieved
    printf("The tolerance achieved is: %g\n", tol);
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
The Gradient method has converged? Yes
The number of iterations performed: 4788
The absolute error is: 0.000235242
The tolerance achieved is: 9.96454e-08\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Repeat the previous point using the Conjugate Gradient method implemented in the \texttt{cg.hpp} template combined with the diagonal preconditioner provided by Eigen. Set the solution obtained at the previous point as initial guess. Report the iteration counts (required to achieve a precision of $10^{-14}$) and the norm of the absolute error at the last iteration.
    }}

    The \texttt{cg.hpp} file can be found here:
    \begin{center}
        \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-cg-hpp}
    \end{center}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include "headers/cg.hpp"

int main() {
    // Set the tolerance,
    // the maximum iterations (avoid infinite loop)
    double tol_CG = 1.e-14; // 10^-14
    int max_iter_CG = 100;
    // Create the diagonal preconditioner
    Eigen::DiagonalPreconditioner<double> diagonalPreconditioner_CG(A);
    // Solve the system using the Conjugate Gradient method
    // We set the previous solution as the initial guess
    int result_CG = LinearAlgebra::CG(A, x_gradient, b, diagonalPreconditioner_CG, max_iter_CG, tol_CG);
    // The number of iterations performed is overwritten
    // by the function on the maxIterations variable
    printf("The Conjugate Gradient method has converged? %s", result_CG ? "No\n" : "Yes\n");
    printf("The number of iterations performed: %d\n", max_iter_CG);
    // The solution is stored in x_CG
    printf("The absolute error is: %g\n", (x_gradient - x_star).norm());
    // Finally, just for curiosity, print the tolerance achieved
    printf("The tolerance achieved is: %g\n", tol_CG);
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
The Conjugate Gradient method has converged? Yes
The number of iterations performed: 7
The absolute error is: 2.8326e-13
The tolerance achieved is: 8.94393e-15\end{lstlisting}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 2 - Theory}

\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Consider the rectangular linear system $A\mathbf{x} = \mathbf{b}$, where $A$ is an $m \times n$ matrix, $m \ge n$. Provide the definition of the solution in the least-square sense and state under which condition the problem is well-posed.%
    }}

    \textbf{Answer}: The solution method least-square finds an approximate solution of the overdetermined linear systems, by minimizing the sum of the squares of the residuals (difference between the left and the right sides of the equation):
    \begin{equation*}
        \Phi\left(\mathbf{x}^{*}\right) = \min_{\mathbf{y} \in \mathbb{R}^{n}}\Phi\left(\mathbf{y}\right)
    \end{equation*}
    Where:
    \begin{equation*}
        \Phi\left(\mathbf{y}\right) = \left\|A\mathbf{x} - \mathbf{b}\right\|_{2}^{2}
    \end{equation*}
    An overdetermined linear system is a system of linear equations in which there are more equations than unknowns.

    The solution $\mathbf{x}^{*}$ can be found by imposing the condition that the gradient of the function $\Phi\left(\cdot\right)$ must be equal to zero at $\mathbf{x}^{*}$:
    \begin{equation*}
        \nabla \Phi\left(\mathbf{y}\right) = 2A^{T}A\mathbf{y} - 2A^{T}\mathbf{b}
    \end{equation*}
    From which it follows that $\mathbf{x}^{*}$ must be the solution of the square system:
    \begin{equation*}
        A^{T} A \mathbf{x}^{*} = A^{T} \mathbf{b}
    \end{equation*}
    
    \emph{\textbf{Well-Posed Condition}}. The least-squares problem is well-posed if:
    \begin{itemize}
        \item The matrix $A \in \mathbb{R}$ has full column rank $\mathrm{rank}\left(A\right) = n$. This means that the matrix $A$ has a solution and it is unique.
    \end{itemize}
    The full rank is very important because it guarantees us that there is a least-square solution and it is unique.

    \begin{tipbox}
        It is not necessary to remember every equation. It may be enough to remember what the least squares method is (not necessary what an overdetermined linear system is), how to write mathematically and which is the system we have to solve to get the solution $\mathbf{x}^{*}$.

        \highspace
        About the gradient, it is enough to remember that starting from $\Phi\left(\mathbf{y}\right)$ and applying the gradient operator (which points in the direction of the maximum rate of increase of the function), we can find a system where we can get the vector solution $\mathbf{x}^{*}$. We also want to use the gradient because we want the solution to converge to zero as the rate of the function increases.

        \highspace
        Finally, the well-posed condition is only the full rank. Because it is an essential property to guarantee that we have found the best (and unique) solution applicable.
    \end{tipbox}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Describe the QR factorization of the rectangular matrix $A$.
    }}

    \textbf{Answer}: QR factorization is a decomposition of a matrix $A$ into a product of an orthogonal (or unitary) matrix $Q$ and an upper triangular matrix $R$.

    \textbf{\emph{Definition}}. Given an $m \times n$ matrix $A$ (with $m \ge n$), the QR factorization of $A$ is expressed as:
    \begin{equation*}
        A = QR
    \end{equation*}
    Where:
    \begin{itemize}
        \item $Q$ is an $m \times m$ orthogonal (or unitary) matrix ($Q^{T} Q = I$).
        \item $R$ is an $m \times n$ upper triangular matrix.
    \end{itemize}
    
    \textbf{\emph{Algorithm}}. The most basic QR decomposition method is the Classical Gram-Schmidt (CGS) process and the more stable version is the Modified Gram-Schmidt.
    \begin{itemize}
        \item Classical Gram-Schmidt (CGS) Process. Given the columns of $A$, $\mathbf{a}_{1}$, $\mathbf{a}_{2}, \dots,$ $\mathbf{a}_{n}$, the goal is to find orthogonal vectors $\mathbf{q}_{j}$.
        
        The CGS process involves subtracting projections of the current vector $\mathbf{a}_{j}$ onto all previously calculated orthogonal vectors:
        \begin{equation*}
            \mathbf{w}_{j} = \mathbf{a}_{j} - \displaystyle\sum_{k=1}^{j-1}\left(\overline{\mathbf{q}}_{k}^{T}\mathbf{a}_{j}\right)\mathbf{q}_{k}
        \end{equation*}
        Normalizing to get $\mathbf{q}_{j}$:
        \begin{equation*}
            \mathbf{q}_{j} = \dfrac{\mathbf{w}_{j}}{\left\|\mathbf{w}_{j}\right\|}
        \end{equation*}
        We obtain QR factorization with:
        \begin{equation*}
            r_{ij} = \overline{\mathbf{q}}_{i}^{T} \mathbf{a}_{j} \hspace{2em} i \ne j
        \end{equation*}

        \item Modified Gram-Schmidt (MGS) Process. The CGS method can be sensitive to rounding errors, making it numerically unstable for some applications. The modification involves using a different projection step, making the process more stable:
        \begin{equation*}
            r_{ij} = \overline{\mathbf{q}}_{i}^{T} \mathbf{w}_{j}
        \end{equation*}
    \end{itemize}
    
    \textbf{\emph{When is it used?}} QR factorization is used when:
    \begin{itemize}
        \item Solving linear systems.
        \item Least-Squares Problems.
    \end{itemize}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Discuss how the QR factorization can be employed to solve the above linear system in the least-square sense.
    }}

    \textbf{Answer}: We need to solve the rectangular linear system $A\mathbf{x} = \mathbf{b}$ in the least-squares sense.

    Given:
    \begin{itemize}
        \item $A$ is an $m \times n$ matrix with $m \ge n$.
        \item $\mathbf{b}$ is an $m$-dimensional vector.
    \end{itemize}
    
    The goal (least-square goal) is to find the vector $\mathbf{x}$ that minimizes the residual $\|A\mathbf{x} - \mathbf{b}\|_{2}^{2}$, which is the square Euclidean norm of the difference between $A\mathbf{x}$ and $\mathbf{b}$.
    
    \begin{enumerate}
        \item \textbf{\emph{QR Factorization}}. Perform QR factorization on matrix $A$:
        \[ A = QR \]
        Where:
        \begin{itemize}
            \item $Q$ is an $m \times m$ orthogonal (or unitary) matrix.
            \item $R$ is an $m \times n$ upper triangular matrix.
        \end{itemize}

        \item \textbf{\emph{Simplify the problem}}.
        \begin{itemize}
            \item Substitute $A$ in the linear system with $QR$:
            \begin{equation*}
                A\mathbf{x} = \mathbf{b} \: \Rightarrow \: QR\mathbf{x} = \mathbf{b}
            \end{equation*}
            \item Pre-multiply both sides by the transpose of $Q$ $\left(Q^{T}\right)$, to simplify:
            \begin{equation*}
                Q^{T} QR\mathbf{x} = Q^{T} \mathbf{b}
            \end{equation*}
            \item Since $Q^{T} Q = I$ (because $Q$ is invertible and orthogonal), this reduces to:
            \begin{equation*}
                R\mathbf{x} = Q^{T} \mathbf{b}
            \end{equation*}
        \end{itemize}

        \item \textbf{\emph{Solve the Triangular System}}. The problem now is to solve the upper triangular system:
        \[ R\mathbf{x} = Q^{T} \mathbf{b} \]
        Since $R$ is an upper triangular matrix, we can use back-substitution to solve for $\mathbf{x}$.
    \end{enumerate}
    In summary:
    \begin{itemize}
        \item \emph{QR Factorization} decomposes $A$ into $Q$ (orthogonal) and $R$ (upper triangular).
        \item \emph{Simplification}: We transform the problem into solving $R\mathbf{x} = Q^{T} \mathbf{b}$.
        \item We use \emph{back-substitution} to find $\mathbf{x}$.
    \end{itemize}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 2 - Laboratory}

The exam text provides the following matrix:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-aex2-mtx}
\end{center}
The solution code snippet is available here:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/18c6d0d8b94390a3787889207354eac0#file-exer2-cpp}
\end{center}
\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Download the matrix \texttt{Aex2.mtx} from the Exam folder and save it. Load the matrix in a new file called \texttt{exer2.cpp} using the \texttt{unsupported/Eigen/SparseExtra} module. Report on the sheet\break $\left\| A^{T} A \right\|$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <unsupported/Eigen/SparseExtra>

int main() {
    // Load matrix Aex2.mtx
    Eigen::SparseMatrix<double> A;
    loadMarket(A, "Aex2.mtx");

    // Check matrix properties (just to get an idea of the matrix)
    printf("Matrix size: %ldx%ld\n", A.rows(), A.cols());
    printf("Non zero entries: %ld\n", A.nonZeros());

    // Calculate A^T * A, that is called Gram matrix
    // Refs: https://en.wikipedia.org/wiki/Gram_matrix
    const Eigen::SparseMatrix<double> gram_matrix = A.transpose() * A;
    printf("Norm of A^T * A: %f\n", gram_matrix.norm());
    printf("Gram matrix size: %ldx%ld\n", gram_matrix.rows(), gram_matrix.cols());
    // Note, the Gram matrix is symmetric by definition
    printf("Gram Symmetry Proof: ");
    if (gram_matrix.isApprox(gram_matrix.transpose())) {
        printf("The Gram matrix is symmetric. QED\n");
    } else {
        printf("The matrix is not symmetric, so it is not a Gram matrix\n");
    }
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Matrix size: 800x400
Non zero entries: 8431
Norm of A^T * A: 335.009675
Gram matrix size: 400x400
Gram Symmetry Proof: The Gram matrix is symmetric. QED\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Define an \texttt{Eigen} vector $\mathbf{b} = \left(A^{T} A\right) \mathbf{x}^{*}$, where $\mathbf{x}^{*} = \left(1, 1, \dots, 1\right)^{T}$. Report on the sheet $\left\|\mathbf{b}\right\|$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Create a vector e with all elements equal to 1;
// The size of the vector is the number of columns in the Gram matrix,
// since we are multiplying it by the Gram matrix
const Eigen::VectorXd x_star = Eigen::VectorXd::Ones(gram_matrix.cols());
// Calculate b = (A^T * A) * x_star
const Eigen::VectorXd b = gram_matrix * x_star;
printf("The norm of b: %f\n", b.norm());
    \end{lstlisting}
    And the result is:
    \begin{lstlisting}
The norm of b: 113.941047\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Use the \texttt{SparseQR} solver available in the \texttt{Eigen} library to compute the approximate solution of the least-square problem associated to $A\mathbf{x} = A\mathbf{b}$. Report on the sheet the Euclidean norm of the residual $\left\| A\mathbf{x}_{\text{SQR}} - A\mathbf{b} \right\|$, where $\mathbf{x}_{\text{SQR}}$ is the obtained approximate solution.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Create the QR factorization solver
Eigen::SparseQR<Eigen::SparseMatrix<double>, Eigen::COLAMDOrdering<int>> qr_solver;
// Compute the QR factorization of the matrix A to solve the system of equations Ax = Ab
qr_solver.compute(A);
// Check if the factorization was successful
if (qr_solver.info() != Eigen::Success) {
    printf("Cannot factorize the matrix using QR factorization\n");
    return 0;
}
// Solve the system of equations using the QR factorization
const Eigen::VectorXd x_qr = qr_solver.solve(A * b);
// Print the norm of the difference between the solution obtained
// by the QR factorization and the expected solution
printf("\nSolution with Eigen QR:\n");
printf("Norm of the difference || Ax_SQR - Ab ||: %g\n", (x_qr - b).norm());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of the difference || Ax_SQR - Ab ||: 3.89145e-13\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Use the \texttt{LeastSquareConjugateGradient} solver available in the\break \texttt{Eigen} library to compute the approximate solution of the previous least-square problem up to a tolerance of $10^{10}$. Report on the sheet the iteration counts and the norm of the residual\break $\left\| A\mathbf{x}_{\text{LSCQ}} - A\mathbf{b} \right\|$, where $\mathbf{x}_{\text{LSCQ}}$ is the obtained approximate solution.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// We use the Least Squares Conjugate Gradient (LSCG) solver to solve the previous problem
// Define the convergence tolerance and the maximum number of iterations
double tol = 1e-10;
int max_iter = 1000;
// Create the LSCG solver
Eigen::LeastSquaresConjugateGradient<Eigen::SparseMatrix<double>> lscg_solver;
// Set the maximum number of iterations and the convergence tolerance
lscg_solver.setMaxIterations(max_iter);
lscg_solver.setTolerance(tol);
// Compute the LSCG solver
lscg_solver.compute(A);
// Check if the computation was successful (not necessary at all)
if (lscg_solver.info() != Eigen::Success) {
    printf("Cannot compute the LSCG solver\n");
    return 0;
}
// Solve the system of equations using the LSCG solver
const Eigen::VectorXd x_lscg = lscg_solver.solve(A * b);
// Print the number of iterations, the relative residual, and the norm of the difference
// between the solution obtained by the LSCG solver and the expected solution
printf("\nSolution with Eigen LSCG:\n");
printf("Number of iterations: %ld\n", lscg_solver.iterations());
printf("Relative residual: %g\n", lscg_solver.error());
printf("Norm of the difference || Ax_SQR - Ab ||: %g\n", (x_lscg - b).norm());
    \end{lstlisting}
    And the result is:
    \begin{lstlisting}
Solution with Eigen LSCG:
Number of iterations: 87
Relative residual: 9.88679e-11
Norm of the difference || Ax_SQR - Ab ||: 2.14369e-07\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Export the matrix $A^{T}A$ in the matrix market format (save it as \texttt{AtA.mtx}) and move it to the lis-2.0.34/test folder. Using the Conjugate Gradient iterative solver available in LIS compute the approximate solution of the linear systm $A^{T}A\mathbf{x} = \mathbf{b}$ up to a tolerance of $10^{-10}$. Report the iteration counts and the relative residual at the last iteration.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Save the Gram matrix to a file named AtA.mtx
saveMarket(gram_matrix, "AtA.mtx");\end{lstlisting}
    We \href{https://www.ssisc.org/lis/dl/lis-2.1.6.zip}{download} and unzip into the lis folder. Then we move the matrix to the test folder. After have compiled \texttt{test1.c}, we run the command:
    \begin{lstlisting}
./test1 AtA.mtx 2 sol.mtx hist.txt -i cg -tol 1.e-10\end{lstlisting}
    \begin{itemize}
        \item \texttt{AtA.mtx} is the input matrix.
        \item \texttt{2} uses $b = A \times \left(1, 1, \dots, 1\right)^{T}$.
        \item \texttt{sol.mtx} and \texttt{hist.txt} are the results.
        \item \texttt{-i cg} is the type of solver (Conjugate Gradient).
        \item \texttt{-tol 1.e-10} is the tolerance.
    \end{itemize}
    And the final result is:
    \begin{lstlisting}
number of processes = 1
matrix size = 400 x 400 (19590 nonzero entries)

initial vector x      : all components set to 0
precision             : double
linear solver         : CG
preconditioner        : none
convergence condition : ||b-Ax||_2 <= 1.0e-10 * ||b-Ax_0||_2
matrix storage format : CSR
linear solver status  : normal end

CG: number of iterations = 114
CG:   double             = 114
CG:   quad               = 0
CG: elapsed time         = 2.149641e-03 sec.
CG:   preconditioner     = 2.702900e-05 sec.
CG:     matrix creation  = 1.730000e-07 sec.
CG:   linear solver      = 2.122612e-03 sec.
CG: relative residual    = 8.993059e-11\end{lstlisting}
\end{enumerate}