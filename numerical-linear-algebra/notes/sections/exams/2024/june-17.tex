\subsubsection{June 17}

\subsubsection*{Exercise 1 - Theory}

\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Consider the following problem: find $\mathbf{x} \in \mathbb{R}^{n}$, $A\mathbf{x} = \mathbf{b}$, where $A \in \mathbb{R}^{n \times n}$ and $\mathbf{b} \in \mathbb{R}^{n}$ are given. State under which conditions the mathematical problem is well posed.
    }}

    \textbf{Answer}: page \hqpageref{exam: when a linear system is mathematically well posed}.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Describe the general form of a linear iterative method for the approximate solution of $A\mathbf{x} = \mathbf{b}$ and describe the stopping criteria.
    }}

    \textbf{Answer}: Linear iterative methods are used to find approximate solutions to the linear system $A\mathbf{x} = \mathbf{b}$. These methods generate a sequence of approximations that ideally converge to the exact solution.

    \textbf{\emph{Definition}}. A linear iterative method updates the approximation $\mathbf{x}^{(k)}$ at each step $k$ according to a specific rule. The general form of an iterative method can be written as:
    \begin{equation*}
        \mathbf{x}^{(k+1)} = B \mathbf{x}^{(k)} + \mathbf{f}
    \end{equation*}
    Where:
    \begin{itemize}
        \item $\mathbf{x}^{(k)}$ is the \emph{approximation of the solution} at iteration $k$.
        \item $B$ is the \emph{iteration matrix}.
        \item $\mathbf{f}$ is a component that identifies the \emph{selected method}.
    \end{itemize}

    \textbf{\emph{Stopping Criteria}}. The iterative process is stopped when one of the following criteria is met:
    \begin{itemize}
        \item \textbf{\emph{Maximum iterations}}. The simpler way. It is used to avoid an infinite loop in case the chosen method doesn't converge.

        \item \textbf{\emph{Residual Norm}}. Since the residual gets smaller as the solution gets closer to the exact answer, we stop the iteration method when it is small enough. This works because the residual essentially tracks the behavior of the error (usually small residual, small error).
        \begin{equation*}
            \dfrac{
                \left\| \mathbf{x} - \mathbf{x}^{\left(k\right)} \right\|
            }{
                \left\|\mathbf{x}^{\left(k\right)}\right\|
            }
            \le
            K\left(A\right) \cdot \dfrac{
                \left\| \mathbf{r}^{\left(k\right)} \right\|
            }{
                \left\| \mathbf{b} \right\|
            }
            \: \Rightarrow \:
            \dfrac{
                \left\| \mathbf{r}^{\left(k\right)} \right\|
            }{
                \left\| \mathbf{b} \right\|
            } \le \varepsilon
        \end{equation*}
        Where $K\left(A\right)$ is the condition number and $\mathbf{r}^{\left(k\right)}$ is the residual.

        \item \textbf{\emph{Distance between consecutive iterates criteria}}. It is a criterion that looks at how much the current iterate (solution) changes from the previous one. When this difference becomes small enough, it's a signal that the method is converging and can be stopped.
        \begin{equation*}
            \delta^{\left(k\right)} = \mathbf{x}^{\left(k+1\right)} - \mathbf{x}^{\left(k\right)} \: \Rightarrow \: \left\| \delta^{\left(k\right)} \right\| \le \varepsilon \: \Rightarrow \: \left\| \mathbf{x}^{\left(k+1\right)} - \mathbf{x}^{\left(k\right)} \right\| \le \varepsilon
        \end{equation*}
    \end{itemize}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        State the necessary and sufficient condition for convergence.
    }}

    \textbf{Answer}: The necessary and sufficient condition for convergence is given by the following theorem.
    
    \emph{A consistent iterative method with iteration matrix $B$ converges if and only if its spectral radius is less than 1: $\rho\left(B\right) < 1$.}

    The spectral radius of a matrix is the largest absolute value of its eigenvalues:
    \begin{equation*}
        \rho\left(B\right) = \underset{j}{\max} \left|\lambda_{j}\left(B\right)\right|
    \end{equation*}
    Where $\lambda_{j}\left(B\right)$ are the eigenvalues of $B$. Also, when the matrix $B$ is Singular Positive-Definite (SPD), the spectral radius is equal to the Euclidean norm:
    \begin{equation*}
        B \text{ is SPD } \Rightarrow \left\| B \right\|_{2} = \rho\left(B\right) \land \rho\left(B\right) < 1
    \end{equation*}
    This result is very helpful because it indicates that the influence of the matrix is well distributed.


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Describe the Jacobi iterative method. Recall the derivation of the scheme and the main theorical results.
    }}

    \textbf{Answer}: Given a system of linear equations $A\mathbf{x} = \mathbf{b}$, where $A$ is a square matrix, $\mathbf{x}$ is the vector of unknowns, and $\mathbf{b}$ is the result vector, the Jacobi method iterates to approximate the solution.

    \textbf{\emph{Algorithm}}
    \begin{enumerate}
        \item Start with an initial guess $\mathbf{x}^{\left(0\right)}$, also zero.
        \item Update each component:
        \begin{equation*}
            \mathbf{x}_{i}^{\left(k+1\right)} = \dfrac{
                b_{i} - \displaystyle\sum_{j \ne i} a_{ij}x_{j}^{\left(k\right)}
            }{a_{ii}} \hspace{2em} \forall i = 1, \dots, n
        \end{equation*}
        where $a_{ii}$ are the diagonal elements of $A$, and $a_{ij}$ are the off-diagonal elements.
        \item Repeat until we meet the stop criteria.
    \end{enumerate}

    \textbf{\emph{Theoretical Results}}
    \begin{itemize}
        \item \textbf{\emph{Convergence Condition}}. The Jacobi method converges if and only if the spectral radius $\rho(B)$ of the iteration matrix is less than 1: $\rho\left(B\right) < 1$.
        
        \item \textbf{\emph{Diagonal Dominance}}. A sufficient condition for the convergence of the Jacobi method is that the matrix $A$ is diagonally dominant. This means that for each row $i$:
        \begin{equation*}
            |a_{ii}| > \sum_{j \neq i} |a_{ij}|
        \end{equation*}

        \item \textbf{\emph{Convergence Rate}}. The convergence rate of the Jacobi method depends on the spectral radius of the iteration matrix. A smaller spectral radius results in faster convergence.
    \end{itemize}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 1 - Laboratory}

The exam text provides the following matrix:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/accf6936dbda5664018906c81a51fb2c#file-aex1-mtx}
\end{center}
The solution code snippet is available here:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/accf6936dbda5664018906c81a51fb2c#file-exer1-cpp}
\end{center}
\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Download the matrix \texttt{Aex1.mtx} from the Exam folder and save it. Load the matrix in a new Ô¨Åle called \texttt{exer1.cpp} using the \texttt{unsupported/Eigen/SparseExtra} module and check if the matrix is symmetric. Report on the sheet $\left\|A\right\|$ where $\left\|\cdot\right\|$ denotes the Euclidean norm.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <unsupported/Eigen/SparseExtra>

int main() {
    // Load the matrix
    Eigen::SparseMatrix<double> A;
    Eigen::loadMarket(A, "Aex1.mtx");
    // Matrix properties
    printf("Matrix size: %ldx%ld\n", A.rows(), A.cols());
    // Calculate the norm of the matrix
    const Eigen::SparseMatrix<double> A_t = A.transpose();
    const Eigen::SparseMatrix<double> B = A_t - A;
    printf("Euclidean norm of A: %g\n", A.norm());
    printf("Euclidean norm of A.t - A: %g\n", B.norm());
    // The matrix is symmetric if the norm of the skew-symmetric part
    // is approximately zero; we use a tolerance of 1.e-10;
    // This is because if the matrix is symmetric,
    // then A.t - A = 0 and the norm of the zero matrix is zero
    printf("The matrix is %s symmetric.\n", B.norm() > 1.e-10 ? "not" : "");
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Matrix size: 500x500
Euclidean norm of A: 105.193
Euclidean norm of A.t - A: 47.9613
The matrix is not symmetric.\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Define the \texttt{Eigen} vector $\mathbf{b} = \left(1, 1, \dots, 1\right)^{T}$ and find the approximate solution $\mathbf{x}_{j}$ of $A\mathbf{x} = \mathbf{b}$ using the Jacobi method (implemented in the \texttt{jacobi.hpp} template). Fix a maximum number of iterations which is sufficient to reduce the (relative) residual below than $10^{-5}$ and take $\mathbf{x}_{0} = \mathbf{b}$ as initial guess. Report on the sheet the iteration counts and $\left\| \mathbf{x}_{j} \right\|$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include "headers/jacobi.hpp"

int main() {
    // ...
    // Define the vector b
    const Eigen::VectorXd b = Eigen::VectorXd::Ones(A.rows());
    // Calculate the approximate solution using the Jacobi method
    // Define the tolerance and maximum number of iterations
    double tol = 1.e-5;
    int max_iter = 5000;
    // Create the Diagonal Preconditioner needed for the Jacobi method
    Eigen::DiagonalPreconditioner<double> D(A);
    // Create the vector x to store the approximate solution
    Eigen::VectorXd x = Eigen::VectorXd::Zero(A.rows());
    // Solve the system using the Jacobi method
    LinearAlgebra::Jacobi(A, x, b, D, max_iter, tol);
    // Print the results
    printf("- Jacobi method -\n");
    printf("Iterations count: %d\n", max_iter);
    printf("Norm of the solution: %g\n", x.norm());
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
- Jacobi method -
Iterations count: 13
Norm of the solution: 7.07625\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Compute the approximate solution $\mathbf{x}_{g}$ of $A\mathbf{x} = \mathbf{b}$ obtained using the BICGSTAB method available in \texttt{Eigen}. Fix a maximum number of iterations which is sufficient to reduce the residual below than $10^{-10}$ considering $\mathbf{x}_{j}$ (computed in the previous point) as initial guess. Use the \texttt{Eigen} diagonal preconditioner. Report the iteration counts and $\left\| \mathbf{x}_{j} - \mathbf{x}_{g} \right\|$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
// Calculate the approximate solution using the BiCGSTAB method
// Define the tolerance and maximum number of iterations
tol = 1.e-10;
max_iter = 500;
// Create the vector x to store the approximate solution
Eigen::VectorXd x_bicgstab = Eigen::VectorXd::Zero(A.rows());
// Solve the system using the BiCGSTAB method and set the preconditioner to the Diagonal Preconditioner
Eigen::BiCGSTAB<Eigen::SparseMatrix<double>, Eigen::DiagonalPreconditioner<double>> bi_cgstab(A);
bi_cgstab.setMaxIterations(max_iter);
bi_cgstab.setTolerance(tol);
x_bicgstab = bi_cgstab.solveWithGuess(b, x);
// Print the results
printf("- BiCGSTAB method -\n");
printf("Iterations count: %ld\n", bi_cgstab.iterations());
printf("Estimated error: %g\n", bi_cgstab.error());\end{lstlisting}
    And the result is:
    \begin{lstlisting}
- BiCGSTAB method -
Iterations count: 4
Estimated error: 3.62252e-12\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Repeat the previous point using the iterative solvers available in the LIS library. First, compute the approximate solution obtained with the Jacobi method prescribing a tolerance of $10^{-5}$. Then, using the BICGSTAB solver compute the approximate solution up to a tolerance of $10^{-10}$. Find a preconditioning strategy that yield a decrease in the number of required iterations with respect to the BICGSTAB method without preconditioning. Report on the sheet the iteration counts and the relative residual at the last iteration.
    }}

    \textbf{Answer}: We \href{https://www.ssisc.org/lis/dl/lis-2.1.6.zip}{download} and unzip into the lis folder. Then we move the matrix to the test folder. After have compiled \texttt{test1.c}, we run the command:
    \begin{lstlisting}[mathescape=false]
$ ./test1 Aex1.mtx 1 sol.mtx hist.txt -i jacobi -tol 1.e-5
number of processes = 1
matrix size = 500 x 500 (7708 nonzero entries)

initial vector x      : all components set to 0
precision             : double
linear solver         : Jacobi
preconditioner        : none
convergence condition : ||b-Ax||_2 <= 1.0e-05 * ||b-Ax_0||_2
matrix storage format : CSR
linear solver status  : normal end

Jacobi: number of iterations = 14
Jacobi:   double             = 14
Jacobi:   quad               = 0
Jacobi: elapsed time         = 1.407420e-04 sec.
Jacobi:   preconditioner     = 4.766000e-06 sec.
Jacobi:     matrix creation  = 2.720000e-07 sec.
Jacobi:   linear solver      = 1.359760e-04 sec.
Jacobi: relative residual    = 9.603771e-06\end{lstlisting}
    This is the Jacobi method with a tolerance of $10^{-5}$. Then we use the BICGSTAB solver:
    \begin{lstlisting}[mathescape=false]
$ ./test1 Aex1.mtx 1 sol.mtx hist.txt -i bicgstab -tol 1.e-10
number of processes = 1
matrix size = 500 x 500 (7708 nonzero entries)

initial vector x      : all components set to 0
precision             : double
linear solver         : BiCGSTAB
preconditioner        : none
convergence condition : ||b-Ax||_2 <= 1.0e-10 * ||b-Ax_0||_2
matrix storage format : CSR
linear solver status  : normal end

BiCGSTAB: number of iterations = 10
BiCGSTAB:   double             = 10
BiCGSTAB:   quad               = 0
BiCGSTAB: elapsed time         = 2.180690e-04 sec.
BiCGSTAB:   preconditioner     = 6.565000e-06 sec.
BiCGSTAB:     matrix creation  = 3.380000e-07 sec.
BiCGSTAB:   linear solver      = 2.115040e-04 sec.
BiCGSTAB: relative residual    = 5.109306e-11\end{lstlisting}
    \newpage
    The ILU preconditioner is one of the best, so we can try it right away:
    \begin{lstlisting}[mathescape=false]
$ ./test1 Aex1.mtx 1 sol.mtx hist.txt -i bicgstab -tol 1.e-10 -p ilu
number of processes = 1
matrix size = 500 x 500 (7708 nonzero entries)

initial vector x      : all components set to 0
precision             : double
linear solver         : BiCGSTAB
preconditioner        : ILU(0)
convergence condition : ||b-Ax||_2 <= 1.0e-10 * ||b-Ax_0||_2
matrix storage format : CSR
linear solver status  : normal end

BiCGSTAB: number of iterations = 4
BiCGSTAB:   double             = 4
BiCGSTAB:   quad               = 0
BiCGSTAB: elapsed time         = 2.838500e-04 sec.
BiCGSTAB:   preconditioner     = 1.208690e-04 sec.
BiCGSTAB:     matrix creation  = 9.980000e-07 sec.
BiCGSTAB:   linear solver      = 1.629810e-04 sec.
BiCGSTAB: relative residual    = 8.914148e-11\end{lstlisting}
    It is also interesting notice that if we use the fill-in technique with $k = 4$ (\texttt{ilu\_fill} parameter) we obtain a single iteration. Fill-in refers to the number of non-zero elements that are added to the lower and upper triangular matrices during the ILU factorization process.
    \begin{lstlisting}[mathescape=false]
$ ./test1 Aex1.mtx 1 sol.mtx hist.txt -i bicgstab -tol 1.e-10 -p ilu -ilu_fill 4
number of processes = 1
matrix size = 500 x 500 (7708 nonzero entries)

initial vector x      : all components set to 0
precision             : double
linear solver         : BiCGSTAB
preconditioner        : ILU(4)
convergence condition : ||b-Ax||_2 <= 1.0e-10 * ||b-Ax_0||_2
matrix storage format : CSR
linear solver status  : normal end

BiCGSTAB: number of iterations = 1
BiCGSTAB:   double             = 1
BiCGSTAB:   quad               = 0
BiCGSTAB: elapsed time         = 9.994100e-05 sec.
BiCGSTAB:   preconditioner     = 5.042200e-05 sec.
BiCGSTAB:     matrix creation  = 1.720000e-07 sec.
BiCGSTAB:   linear solver      = 4.951900e-05 sec.
BiCGSTAB: relative residual    = 1.978281e-11\end{lstlisting}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 2 - Theory}

\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Consider the following eigenvalue problem: $A\mathbf{x} = \lambda\mathbf{x}$, where $A \in \mathbb{R}^{n \times n}$ is given. Describe the power method for the numerical approximation of the largest in modulus eigenvalue of $A$. Introduce the notation, the algorithm, ad the applicability conditions.
    }}

    \textbf{Answer}: The power method is an iterative technique used to find the largest eigenvalue (in absolute value) of a matrix and its corresponding eigenvector.

    \textbf{\emph{Notation}}
    \begin{itemize}
        \item $A$: An $n \times n$ real matrix.
        \item $\mathbf{x}$: An eigenvector corresponding to the eigenvalue $\lambda$.
        \item $\lambda$: The eigenvalue associated with the eigenvector $\mathbf{x}$.
        \item $\mathbf{x}^{(k)}$: The approximation of the eigenvector at iteration $k$.
        \item $\lambda^{(k)}$: The approximation of the eigenvalue at iteration $k$.
    \end{itemize}
    
    \textbf{\emph{Algorithm}}
    \begin{enumerate}
        \item Start with an initial guess, a nonzero vector $\mathbf{x}^{\left(0\right)}$ such that its norm is one $\left\| \mathbf{x}^{\left(0\right)} \right\| = 1$.
        
        \item For $k \ge 0$:
        \begin{enumerate}
            \item Multiply the current vector by the matrix:
            \begin{equation*}
                \mathbf{y}^{\left(k+1\right)} = A\mathbf{x}^{\left(k\right)}
            \end{equation*}

            \item After each multiplication, normalize the vector to prevent it from becoming too large:
            \begin{equation*}
                \mathbf{x}^{\left(k+1\right)} = \dfrac{
                    \mathbf{y}^{\left(k+1\right)}
                }{
                    \left\| \mathbf{y}^{\left(k+1\right)} \right\|
                }
            \end{equation*}

            \item Approximate the eigenvalue (an estimate of the eigenvalue associated with the current eigenvector approximation):
            \begin{equation*}
                \lambda^{\left(k+1\right)} = \left[\mathbf{x}^{\left(k+1\right)}\right]^{H} A \mathbf{x}^{\left(k+1\right)}
            \end{equation*}
        \end{enumerate}

        \item Repeat until we meet a specific stopping criteria.
    \end{enumerate}
    
    \textbf{\emph{Applicability conditions}}
    \begin{itemize}
        \item \textbf{\emph{Distinct Dominant Eigenvalue}}: Convergence is guaranteed if the dominant eigenvalue $\lambda_{1}$ is unique and the largest in absolute value compared to other eigenvalues. The dominant eigenvalue must be well-separated from the other eigenvalues for rapid convergence.
        \item \textbf{\emph{Starting Vector}}: The initial vector $\mathbf{x}^{(0)}$ must have a non-zero component in the direction of the dominant eigenvector $\mathbf{x}_{1}$. Typically, a random initial vector suffices because it will almost always have a component in the direction of $\mathbf{x}_{1}$.
    \end{itemize}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        State the main theoretical results.
    }}

    \textbf{Answer}:
    \begin{itemize}
        \item Dominant Eigenvalue. The power method converges to the eigenvalue $\lambda_{1}$ with the largest absolute value (dominant eigenvalue) of the matrix $A$. The associated eigenvector $\mathbf{x}_{1}$ will also be obtained.
        \item Convergence Rate. The convergence rate of the power method depends on the ratio of the largest absolute eigenvalue $\left|\lambda_{1}\right|$ to the second largest absolute eigenvalue $\left|\lambda_{2}\right|$:
        \begin{itemize}
            \item $\dfrac{\left|\lambda_{2}\right|}{\left|\lambda_{1}\right|} \ll 1$, convergence rate high, the method converges quickly.
            \item $\dfrac{\left|\lambda_{2}\right|}{\left|\lambda_{1}\right|} \approx 1$, convergence rate low, the method converges slowly.
        \end{itemize}
    \end{itemize}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Comment of the computational costs.
    }}

    \textbf{Answer}: The computational cost of the power method is determined by several factors, including the size of the matrix \( A \), the number of iterations required to achieve convergence, and the operations performed during each iteration.

    At each iteration we need to perform a matrix-vector multiplication $A \mathbf{x}^{(k)}$ and this involves in:
    \begin{itemize}
        \item Dense matrix: $O\left(n^{2}\right)$.
        \item Sparse matrix: $O\left(m\right)$, where $m$ is the number of nonzero entries.
    \end{itemize}
    Also, normalization is required at each iteration. Therefore, the normalization and the division take $O\left(n\right)$ operations (both dense and sparse matrices). Finally, the number of iterations of the algorithm is $K$, and its value depends on how much accuracy we want and on the spectral gap (absolute value of the second largest eigenvalue divided by the absolute value of the largest eigenvalues).

    We can conclude that the complexity is:
    \begin{itemize}
        \item Dense matrix: $O\left(K \cdot \left(n^{2} + n\right)\right)$
        \item Sparse matrix: $O\left(K \cdot \left(m + n\right)\right)$
    \end{itemize}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsubsection*{Exercise 2 - Laboratory}

The solution code snippet is available here:
\begin{center}
    \qrcode{https://gist.github.com/AndreVale69/accf6936dbda5664018906c81a51fb2c#file-exer2-cpp}
\end{center}
\begin{enumerate}[label=\textcolor{Green3}{\textbf{\arabic*.}}]
    \item \textcolor{Green3}{\textbf{%
        Let $A$ be a $100 \times 100$ pentadiagonal matrix defined such that
    }}
    \begin{equation*}
        A = \begin{pmatrix}
            -8 & 3 & 1 & 0 & 0 & \cdots & 0 \\
             3 & -8 & 3 & 1 & 0 & \cdots & 0 \\
             1 & 3 & -8 & 3 & 1 & \ddots & \vdots \\
             0 & \ddots & \ddots & \ddots & \ddots & \cdots & 0 \\
            \vdots & \ddots & 1 & 3 & -8 & 3 & 1 \\
            \vdots & \vdots & 0 & 1 & 3 & -8 & 3 \\
            0 & 0 & \cdots & 0 & 1 & 3 & -8 \\
        \end{pmatrix}
    \end{equation*}
    \textcolor{Green3}{\textbf{In a new file called \texttt{exer2.cpp}, define the matrix $A$ in the sparse format. Report on the sheet $\mathbf{v}^{T} A \mathbf{v}$, where $\mathbf{v}$ is such that $v_{i} = -1$ for all $0 \le i < 100$.}}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <unsupported/Eigen/SparseExtra>

int main() {
    // Create a 100x100 sparse matrix
    Eigen::SparseMatrix<double> A(100, 100);
    const int n = static_cast<int>(A.rows());
    // Create a pentadiaonal matrix
    for (int row = 0; row < n; ++row) {
        A.insert(row, row) = -8;
        if(row > 0) A.coeffRef(row, row-1) = 3;
        if(row < n-1) A.coeffRef(row, row+1) = 3;
        if(row > 1) A.coeffRef(row, row-2) = 1;
        if(row < n-2) A.coeffRef(row, row+2) = 1;
    }
    // Create vector v composed of -1 from 1 to 100
    Eigen::VectorXd v = -Eigen::VectorXd::Ones(n);
    // Report the norm of v.t A v
    printf("Norm of v.t A v: %g\n", (v.transpose() * A * v).value());
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
Norm of v.t A v: -10\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Solve the eigenvalue problem $A\mathbf{x} = \lambda\mathbf{x}$ using the proper solver provided by \texttt{Eigen}. Report on the sheet the smallest and largest $\lambda_{\min} < \lambda_{\max}$ computed eigenvalues of $A$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[language=C++]
#include <Eigen/Dense>

int main() {
    // ...
    // Create a copy of A
    const Eigen::SparseMatrix<double> A_copy = Eigen::SparseMatrix(A);
    // Use selfadjoint eigen solver to compute the eigenvalues of A
    const Eigen::SelfAdjointEigenSolver<Eigen::SparseMatrix<double>> self_adjoint_eigen_solver(A_copy);
    // Check if the computation was successful
    if (self_adjoint_eigen_solver.info() != Eigen::Success) {
        printf("Error: Computation failed.");
        return 1;
    }
    // Print the eigenvalues of A
    const double lambda_min = self_adjoint_eigen_solver.eigenvalues()[0];
    const double lambda_max = self_adjoint_eigen_solver.eigenvalues()[n-1];
    printf("The minimum eigenvalue of A is: %g\n", lambda_min);
    printf("The maximum eigenvalue of A is: %g\n", lambda_max);
}\end{lstlisting}
    And the result is:
    \begin{lstlisting}
The minimum eigenvalue of A is: -12.2484
The maximum eigenvalue of A is: -0.00672505\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Using the \texttt{unsupported/Eigen/SparseExtra} module, export matrix $A$ in the matrix market format (save as \texttt{Aex2.mtx}) and move it to the folder \texttt{lis-2.0.34/test}. Using the proper iterative solver available in the LIS library compute the largest eigenvalue $\lambda_{\max}$ of $A$ up to a tolerance of $10^{-7}$. Report the computed eigenvalue and the number of iterations required to achieve the prescribed tolerance.
    }}

    \textbf{Answer}: We export the matrix:
    \begin{lstlisting}[language=C++]
// Export matrix
saveMarket(A, "./Aex2.mtx");\end{lstlisting}
    We \href{https://www.ssisc.org/lis/dl/lis-2.1.6.zip}{download} and unzip into the lis folder. Then we move the matrix to the test folder. After have compiled \texttt{eigen1.c}, we run the command:
    \begin{lstlisting}[mathescape=false]
$ ./eigen1 Aex2.mtx eigvec.txt hist.txt -e ii -emaxiter 25000 -etol 1.e-7

number of processes = 1
matrix size = 100 x 100 (494 nonzero entries)

initial vector x      : all components set to 1
precision             : double
eigensolver           : Inverse
convergence condition : ||lx-(B^-1)Ax||_2 <= 1.0e-07 * ||lx||_2
matrix storage format : CSR
shift                 : 0.000000e+00
linear solver         : BiCG
preconditioner        : none
eigensolver status    : normal end

Inverse: mode number          = 0
Inverse: eigenvalue           = -6.725050e-03
Inverse: number of iterations = 8
Inverse: elapsed time         = 4.748510e-04 sec.
Inverse:   preconditioner     = 3.319400e-05 sec.
Inverse:     matrix creation  = 2.200000e-07 sec.
Inverse:   linear solver      = 3.652310e-04 sec.
Inverse: relative residual    = 6.262051e-08\end{lstlisting}
    Since the eigenvalues are negative, we must use the inverse power method to find the largest eigenvalue (\texttt{-0.00672505}). Normally we use the inverse power method to find the minimum, but here the eigenvalues are negative!


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage

    \item \textcolor{Green3}{\textbf{%
        Using the proper iterative solver available in the LIS library compute the smallest eigenvalue $\lambda_{\min}$ of $A$ up to a tolerance of $10^{-7}$. Report the computed eigenvalue and the number of iterations required to achieve the prescribed tolerance.
    }}

    \textbf{Answer}: Since the eigenvalues are negative, we must use the power method to find the smallest eigenvalue:
    \begin{lstlisting}[mathescape=false]
$ ./eigen1 Aex2.mtx eigvec.txt hist.txt -e pi -emaxiter 25000 -etol 1.e-7

number of processes = 1
matrix size = 100 x 100 (494 nonzero entries)

initial vector x      : all components set to 1
precision             : double
eigensolver           : Power
convergence condition : ||lx-(B^-1)Ax||_2 <= 1.0e-07 * ||lx||_2
matrix storage format : CSR
shift                 : 0.000000e+00
eigensolver status    : normal end

Power: mode number          = 0
Power: eigenvalue           = -1.224829e+01
Power: number of iterations = 23732
Power: elapsed time         = 1.925408e-02 sec.
Power:   preconditioner     = 0.000000e+00 sec.
Power:     matrix creation  = 0.000000e+00 sec.
Power:   linear solver      = 0.000000e+00 sec.
Power: relative residual    = 9.996329e-08\end{lstlisting}


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \item \textcolor{Green3}{\textbf{%
        Find a shift $\mu \in \mathbb{R}$ yielding an acceleration of the previous eigensolver. Report $\mu$ and the number of iterations required to achieve a tolerance of $10^{-7}$.
    }}

    \textbf{Answer}:
    \begin{lstlisting}[mathescape=false]
$ ./eigen1 Aex2.mtx eigvec.txt hist.txt -e pi -emaxiter 50000 -etol 1.e-7 -shift -6.0

number of processes = 1
matrix size = 100 x 100 (494 nonzero entries)

initial vector x      : all components set to 1
precision             : double
eigensolver           : Power
convergence condition : ||lx-(B^-1)Ax||_2 <= 1.0e-07 * ||lx||_2
matrix storage format : CSR
shift                 : -6.000000e+00
eigensolver status    : normal end

Power: mode number          = 0
Power: eigenvalue           = -1.224829e+01
Power: number of iterations = 12997
Power: elapsed time         = 1.075117e-02 sec.
Power:   preconditioner     = 0.000000e+00 sec.
Power:     matrix creation  = 0.000000e+00 sec.
Power:   linear solver      = 0.000000e+00 sec.
Power: relative residual    = 9.997123e-08\end{lstlisting}
\end{enumerate}