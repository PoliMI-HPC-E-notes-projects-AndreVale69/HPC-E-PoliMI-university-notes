\subsection{Linear iterative methods}

\subsubsection{Definition}

In general, we consider linear iterative methods of the following form:
\begin{equation*}
    \mathbf{x}^{\left(k+1\right)} = B\mathbf{x}^{\left(k\right)} + \mathbf{f} \hspace{2em} k \ge 0
\end{equation*}
Where $B \in \mathbb{R}^{n \times n}$, $\mathbf{f} \in \mathbb{R}^{n}$ and the matrix $B$ is called \textbf{iteration matrix}. The choice of the iteration matrix and $\mathbf{f}$ uniquely identifies the method.

\highspace
The question is now automatic. \textbf{How to choose} an intelligent iteration matrix and $\mathbf{f}$? There are two main factors to consider:
\begin{itemize}
    \item \textbf{\underline{Consistency}}. This is a necessary condition, but not sufficient to guarantee the convergence. If $\mathbf{x}^{\left(k\right)}$ es the exact solution $\mathbf{x}$, then $\mathbf{x}^{\left(k+1\right)}$ is again equal to $\mathbf{x}$ (no update if the exact solution is found):
    \begin{equation*}
        \mathbf{x} = B \mathbf{x} + \mathbf{f} \longrightarrow \mathbf{f} = \left(I-B\right)\mathbf{x} = \left(I-B\right)A^{-1}\mathbf{b}
    \end{equation*}
    The former identity gives a relationship between $B$ and $\mathbf{f}$ as a function of the data.

    \item \textbf{\underline{Convergence}}. To study the convergence we need the error and the spectral radius:
    \begin{itemize}
        \item \textbf{Error}. Let us introduce the error at step $\left(k+1\right)$:
        \begin{equation*}
            \mathbf{e}^{\left(k+1\right)} = \mathbf{x} - \mathbf{x}^{\left(k+1\right)}
        \end{equation*}
        And an appropriate vector norm, such as the Euclidean norm $\vectorNormSymbol$.
        
        Then we have:
        \begin{equation*}
            \begin{array}{rcl}
                \left|\left|\mathbf{e}^{\left(k+1\right)}\right|\right| &=& \left|\left|\mathbf{x}-\mathbf{x}^{\left(k+1\right)}\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-\left(B\mathbf{x}^{\left(k\right)} + \mathbf{f}\right)\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-B\mathbf{x}^{\left(k\right)} - \mathbf{f}\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-B\mathbf{x}^{\left(k\right)} - \left(I-B\right)\mathbf{x}\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-B\mathbf{x}^{\left(k\right)} - I\mathbf{x} + B\mathbf{x}\right|\right| \\ [.5em]
                %
                &=& \left|\left|\mathbf{x}-B\mathbf{x}^{\left(k\right)} - \mathbf{x} + B\mathbf{x}\right|\right| \\ [.5em]
                %
                &=& \left|\left|-B\mathbf{x}^{\left(k\right)} + B\mathbf{x}\right|\right| \\ [.5em]
                %
                &=& \left|\left|B\left(\mathbf{x} - \mathbf{x}^{\left(k\right)}\right)\right|\right| \\ [.5em]
                %
                &=& \left|\left|B\mathbf{e}^{\left(k\right)}\right|\right| \\ [.5em]
                %
                &\le& \left|\left|B\right|\right| \cdot \left|\left|\mathbf{e}^{\left(k\right)}\right|\right|
            \end{array}
        \end{equation*}
        Note that $\left|\left|B\right|\right|$ is the matrix norm induced by the vector norm $\vectorNormSymbol$.

        Using recursion, we get:
        \begin{equation*}
            \begin{array}{rcl}
                \left|\left|\mathbf{e}^{\left(k+1\right)}\right|\right| &\le& \left|\left|B\right|\right| \cdot \left|\left|\mathbf{e}^{\left(k\right)}\right|\right| \\ [.5em]
                %
                &\le& \left|\left|B\right|\right| \cdot \left|\left|B\right|\right| \cdot \left|\left|\mathbf{e}^{\left(k-1\right)}\right|\right| \\ [.5em]
                %
                &\le& \left|\left|B\right|\right| \cdot \left|\left|B\right|\right| \cdot \left|\left|B\right|\right| \cdot \left|\left|\mathbf{e}^{\left(k-2\right)}\right|\right| \\ [.5em]
                %
                &\le& \cdots \\ [.5em]
                %
                &\le& {\left|\left|B\right|\right|}^{\left(k+1\right)} \cdot \left|\left|\mathbf{e}^{\left(0\right)}\right|\right| \\ [.5em]
                %
                \lim\limits_{k \rightarrow \infty} \left|\left|\mathbf{e}^{\left(k+1\right)}\right|\right| &\le& \left(\lim\limits_{k \rightarrow \infty} {\left|\left|B\right|\right|}^{\left(k+1\right)}\right) \cdot \left|\left|\mathbf{e}^{\left(0\right)}\right|\right|
            \end{array}
        \end{equation*}
        And here is the key. The \textbf{sufficient condition for convergence is to choose a matrix $B$ that has the norm less than $1$}:
        \begin{equation*}
            \left|\left|B\right|\right| < 1 \Longrightarrow \lim\limits_{k \rightarrow \infty} \left|\left|\mathbf{e}^{\left(k+1\right)}\right|\right| = 0
        \end{equation*}
        We recall that the \emph{Euclidean norm} (commonly used) of a matrix is calculated by taking the square root of the sum of the absolute squares of its elements. Let $A$ be a matrix of size $m \times n$, the Euclidean norm:
        \begin{equation*}
            {\left|\left|A\right|\right|}_{2} \equiv \sqrt{\displaystyle\sum_{i=1}^{m}\sum_{j=1}^{n} {\left|a_{ij}\right|}^{2}}
        \end{equation*}
        
        
        \item \definition{Spectral radius}. The spectral radius of a matrix is the \textbf{largest absolute value of its eigenvalues}. We define:
        \begin{equation*}
            \rho\left(B\right) = \underset{j}{\max} \left|\lambda_{j}\left(B\right)\right|
        \end{equation*}
        Where $\lambda_{j}\left(B\right)$ are the eigenvalues of $B$.

        Why is the spectral radius useful? Well, if the matrix $B$ is symmetric positive definite (SPD)\footnote{\definition{SPD (Symmetric Positive Definite)} is a matrix: \begin{itemize}
            \item Symmetric: $A = A^{T}$
            \item Positive Definite: $x^{T}AX > 0$, $\forall x \in \mathbb{R}^{n} \setminus \left\{0\right\}$
        \end{itemize}}, then the spectral radius is equal to the Euclidean norm of the matrix.
        \begin{equation*}
            B \text{ is SPD } \Rightarrow {\left|\left|B\right|\right|}_{2} = \rho\left(B\right) \: \land \: \rho\left(B\right) < 1 \iff \text{method convergences}
        \end{equation*}
        And this is a very big help to us for many reasons.
        \begin{itemize}
            \item \textbf{Balance and Predictability}. When the norm is equal to the spectral, it means that the influence of the matrix is well distributed. In other words, this uniformity can help make our iterative methods more predictable, reducing the possibility of non-convergence.

            \item \textbf{Efficiency}. It avoids scenarios where the matrix might have hidden large entries affecting convergence or stability.
        \end{itemize}   
    \end{itemize}
\end{itemize}
\newpage
\noindent
Let $C \in \mathbb{R}^{n \times n}$ then the spectral radius of a matrix is equal to the \href{https://en.wikipedia.org/wiki/Infimum_and_supremum}{infimum} (lower bound) of its matrix norm:
\begin{equation}
    \rho\left(C\right) = \mathrm{inf}\left\{ \left|\left|C\right|\right| \:\: \forall \text{ induced matrix norm }\vectorNormSymbol\right\}
\end{equation}
It follows from this property that:
\begin{equation}\label{eq: spectral radius less/equal than norm}
    \rho\left(B\right) \le \left|\left|B\right|\right| \hspace{2em} \forall \text{ induced matrix norm } \vectorNormSymbol
\end{equation}
Note that thanks to \ref{eq: spectral radius less/equal than norm} we can observe that if:
\begin{equation*}
    \exists \vectorNormSymbol \: \text{such that} \: \left|\left|B\right|\right| < 1 \Longrightarrow \rho\left(B\right) < 1
\end{equation*}
The convergence of the method is guaranteed by the following theorem.

\highspace
\begin{theorem}[\textbf{necessary and sufficient condition for convergence}]
    A \textbf{consistent} iterative method with iteration matrix B converges if and only if $\rho\left(B\right) < 1$.
\end{theorem}
