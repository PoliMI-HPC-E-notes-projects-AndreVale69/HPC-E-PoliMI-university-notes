\subsection{Power method}

The \definition{Power method} is an iterative technique used to \textbf{find the largest eigenvalue} (in absolute value) of a matrix and \textbf{its corresponding eigenvector}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{tools} \textbf{Algorithm}}
\end{flushleft}
Assume that the matrix $A$ has a unique eigenvalue $\lambda_{1}$ of maximum modulus:
\begin{equation*}
    \left|\lambda_{1}\right| > \left|\lambda_{2}\right| \ge \left|\lambda_{3}\right| \ge \dots \ge \left|\lambda_{n}\right|
\end{equation*}
With corresponding eigenvector $\mathbf{v}_{1}$. The algorithm is:
\begin{enumerate}
    \item \textbf{Start with an initial guess}, a nonzero vector $\mathbf{x}^{\left(0\right)}$ such that its norm is one $\left|\left|\mathbf{x}^{\left(0\right)}\right|\right| = 1$.
    \item \textbf{Iteration}. For each $k \ge 0$:
    \begin{enumerate}
        \item Multiply the current vector by the matrix:
        \begin{equation*}
            \mathbf{y}^{\left(k+1\right)} = A\mathbf{x}^{\left(k\right)}
        \end{equation*}

        \item After each multiplication, normalize the vector to prevent it from growing too large:
        \begin{equation*}
            \mathbf{x}^{\left(k+1\right)} = \dfrac{
                \mathbf{y}^{\left(k+1\right)}
            }{
                \left|\left|\mathbf{y}^{\left(k+1\right)}\right|\right|
            }
        \end{equation*}

        \label{eq: Rayleigh quotient}
        \item Computes the Rayleigh quotient. It is computed to approximate the eigenvalue corresponding to the eigenvector $\mathbf{x}^{\left(k+1\right)}$. It provides an estimate of the eigenvalue associated with the current eigenvector approximation.
        
        We can think of it as a checkpoint that tells us how close our current vector is to being an actual eigenvector, and thus how close our estimate is to the actual eigenvalue. This helps us understand the convergence of the iterative process, and ensures that we are on the right track.
        \begin{equation*}
            \nu^{\left(k+1\right)} = \left[\mathbf{x}^{\left(k+1\right)}\right]^{H} A\mathbf{x}^{\left(k+1\right)}
        \end{equation*}
    \end{enumerate}
    \item \textbf{Repeat until we meet a specific stopping criteria}.
\end{enumerate}
It can be shown that the \textbf{iteration scheme converges to a multiple of} $\mathbf{v}_{1}$, the \textbf{eigenvector corresponding to the dominant eigenvalue} $\lambda_{1}$.

\highspace
The \textbf{\underline{convergence rate}} of the power method depends on the ratio of the largest absolute eigenvalue $\left|\lambda_{1}\right|$ to the second largest absolute eigenvalue $\left|\lambda_{2}\right|$.
\begin{itemize}
    \item $\dfrac{\lambda_{2}}{\lambda_{1}} \ggg 1$, convergence rate high, the method converges \textbf{quickly}.
    \item $\dfrac{\lambda_{2}}{\lambda_{1}} \approx 1$, convergence rate low, the method converges \textbf{slowly}.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Red2}{\faIcon{dollar-sign} \textbf{How much does it cost?}}
\end{flushleft}
It depends on the matrix used:
\begin{itemize}
    \item \textbf{Dense matrix}. Each iteration costs $\approx n^{2}$ operations,.
    \item \textbf{Sparse matrix}. Each iteration costs only $\approx n$ operations.
\end{itemize}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{network-wired} \textbf{Can it be parallelized?}}
\end{flushleft}
The power method \textbf{can be parallelized to increase its efficiency}, \textbf{especially for large matrices}. This is one of the reasons it is used to solve large eigenvalue problems. A simple introduction to parallelization:
\begin{itemize}
    \item \emph{Matrix-Vector Multiplication}. The main computational task, multiplying the matrix $A$ by the vector $\mathbf{x}$, can be distributed across multiple processors. Each processor handles a portion of the matrix and vector and performs the multiplication in parallel.

    \item \emph{Normalization}. Vector norming and scaling can also benefit from parallel processing. The norm calculation is a sum of squares that can be computed in parallel.
    
    \item \emph{Rayleigh Quotient}. Computing the Rayleigh quotient for eigenvalue approximation can be parallelized similarly to matrix-vector multiplication.
\end{itemize}
