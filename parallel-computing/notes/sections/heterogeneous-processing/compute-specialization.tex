\subsection{Compute Specialization}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{check-circle} \textbf{Efficiency Benefits of Compute Specialization}}
\end{flushleft}
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Throughput-Maximized Processor Architectures}}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{book}}] These architectures are \textbf{designed to maximize the number of operations performed per second}, leveraging the parallelism inherent in certain applications (for example the GPU cores).
        \item[\textcolor{Green3}{\faIcon{tachometer-alt}}] Performance Gains: Approximately \textbf{10x improvement in performance per watt} compared to high-quality C code running on a CPU.
        \begin{itemize}
            \item \textbf{\emph{Conditions for Improvement}}: This level of improvement is\break achieved when the code:
            \begin{enumerate}
                \item Is compute-bound;
                \item Maps well to wide data-parallel execution.
            \end{enumerate}
            \item \textbf{\emph{Application Areas}}: Image processing, machine learning, and scientific simulations where operations can be parallelized across multiple data points.
        \end{itemize}
    \end{itemize}

    \item \textcolor{Green3}{\textbf{Fixed-Function Application-Specific Integrated Circuits (ASICs)}}
    \begin{itemize}
        \item[\textcolor{Green3}{\faIcon{book}}] ASICs are \textbf{custom-designed to perform specific tasks with maximum efficiency}, tailored to the needs of a particular application.
        \item[\textcolor{Green3}{\faIcon{tachometer-alt}}] Performance Gains: Can provide \textbf{100-1000x or greater improvement in performance per watt} compared to general-purpose\break CPUs (GPPs).
        \begin{itemize}
            \item \textbf{\emph{Conditions for Improvement}}: This level of efficiency is\break achieved when the workload:
            \begin{enumerate}
                \item Is compute-bound;
                \item Does not rely heavily on floating-point arithmetic.
            \end{enumerate}
            \item \textbf{\emph{Application Areas}}: Cryptocurrency mining, specific types of data encryption/decryption, and certain signal processing tasks, where the same set of operations is repeated multiple times.
        \end{itemize}
    \end{itemize}
\end{itemize}
These benefits are derived from focusing on the strengths of dedicated hardware, allowing them to outperform general-purpose processors in their designated tasks.

\begin{remarkbox}[: When is a workload compute-bound?]
    When a workload is described as compute-bound, it means that the \textbf{performance of the task is primarily limited by the speed of the processor performing the computations}. In other words, the bottleneck in this case is the processor's ability to perform computations, as opposed to other factors such as data retrieval or memory access.

    \highspace
    Therefore, when we say that the workload is compute-bound, we are referring to \textbf{workloads that spend most of their time performing computations and arithmetic operations rather than other tasks} (such as accessing memory).

    \highspace
    Tasks such as complex mathematical calculations, scientific simulations, and certain types of data processing (such as graphics rendering) are often compute-bound.

    \highspace
    In contrast, a memory-bound workload would be limited by the speed of memory access, while an I/O-bound workload would be limited by the speed of input/output operations.
\end{remarkbox}

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{Complexity of Executing an Instruction on a Modern Processor}}
\end{flushleft}
The steps involved are:
\begin{enumerate}
    \item \textbf{Read Instruction}:
    \begin{itemize}
        \item Converts virtual addresses to physical addresses.
        \item Interacts with the instruction cache to fetch the instruction.
        \item Retrieves the instruction from the cache memory.
    \end{itemize}
 
    \item \textbf{Decode Instruction}:
    \begin{itemize}
        \item Converts the high-level instruction into micro-operations.
        \item Fetches the micro-operations from the uop cache (Micro-Operation Cache).
    \end{itemize}
 
    \item \textbf{Check for Dependencies/Pipeline Hazards}: Ensures that there are no conflicts or dependencies that could cause delays in execution.
 
    \item \textbf{Identify Available Execution Resource}: Determines which execution units are free to perform the required operations.
 
    \item \textbf{Data Retrieval}:
    \begin{itemize}
        \item Accesses necessary data from the register file SRAM.
        \item Transfers the retrieved data to the appropriate execution resource.
    \end{itemize}
 
    \item \textbf{Perform Arithmetic Operation}: Executes the arithmetic or logical operation specified by the instruction.
 
    \item \textbf{Data Write-Back}:
    \begin{itemize}
        \item Move Data from Execution Resource to Register File: Transfers the result back to the register file.
        \item Control Write to Register File: Ensures that the result is correctly written back to the register file SRAM.
    \end{itemize}
\end{enumerate}

\newpage

\noindent
The overhead distribution is:
\begin{multicols}{2}
    \begin{itemize}
        \item Instruction Supply: 42\%
        \item Data Supply: 28\%
        \item Clock and Control: 24\%
        \item Arithmetic: 6\%
    \end{itemize}
\end{multicols}
\noindent
This breakdown shows that \textbf{a significant portion of the overhead comes from instruction and data supply, emphasizing the importance of efficient cache and memory management}.

\highspace
\begin{flushleft}
    \textcolor{Green3}{\faIcon{question-circle} \textbf{How does SIMD execution reduce the overhead of certain types of computations?}}
\end{flushleft}
SIMD (Single Instruction, Multiple Data) execution allows a single instruction to perform the same operation on multiple data points simultaneously. This reduces the overhead by minimizing repetitive fetch and decode cycles.

\highspace
Computations that benefit from SIMD have data-parallel characteristics, meaning they can be broken down and executed in parallel on different data elements with the same operation.
\begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
    \item \textcolor{Green3}{\textbf{Reducing Overhead}}
    \begin{itemize}
        \item \textbf{Fetch and Decode}: By fetching and decoding a single instruction that applies to multiple data points, SIMD reduces the repetitive overhead of fetching and decoding several instructions. This saves time and resources.
        \item \textbf{Parallel Execution}: SIMD utilizes parallelism, executing the same operation across various data points in one go, which helps streamline the overall process.
    \end{itemize}
    \item \textbf{SIMD helps \emph{simplify} the process by reducing the number of steps required to achieve the same result}. Instead of executing multiple instructions sequentially, SIMD groups these into a single instruction cycle, minimizing the overhead related to instruction supply and data supply.
\end{itemize}
This efficiency gain is crucial in workloads that are data-parallel and compute-bound, where performing the same operation on large datasets can benefit significantly from SIMD's parallel processing capabilities.

\newpage

\begin{examplebox}[: H.264 Video Encoding Energy Consumption]
    This figure shows the energy consumption breakdown for various stages of H.264 video encoding.

    \begin{center}
        \includegraphics[width=\textwidth]{img/h264-example-1.pdf}
    \end{center}
    
    Stages:
    \begin{itemize}
        \item \textbf{IME (Integer Motion Estimation)}: Identifies block movement between frames.
        \item \textbf{FME (Fractional Motion Estimation)}: Refines the movement estimation at a sub-pixel level.
        \item \textbf{Intra (Intra-frame Prediction, DCT, Quantization)}: Predicts and transforms data within a single frame.
        \item \textbf{CABAC (Context-Adaptive Binary Arithmetic Coding)}: A form of arithmetic encoding used for entropy coding.
        \item \textbf{Others}: Includes additional stages and tasks within the encoding process.
    \end{itemize}
    Energy Components:
    \begin{itemize}
        \item \textbf{FU (Functional Units)}: Represents the energy consumed by the functional units.
        \item \textbf{RF (Register Fetch)}: Energy used for fetching data from registers.
        \item \textbf{Ctrl (Misc Pipeline Control)}: Control-related energy consumption.
        \item \textbf{Pip (Pipeline Registers)}: Energy used for pipeline registers.
        \item \textbf{DS (Datacache)}: Data cache energy consumption.
        \item \textbf{IF (Instruction Fetch $+$ Instruction Cache)}: Energy used for fetching instructions and accessing instruction cache.
    \end{itemize}
    The fraction of energy consumed by functional units (FU) is relatively small compared to other components, even when SIMD (Single Instruction, Multiple Data) instructions are used.

    \highspace
    We want to emphasize the efficiency benefits of using specialized circuits for specific operations, as well as the importance of understanding the power distribution for complex tasks such as video encoding. \textbf{While specialized circuits can perform certain tasks more efficiently, overall power consumption must be considered to optimize performance}.
\end{examplebox}

\newpage

\begin{examplebox}[: Throughput and Energy Benefits of Specialization for Fast Fourier Transform (FFT)]
    The following graph compares the \textbf{performance per unit area} of different types of processors for executing the Fast Fourier Transform (FFT) using a 40nm semiconductor process.

    \begin{center}
        \includegraphics[width=\textwidth]{img/fft-example-1.pdf}
    \end{center}

    It illustrates the substantial benefits of using specialized hardware like ASICs and GPUs for FFT computations. ASICs, in particular, demonstrate remarkable area efficiency, making them ideal for applications where chip space is a critical constraint. GPUs also show considerable improvements, highlighting their role in high-performance, area-efficient computing tasks.

    \highspace
    The following graph compares the \textbf{energy efficiency} of different processors for executing the Fast Fourier Transform (FFT) using a 40nm semiconductor process.

    \begin{center}
        \includegraphics[width=\textwidth]{img/fft-example-2.pdf}
    \end{center}
    The graph illustrates the substantial energy-efficiency benefits of using specialized hardware like ASICs and GPUs for FFT computations. ASICs, in particular, offer remarkable energy savings, making them ideal for applications where power consumption is a critical constraint. GPUs also provide considerable improvements in energy efficiency, underscoring their role in high-performance, energy-efficient computing tasks.
\end{examplebox}

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Heterogeneous Multi-Core Architecture in GPUs}}
\end{flushleft}
\begin{figure}[!htp]
    \centering
    \includegraphics[width=\textwidth]{img/heterogeneous-gpus-1.pdf}
\end{figure}
\begin{itemize}
    \item \important{Diverse Core Types}
    \begin{itemize}
        \item \textbf{SIMD (Single Instruction, Multiple Data) Cores}: These cores are designed to execute the same instruction across multiple data points simultaneously. This structure excels in parallel processing tasks, such as matrix multiplications and vector operations, which are common in graphics and scientific computations.
        \item \textbf{Special-Purpose Cores}: GPUs include cores that are specifically tailored for tasks like texture mapping, shading, and rasterization. These cores handle operations that are unique to graphical rendering, giving GPUs their prowess in handling extensive graphical data more efficiently than CPUs.
    \end{itemize}

    \item \important{Fixed-Function Units}
    \begin{itemize}
        \item \textbf{Texture Units}: These units perform operations like texture filtering and texture mapping, essential for applying texture to 3D models.
        \item \textbf{Tessellation Units}: These break down complex, high-order surfaces into simpler polygons, which are easier to render.
        \item \textbf{Rasterization Units}: These convert vector graphics (such as\break shapes and models) into a raster image (pixels or dots) for display on the screen.
    \end{itemize}

    \item \important{Memory Controllers}. GPUs use unified memory architecture, where both CPU and GPU can access the same memory space. This architecture eliminates the need to copy data between CPU and GPU, reducing latency and increasing computational efficiency.

    \item \important{Scheduler and Work Distribution}. GPUs have sophisticated schedulers that dynamically allocate tasks to different processing units. These schedulers ensure optimal usage of the GPU's resources by balancing workloads across the multiple cores and units.
\end{itemize}
By \textbf{combining different types of cores and units}, \textbf{GPUs can handle a wide array of tasks concurrently}. For instance, while general SIMD cores might be processing large batches of data in parallel, texture units could be working on rendering images, and rasterization units converting those rendered images to pixels-all happening simultaneously, thanks to the architecture's heterogeneous nature.

\highspace
In summary, the \textbf{heterogeneous multi-core architecture of GPUs allows them to excel at parallel processing and specialized graphics tasks}, making them exponentially faster and more efficient than CPUs for such workloads.

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Digital signal processors (DSPs) - Qualcomm Hexagon DSP}}\index{Qualcomm Hexagon DSP}
\end{flushleft}
\definition{Digital Signal Processors (DSPs)} can be a \textbf{type of processor} used in the heterogeneous computing, \textbf{specialized for signal processing tasks, handling audio, video, and communication data}.

\highspace
\begin{examplebox}[: Qualcomm Hexagon DSP]
    An example of a DSP is the Qualcomm Hexagon. Its role in heterogeneous systems is:
    \begin{itemize}
        \item \textbf{Specialized Processing Tasks}: The Hexagon DSP is designed to efficiently manage tasks like modem processing, audio processing, and image processing-tasks that would be less efficient if handled by a general-purpose CPU.
        \item \textbf{Parallelism and Efficiency}: By using complex instructions such as SIMD (Single Instruction, Multiple Data) and VLIW (Very Long Instruction Word), the Hexagon DSP can handle multiple operations per instruction cycle, improving speed and efficiency for specific types of data processing.
        \item \textbf{Integration with Other Processors}: In heterogeneous computing systems, the Hexagon DSP works alongside CPUs and GPUs. For example, in smartphones like the Google Pixel, the Hexagon DSP handles signal processing while the CPU manages general tasks, and the GPU handles graphical rendering.
    \end{itemize}
    The \textbf{benefits} of the Hexagon DSP in Heterogeneous Computing:
    \begin{itemize}[label=\textcolor{Green3}{\faIcon{check}}]
        \item \textcolor{Green3}{\textbf{Performance Optimization}}: Offloading specific tasks to the DSP frees up the CPU and GPU to focus on other tasks, leading to better overall system performance.
        \item \textcolor{Green3}{\textbf{Energy Efficiency}}: The Hexagon DSP's specialized design consumes less power for certain tasks compared to using a CPU, making the system more energy-efficient. This is crucial for battery-operated devices like smartphones.
        \item \textcolor{Green3}{\textbf{Design Flexibility}}: Incorporating the Hexagon DSP into a heterogeneous computing system allows for more flexible design options, catering to various workloads and processing requirements.
    \end{itemize}
    In devices like the Google Pixel phone, the Qualcomm Hexagon DSP showcases the practical application of heterogeneous computing. It performs tasks such as audio processing and image recognition, enhancing the device's overall capabilities and efficiency.
\end{examplebox}

\newpage

\begin{examplebox}[: Anton Supercomputer]
    The \definition{Anton Supercomputer} is a specialized machine designed for molecular dynamics (MD) simulations, developed by \href{https://www.deshawresearch.com/}{D. E. Shaw Research}. It represents an excellent example of how specialized processors are integrated into heterogeneous computing systems to tackle specific scientific tasks.

    Key features of the Anton Supercomputer:
    \begin{itemize}
        \item Anton is designed explicitly for simulating the time evolution of proteins and other biological macromolecules. Achieves substantially higher performance for MD simulations compared to general-purpose supercomputers.

        \item Anton's architecture consists of many ASICs tailored to handle particle-particle interactions efficiently. For example, Anton 2 contains 512 ASICs. This subsystem performs calculations of electrostatic and van der Waals forces using 32 deeply pipelined modules running at 800 MHz. Handles bond forces and fast Fourier transforms, using general-purpose Tensilica cores and specialized SIMD cores called geometry cores.

        \item Anton's custom network is designed to handle the communication patterns of N-body simulations with a three-dimensional torus topology. Each ASIC connects to a high-speed 3D torus network with significant bandwidth to manage data transfer.

        \item By integrating specialized hardware components and a custom communication network, Anton can perform MD simulations much faster than general-purpose supercomputers. Anton achieves simulation speeds of up to 17,000 nanoseconds per day for complex systems like protein-water.
    \end{itemize}

    Linking to Heterogeneous Computing:
    \begin{itemize}
        \item Anton exemplifies heterogeneous computing by using specialized processors (ASICs) for specific tasks. This approach contrasts with general-purpose processors used in homogeneous systems.

        \item Similar to how DSPs handle signal processing tasks in heterogeneous systems, Anton's ASICs are optimized for MD simulations, leading to increased computational efficiency and speed.

        \item In a heterogeneous system, different types of processors (CPUs, GPUs, DSPs, ASICs) collaborate to perform various tasks optimally. Anton's architecture integrates ASICs and custom communication networks to achieve this.

        \item Anton's use in molecular dynamics demonstrates how specialized hardware can advance scientific research by achieving simulations previously impossible with traditional supercomputers.
    \end{itemize}
\end{examplebox}

\newpage

\begin{flushleft}
    \textcolor{Green3}{\faIcon{book} \textbf{Field Programmable Gate Arrays (FPGAs)}}
\end{flushleft}
\definition{Field Programmable Gate Arrays (FPGAs)} are \textbf{integrated circuits that can be configured by the user after manufacturing}. FPGAs offer a middle ground between Application-Specific Integrated Circuits (ASICs) and general-purpose processors, providing a high degree of customization and flexibility.

\highspace
The main features are:
\begin{itemize}
    \item \important{Configurable Logic Blocks}
    \begin{itemize}
        \item FPGAs consist of an array of programmable logic blocks that can be configured to perform specific computational tasks.
        \item These logic blocks can be interconnected through programmable\break routing to form complex digital circuits.
    \end{itemize}

    \item \important{High Parallelism}
    \begin{itemize}
        \item FPGAs are well-suited to tasks that benefit from parallel processing, such as signal processing, encryption, and machine learning.
        \item Multiple operations can be executed concurrently, making FPGAs highly efficient for certain workloads.
    \end{itemize}

    \item \important{Reconfigurability}
    \begin{itemize}
        \item Unlike ASICs, which are permanently configured for specific tasks, FPGAs can be reprogrammed to adapt to different applications and workloads.
        \item This flexibility allows FPGAs to be used in prototyping, custom hardware design, and dynamic system updates.
    \end{itemize}

    \item \important{Low Latency and High Throughput}
    \begin{itemize}
        \item FPGAs offer lower latency and higher throughput for data processing compared to CPUs and even GPUs in some scenarios.
        \item This makes them ideal for real-time applications and scenarios where data must be processed with minimal delay.
    \end{itemize}
\end{itemize}
What is their role in heterogeneous computing?
\begin{itemize}
    \item \textbf{Specialized Task Execution}
    \begin{itemize}
        \item In heterogeneous computing systems, FPGAs serve as accelerators for specific tasks that are highly parallel or require custom hardware.
        \item For example, signal processing tasks offloaded to an FPGA can be handled more efficiently than by a CPU or GPU.
    \end{itemize}

    \item \textbf{Integration with Other Processors}
    \begin{itemize}
        \item Heterogeneous systems often include a combination of CPUs, GPUs, and FPGAs, each contributing its strengths to the overall performance.
        \item The CPU can manage general tasks and high-level control, while the FPGA handles specialized, compute-intensive functions.
    \end{itemize}

    \item \textbf{Efficiency and Flexibility}
    \begin{itemize}
        \item FPGAs provide a balance of performance and versatility, allowing them to be reprogrammed for different applications as needed.
        \item This reconfigurability enhances the flexibility of heterogeneous systems, making them adaptable to changing requirements.
    \end{itemize}
\end{itemize}
In the Google Pixel camera, FPGAs are used to accelerate image processing tasks, improving performance and efficiency. The Qualcomm Hexagon DSP, mentioned earlier, integrates FPGA-like features to handle audio, image, and signal processing tasks efficiently within a heterogeneous system.

\highspace
In summary, FPGAs play a critical role in heterogeneous computing by providing customizable, efficient, and flexible solutions for specialized tasks. They work with other types of processors to improve overall system performance and adaptability.

\newpage

\begin{examplebox}[: Project Catapult]
    \definition{Project Catapult} is a Microsoft Research initiative aimed at enhancing cloud computing efficiency.

    \highspace
    Project Catapult integrates Field-Programmable Gate Arrays (FPGAs) alongside traditional CPUs to boost performance and reduce latency for various datacenter workloads. The project initially targeted improving Bing's search engine performance by offloading parts of the document ranking process to FPGAs.

    \highspace
    The system uses FPGAs as a compute layer that can act as a local accelerator, an inline processor, or a remote accelerator for distributed computing. Used in Microsoft's data centers, Project Catapult deployed FPGAs at a massive scale to improve not only Bing but other applications requiring high-speed computations.

    \highspace
    The advantages of this project are:
    \begin{itemize}
        \item \textbf{Increased Performance}: By accelerating specific workloads, FPGAs provide enhanced computational power, reducing the time required for complex tasks.
        \item \textbf{Flexibility}: FPGAs offer a flexible and reprogrammable architecture, allowing them to adapt to different types of workloads.
    \end{itemize}

    \highspace
    Building on the success of Project Catapult, Microsoft launched Project Brainwave to enable real-time AI, further leveraging FPGA acceleration.

    \highspace
    Project Catapult demonstrates how integrating FPGAs within heterogeneous computing systems can significantly enhance performance and flexibility, especially for intensive tasks like search engine optimization and AI.
\end{examplebox}

\newpage

\begin{examplebox}[: Amazon F1]
    \definition{Amazon EC2 FPGA}, also known as \definition{Amazon F1}, is a compute instance type within Amazon Web Services (AWS) that accelerates workloads by using custom hardware logic. With Amazon F1, developers can create and deploy custom FPGA designs to help accelerate tasks and increase performance.

    \highspace
    F1 instances can significantly boost the performance of compute-intensive applications by leveraging the power of FPGAs. Custom FPGA designs reduce latency and improve throughput for demanding applications.

    \highspace
    AWS provides a set of tools and libraries to develop, test, and deploy FPGA designs easily. The toolkit includes Amazon's FPGA Developer AMI (Amazon Machine Image) and AWS Management Console. F1 integrates seamlessly with other AWS services such as Lambda, S3, and RDS (Relational Database Service), allowing for comprehensive cloud-based solutions.

    \highspace
    Some use cases are:
    \begin{itemize}
        \item Cryptographic Processing: Accelerating encryption and decryption tasks.
        \item Financial Modeling: Running complex financial algorithms.
        \item Genomics: Accelerating DNA sequencing and analysis.
        \item Image and Signal Processing: Enhancing image recognition and processing.
    \end{itemize}
\end{examplebox}